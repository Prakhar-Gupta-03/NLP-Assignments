{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7958772,"sourceType":"datasetVersion","datasetId":4681581}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install torch\n# ! pip install transformers\n# ! pip install tqdm \n# ! pip install pandas\n# ! pip install torchmetrics","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting torchmetrics\n\n  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n\nRequirement already satisfied: numpy>1.20.0 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torchmetrics) (1.26.4)\n\nRequirement already satisfied: packaging>17.1 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torchmetrics) (23.2)\n\nRequirement already satisfied: torch>=1.10.0 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torchmetrics) (2.2.0)\n\nCollecting lightning-utilities>=0.8.0 (from torchmetrics)\n\n  Downloading lightning_utilities-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n\nRequirement already satisfied: setuptools in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.1.1)\n\nRequirement already satisfied: typing-extensions in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n\nRequirement already satisfied: filelock in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n\nRequirement already satisfied: sympy in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n\nRequirement already satisfied: networkx in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n\nRequirement already satisfied: jinja2 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n\nRequirement already satisfied: fsspec in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n\nRequirement already satisfied: mpmath>=0.19 in c:\\users\\richa\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n\nDownloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n\n   ---------------------------------------- 0.0/841.5 kB ? eta -:--:--\n\n   - ------------------------------------- 30.7/841.5 kB 640.0 kB/s eta 0:00:02\n\n   ---------------- ----------------------- 337.9/841.5 kB 4.1 MB/s eta 0:00:01\n\n   ---------------------------------------  839.7/841.5 kB 8.8 MB/s eta 0:00:01\n\n   ---------------------------------------- 841.5/841.5 kB 6.7 MB/s eta 0:00:00\n\nDownloading lightning_utilities-0.11.1-py3-none-any.whl (26 kB)\n\nInstalling collected packages: lightning-utilities, torchmetrics\n\nSuccessfully installed lightning-utilities-0.11.1 torchmetrics-1.3.2\n"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport tqdm\nimport pandas as pd\nfrom torchmetrics.regression import PearsonCorrCoef","metadata":{"execution":{"iopub.status.busy":"2024-03-27T18:47:38.191793Z","iopub.execute_input":"2024-03-27T18:47:38.192174Z","iopub.status.idle":"2024-03-27T18:47:38.197215Z","shell.execute_reply.started":"2024-03-27T18:47:38.192142Z","shell.execute_reply":"2024-03-27T18:47:38.196152Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Loading the data","metadata":{}},{"cell_type":"code","source":"def load_data(file_path):\n    data = pd.read_table(file_path)\n    # check if any missing values\n    print(data.isnull().sum())\n    key = data.keys()\n    # some values were missing in sentence2 column, so did the below (sentence1 didnt split properly)\n    # iterate through the rows in dataframe which have missing values\n    for index, row in data[data.isnull().any(axis=1)].iterrows():\n        if pd.isnull(row[key[2]]):\n            if(len(row[key[1]].split('\\t')) > 2 or len(row[key[1]].split('\\t')) < 2):\n                data.drop(index, inplace=True)\n                continue\n            # split the sentence1 into words into 2 parts based on \\t and assign to sentence1 and sentence2\n            sentence1, sentence2 = row[key[1]].split('\\t')\n            score = row[key[0]]\n            # assign to the row\n            data.at[index, key[1]] = sentence1\n            data.at[index, key[2]] = sentence2\n            data.at[index, key[0]] = score\n    #rescale every score in data from 0-5 to 0-1\n    data[key[0]] = data[key[0]]/5\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:24.095951Z","iopub.execute_input":"2024-03-27T19:00:24.096808Z","iopub.status.idle":"2024-03-27T19:00:24.105159Z","shell.execute_reply.started":"2024-03-27T19:00:24.096779Z","shell.execute_reply":"2024-03-27T19:00:24.104096Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"train_data = load_data('/kaggle/input/nlp-a3/train.csv')\nvalid_data = load_data('/kaggle/input/nlp-a3/dev.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:24.551179Z","iopub.execute_input":"2024-03-27T19:00:24.552045Z","iopub.status.idle":"2024-03-27T19:00:24.599486Z","shell.execute_reply.started":"2024-03-27T19:00:24.552010Z","shell.execute_reply":"2024-03-27T19:00:24.598450Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"score        0\nsentence1    0\nsentence2    5\ndtype: int64\nscore        0\nsentence1    0\nsentence2    2\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"valid_data.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:25.003542Z","iopub.execute_input":"2024-03-27T19:00:25.004231Z","iopub.status.idle":"2024-03-27T19:00:25.015522Z","shell.execute_reply.started":"2024-03-27T19:00:25.004199Z","shell.execute_reply":"2024-03-27T19:00:25.014649Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"      score                                          sentence1  \\\n1460    0.4  New UN peacekeeping chief named for Central Af...   \n1461    1.0                           Oil falls in Asian trade   \n1462    0.6     Israeli forces detain Palestinian MP in Hebron   \n1463    0.6  Israeli police clash with Palestinian proteste...   \n1464    0.0       3 killed, 4 injured in Los Angeles shootings   \n1465    0.4            Scientists prove there is water on Mars   \n1466    0.0  Pranab stresses need to strive for peace by na...   \n1467    0.4  Volkswagen skids into red in wake of pollution...   \n1468    0.0  Obama is right: Africa deserves better leadership   \n1469    0.0  New video shows US police officers beating men...   \n\n                                              sentence2  \n1460  UN takes over peacekeeping in Central African ...  \n1461                     Oil prices down in Asian trade  \n1462  Israeli forces detain 2 Palestinians in overni...  \n1463  Israel Police Clash With Palestinians in Jerus...  \n1464               Five killed in Saudi Arabia shooting  \n1465                 Has Nasa discovered water on Mars?  \n1466     WTO: India regrets action of developed nations  \n1467  Volkswagen's \"gesture of goodwill\" to diesel o...  \n1468  Obama waiting for midterm to name attorney gen...  \n1469  New York police officer critically wounded in ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1460</th>\n      <td>0.4</td>\n      <td>New UN peacekeeping chief named for Central Af...</td>\n      <td>UN takes over peacekeeping in Central African ...</td>\n    </tr>\n    <tr>\n      <th>1461</th>\n      <td>1.0</td>\n      <td>Oil falls in Asian trade</td>\n      <td>Oil prices down in Asian trade</td>\n    </tr>\n    <tr>\n      <th>1462</th>\n      <td>0.6</td>\n      <td>Israeli forces detain Palestinian MP in Hebron</td>\n      <td>Israeli forces detain 2 Palestinians in overni...</td>\n    </tr>\n    <tr>\n      <th>1463</th>\n      <td>0.6</td>\n      <td>Israeli police clash with Palestinian proteste...</td>\n      <td>Israel Police Clash With Palestinians in Jerus...</td>\n    </tr>\n    <tr>\n      <th>1464</th>\n      <td>0.0</td>\n      <td>3 killed, 4 injured in Los Angeles shootings</td>\n      <td>Five killed in Saudi Arabia shooting</td>\n    </tr>\n    <tr>\n      <th>1465</th>\n      <td>0.4</td>\n      <td>Scientists prove there is water on Mars</td>\n      <td>Has Nasa discovered water on Mars?</td>\n    </tr>\n    <tr>\n      <th>1466</th>\n      <td>0.0</td>\n      <td>Pranab stresses need to strive for peace by na...</td>\n      <td>WTO: India regrets action of developed nations</td>\n    </tr>\n    <tr>\n      <th>1467</th>\n      <td>0.4</td>\n      <td>Volkswagen skids into red in wake of pollution...</td>\n      <td>Volkswagen's \"gesture of goodwill\" to diesel o...</td>\n    </tr>\n    <tr>\n      <th>1468</th>\n      <td>0.0</td>\n      <td>Obama is right: Africa deserves better leadership</td>\n      <td>Obama waiting for midterm to name attorney gen...</td>\n    </tr>\n    <tr>\n      <th>1469</th>\n      <td>0.0</td>\n      <td>New video shows US police officers beating men...</td>\n      <td>New York police officer critically wounded in ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(train_data.shape)\nprint(train_data['score'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:25.476175Z","iopub.execute_input":"2024-03-27T19:00:25.476615Z","iopub.status.idle":"2024-03-27T19:00:25.487699Z","shell.execute_reply.started":"2024-03-27T19:00:25.476583Z","shell.execute_reply":"2024-03-27T19:00:25.486465Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"(5709, 3)\nscore\n0.0000    367\n0.8000    351\n0.6000    308\n1.0000    265\n0.7600    263\n         ... \n0.6910      1\n0.8112      1\n0.1778      1\n0.6546      1\n0.5400      1\nName: count, Length: 139, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Defining device variable","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:26.299491Z","iopub.execute_input":"2024-03-27T19:00:26.299861Z","iopub.status.idle":"2024-03-27T19:00:26.305333Z","shell.execute_reply.started":"2024-03-27T19:00:26.299832Z","shell.execute_reply":"2024-03-27T19:00:26.304306Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Using device: cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 1A: using BERT to perform regression","metadata":{}},{"cell_type":"markdown","source":"### Creating a dataset class","metadata":{}},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenizer, data, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['sentence1'] + ' [SEP] ' + self.data.iloc[idx]['sentence2']\n        inputs = self.tokenizer(text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors='pt')\n        inputs['labels'] = torch.tensor(self.data.iloc[idx]['score'], dtype=torch.float)\n        inputs = {key: inputs[key].squeeze() for key in inputs}\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:27.530382Z","iopub.execute_input":"2024-03-27T19:00:27.531177Z","iopub.status.idle":"2024-03-27T19:00:27.538943Z","shell.execute_reply.started":"2024-03-27T19:00:27.531141Z","shell.execute_reply":"2024-03-27T19:00:27.538035Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"max_length = 50","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:27.860424Z","iopub.execute_input":"2024-03-27T19:00:27.860788Z","iopub.status.idle":"2024-03-27T19:00:27.865282Z","shell.execute_reply.started":"2024-03-27T19:00:27.860757Z","shell.execute_reply":"2024-03-27T19:00:27.864184Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"### Defining the dataset and dataloader class:","metadata":{}},{"cell_type":"code","source":"train_dataset = TextDataset(tokenizer, train_data, max_length)\nvalid_dataset = TextDataset(tokenizer, valid_data, max_length)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:28.587150Z","iopub.execute_input":"2024-03-27T19:00:28.587533Z","iopub.status.idle":"2024-03-27T19:00:28.593427Z","shell.execute_reply.started":"2024-03-27T19:00:28.587503Z","shell.execute_reply":"2024-03-27T19:00:28.592191Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\nsep_token = '[SEP]'\n\n# Load pre-trained model tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:28.935209Z","iopub.execute_input":"2024-03-27T19:00:28.935600Z","iopub.status.idle":"2024-03-27T19:00:30.152252Z","shell.execute_reply.started":"2024-03-27T19:00:28.935569Z","shell.execute_reply":"2024-03-27T19:00:30.151414Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\npearson = PearsonCorrCoef()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:30.154160Z","iopub.execute_input":"2024-03-27T19:00:30.154574Z","iopub.status.idle":"2024-03-27T19:00:30.162478Z","shell.execute_reply.started":"2024-03-27T19:00:30.154538Z","shell.execute_reply":"2024-03-27T19:00:30.161616Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"### wandb setup","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:00:45.967763Z","iopub.execute_input":"2024-03-27T19:00:45.968734Z","iopub.status.idle":"2024-03-27T19:01:01.020383Z","shell.execute_reply.started":"2024-03-27T19:00:45.968699Z","shell.execute_reply":"2024-03-27T19:01:01.019472Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  \n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1,\n    part = 'A',\n    model_name = 'bert-base-uncased',\n    max_length = 50,\n    batch_size = 8,\n    learning_rate = 1e-5,\n    optimizer = 'Adam',\n    criterion = 'MSELoss',\n    epochs = 5\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:01:15.794175Z","iopub.execute_input":"2024-03-27T19:01:15.795182Z","iopub.status.idle":"2024-03-27T19:01:15.799723Z","shell.execute_reply.started":"2024-03-27T19:01:15.795143Z","shell.execute_reply":"2024-03-27T19:01:15.798774Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='assignment-3', entity= 'nlp-assignments', config=model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:01:16.660214Z","iopub.execute_input":"2024-03-27T19:01:16.660924Z","iopub.status.idle":"2024-03-27T19:01:51.635317Z","shell.execute_reply.started":"2024-03-27T19:01:16.660889Z","shell.execute_reply":"2024-03-27T19:01:51.634309Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112954300001043, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"939f911f846346029d7ebab7321bf109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240327_190116-tm3vov2c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-3/runs/tm3vov2c' target=\"_blank\">proud-bird-8</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nlp-assignments/assignment-3' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nlp-assignments/assignment-3/runs/tm3vov2c' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-3/runs/tm3vov2c</a>"},"metadata":{}},{"execution_count":74,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-3/runs/tm3vov2c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a5c41211570>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Defining the model architecture:","metadata":{}},{"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self, model):\n        super(Model, self).__init__()\n        self.model = model\n\n    def forward(self, input_ids, attention_mask, labels):\n        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n        return outputs.loss, outputs.logits\n    \nmodel = Model(model)\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:01:51.637157Z","iopub.execute_input":"2024-03-27T19:01:51.637508Z","iopub.status.idle":"2024-03-27T19:01:51.768173Z","shell.execute_reply.started":"2024-03-27T19:01:51.637470Z","shell.execute_reply":"2024-03-27T19:01:51.767327Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"Model(\n  (model): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Defining train and evaluation loops","metadata":{}},{"cell_type":"code","source":"def train(model, loader, optimizer, epochs=1, valid_loader=None):\n    wandb.define_metric('epoch')\n    wandb.define_metric('training_loss', step_metric='epoch')\n    wandb.define_metric('validation_loss', step_metric='epoch')\n    wandb.define_metric('Pearson Correlation', step_metric='epoch')\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in tqdm.tqdm(loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            optimizer.zero_grad()\n            loss, preds = model(input_ids, attention_mask, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f'Epoch: {epoch + 1}, Training loss: {total_loss / len(loader)}')\n        if(valid_loader is not None):\n            evaluate(model, valid_loader)\n        epoch_log = {}\n        epoch_log['epoch'] = epoch\n        epoch_log['training_loss'] = total_loss / len(loader)\n        wandb.log(epoch_log)\n    return total_loss / len(loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    predicted = torch.tensor([])\n    all_labels = torch.tensor([])\n    with torch.no_grad():\n        for batch in tqdm.tqdm(loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            loss, preds = model(input_ids, attention_mask, labels)\n            total_loss += loss.item()\n            for i in preds:\n                i = i.cpu()\n                predicted = torch.cat((predicted, i))\n            for i in labels:\n                i = i.cpu()\n                i = torch.tensor([i])\n                all_labels = torch.cat((all_labels, i))\n    print(f'Validation loss: {total_loss / len(loader)}, Pearson correlation: {pearson(predicted, all_labels)}')\n    epoch_log = {}\n    epoch_log['valid_loss'] = total_loss / len(loader)\n    epoch_log['Pearson Correlation'] = pearson(predicted, all_labels).item()\n    wandb.log(epoch_log)\n    return total_loss / len(loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:01:51.769561Z","iopub.execute_input":"2024-03-27T19:01:51.769835Z","iopub.status.idle":"2024-03-27T19:01:51.783449Z","shell.execute_reply.started":"2024-03-27T19:01:51.769810Z","shell.execute_reply":"2024-03-27T19:01:51.782487Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"train(model, train_loader, optimizer, epochs=5, valid_loader=valid_loader)\n# evaluate(model, valid_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:01:51.785289Z","iopub.execute_input":"2024-03-27T19:01:51.785571Z","iopub.status.idle":"2024-03-27T19:06:30.435702Z","shell.execute_reply.started":"2024-03-27T19:01:51.785547Z","shell.execute_reply":"2024-03-27T19:06:30.434605Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"100%|██████████| 714/714 [00:51<00:00, 13.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Training loss: 0.06139531917283077\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 184/184 [00:05<00:00, 35.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.034116377368184694, Pearson correlation: 0.8023785948753357\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 714/714 [00:50<00:00, 14.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Training loss: 0.020675181214246347\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 184/184 [00:05<00:00, 35.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.02697314087898754, Pearson correlation: 0.8386620879173279\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 714/714 [00:50<00:00, 14.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Training loss: 0.010633578259042655\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 184/184 [00:05<00:00, 35.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.026718384039628763, Pearson correlation: 0.8420818448066711\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 714/714 [00:50<00:00, 14.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Training loss: 0.005479094146731954\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 184/184 [00:05<00:00, 35.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.02651181103568018, Pearson correlation: 0.8404891490936279\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 714/714 [00:50<00:00, 14.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Training loss: 0.003137762849714805\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 184/184 [00:05<00:00, 35.88it/s]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.030492372087040996, Pearson correlation: 0.8379072546958923\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"0.003137762849714805"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:06:37.390751Z","iopub.execute_input":"2024-03-27T19:06:37.391023Z","iopub.status.idle":"2024-03-27T19:06:37.395142Z","shell.execute_reply.started":"2024-03-27T19:06:37.390998Z","shell.execute_reply":"2024-03-27T19:06:37.394318Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '1A_model.pt')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T19:06:38.518841Z","iopub.execute_input":"2024-03-27T19:06:38.519923Z","iopub.status.idle":"2024-03-27T19:06:39.487463Z","shell.execute_reply.started":"2024-03-27T19:06:38.519876Z","shell.execute_reply":"2024-03-27T19:06:39.486343Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"#load data from sample_demo.csv\ntest_data = load_data('/kaggle/input/nlp-a3/sample_demo.csv')\ntest_dataset = TextDataset(tokenizer, test_data, max_length)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"id           0\n\nscore        0\n\nsetence1     0\n\nsentence2    0\n\ndtype: int64\n"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('model.pt'))\nevaluate(model, test_loader)","metadata":{},"execution_count":23,"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'model.pt'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m evaluate(model, test_loader)\n","File \u001b[1;32mc:\\Users\\saras\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[1;32mc:\\Users\\saras\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[1;32mc:\\Users\\saras\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pt'"]}]}]}