{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch\n",
    "! pip install transformers\n",
    "! pip install tqdm\n",
    "! pip install pandas\n",
    "! pip install torchmetrics\n",
    "! pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saras\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "from torchmetrics.regression import PearsonCorrCoef\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses, models, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_table(file_path)\n",
    "    # check if any missing values\n",
    "    print(data.isnull().sum())\n",
    "    key = data.keys()\n",
    "    # some values were missing in sentence2 column, so did the below (sentence1 didnt split properly)\n",
    "    # iterate through the rows in dataframe which have missing values\n",
    "    for index, row in data[data.isnull().any(axis=1)].iterrows():\n",
    "        if pd.isnull(row[key[2]]):\n",
    "            if(len(row[key[1]].split('\\t')) > 2 or len(row[key[1]].split('\\t')) < 2):\n",
    "                data.drop(index, inplace=True)\n",
    "                continue\n",
    "            # split the sentence1 into words into 2 parts based on \\t and assign to sentence1 and sentence2\n",
    "            sentence1, sentence2 = row[key[1]].split('\\t')\n",
    "            sentenceid = row[key[0]]\n",
    "            # assign to the row\n",
    "            data.at[index, key[1]] = sentence1\n",
    "            data.at[index, key[2]] = sentence2\n",
    "            data.at[index, key[0]] = sentenceid\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "sentence1    0\n",
      "sentence2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_data = load_data('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Device Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "# Create the Sentence Transformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = self.data.iloc[idx, 1]\n",
    "        sentence2 = self.data.iloc[idx, 2]\n",
    "        id = torch.tensor(self.data.iloc[idx, 0])\n",
    "        return [sentence1, sentence2], id\n",
    "\n",
    "test_dataset = ValidationDataset(test_data)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "#load checkpoint\n",
    "checkpoint = torch.load('/1C_model.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "def validation(model, valid_loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            sentences, targets = batch\n",
    "            for i in range(len(targets)):\n",
    "                sentence1_features = model.encode(sentences[0][i], convert_to_tensor=True).to(device)\n",
    "                sentence2_features = model.encode(sentences[1][i], convert_to_tensor=True).to(device)\n",
    "                score = util.pytorch_cos_sim(sentence1_features, sentence2_features)\n",
    "                all_scores.append(score.item())\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9926149845123291, 0.7718663811683655, 0.6741741299629211, 0.9912800788879395, 0.7612790465354919, 0.9965555667877197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output1c = validation(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = output1c\n",
    "scores = [i * 5 for i in scores]\n",
    "pearson = PearsonCorrCoef()\n",
    "print(torch.tensor(scores))\n",
    "print(torch.tensor([5.000, 4.750, 5.000, 2.400, 2.750, 2.615]))\n",
    "pearson(torch.tensor(scores), torch.tensor([5.000, 4.750, 5.000, 2.400, 2.750, 2.615]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add scores to test_data\n",
    "test_data['scores'] = scores\n",
    "#make order as id, scores, sentence1, sentence2\n",
    "test_data = test_data[['id', 'scores', 'sentence1', 'sentence2']]\n",
    "test_data.to_csv('test_scores.csv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
