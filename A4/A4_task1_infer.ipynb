{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8176614,"sourceType":"datasetVersion","datasetId":4840102},{"sourceId":8176629,"sourceType":"datasetVersion","datasetId":4840113}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm, trange","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:51.792312Z","iopub.execute_input":"2024-04-20T16:19:51.793271Z","iopub.status.idle":"2024-04-20T16:19:51.798966Z","shell.execute_reply.started":"2024-04-20T16:19:51.793223Z","shell.execute_reply":"2024-04-20T16:19:51.798044Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"num_classes = 7","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:52.401255Z","iopub.execute_input":"2024-04-20T16:19:52.401607Z","iopub.status.idle":"2024-04-20T16:19:52.405901Z","shell.execute_reply.started":"2024-04-20T16:19:52.401578Z","shell.execute_reply":"2024-04-20T16:19:52.404899Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = self.process(data, tokenizer)\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data['input'])\n\n    def __getitem__(self, index):\n        #tokenize the input\n        input = self.data['input'][index]\n        target = self.data['target'][index]\n        encoding = self.tokenizer(input, return_tensors='pt', padding='max_length', max_length=self.max_len, truncation=True)\n        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'target': torch.tensor(target)}\n        \n    def process(self, data, tokenizer):\n        new_data = {'input': [], 'target': []}\n        for i in range(len(data)):\n            newinput = data.loc[i].copy()\n            stringlist = []\n            for j in range(len(newinput['utterances'])):\n                stringlist.append(newinput['speakers'][j] + ': ' + newinput['utterances'][j])\n            for j in range(len(newinput['utterances'])):\n                #join till jth utterance\n                temp = ' '.join(stringlist[:j])\n                temp += '</s></s>'\n                #add jth utterance\n                temp += newinput['speakers'][j] + ': ' + newinput['utterances'][j]\n                temp += '</s></s>'\n                #add later utterances\n                temp += ' '.join(stringlist[j+1:])\n                temp = '<s> ' + temp + '</s>'\n                new_data['input'].append(temp)\n                target = [0]*num_classes\n                target[emotion_to_idx[newinput['emotions'][j]]] = 1.0\n                new_data['target'].append(target)\n        return new_data","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:52.840718Z","iopub.execute_input":"2024-04-20T16:19:52.841077Z","iopub.status.idle":"2024-04-20T16:19:52.852786Z","shell.execute_reply.started":"2024-04-20T16:19:52.841048Z","shell.execute_reply":"2024-04-20T16:19:52.851870Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = []\n    attention_masks = []\n    targets = []\n    for i in batch:\n        input_ids.append(i['input_ids'])\n        attention_masks.append(i['attention_mask'])\n        targets.append(i['target'])\n    input_ids = torch.stack(input_ids, dim=0)\n    attention_masks = torch.stack(attention_masks, dim=0)\n    targets = torch.stack(targets, dim=0)\n    return {\n        'input_ids': input_ids,\n        'attention_masks': attention_masks,\n        'labels': targets\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:53.266636Z","iopub.execute_input":"2024-04-20T16:19:53.266996Z","iopub.status.idle":"2024-04-20T16:19:53.273463Z","shell.execute_reply.started":"2024-04-20T16:19:53.266968Z","shell.execute_reply":"2024-04-20T16:19:53.272397Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# input csv\ntest_data = pd.read_json('/kaggle/input/nlp-a4infer/val_file.json')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:53.660431Z","iopub.execute_input":"2024-04-20T16:19:53.660796Z","iopub.status.idle":"2024-04-20T16:19:53.682719Z","shell.execute_reply.started":"2024-04-20T16:19:53.660768Z","shell.execute_reply":"2024-04-20T16:19:53.681803Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_data['utterances'] = test_data['utterances'].apply(lambda x: '@'.join(x))\ntest_data['speakers'] = test_data['speakers'].apply(lambda x: '@'.join(x))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:54.199594Z","iopub.execute_input":"2024-04-20T16:19:54.200463Z","iopub.status.idle":"2024-04-20T16:19:54.208183Z","shell.execute_reply.started":"2024-04-20T16:19:54.200429Z","shell.execute_reply":"2024-04-20T16:19:54.207253Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.drop_duplicates(subset=['speakers', 'utterances'], keep='first')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:54.827254Z","iopub.execute_input":"2024-04-20T16:19:54.827594Z","iopub.status.idle":"2024-04-20T16:19:54.834977Z","shell.execute_reply.started":"2024-04-20T16:19:54.827567Z","shell.execute_reply":"2024-04-20T16:19:54.834085Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"test_data['utterances'] = test_data['utterances'].apply(lambda x: x.split('@'))\ntest_data['speakers'] = test_data['speakers'].apply(lambda x: x.split('@'))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:55.280143Z","iopub.execute_input":"2024-04-20T16:19:55.280474Z","iopub.status.idle":"2024-04-20T16:19:55.289828Z","shell.execute_reply.started":"2024-04-20T16:19:55.280449Z","shell.execute_reply":"2024-04-20T16:19:55.288834Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:55.704871Z","iopub.execute_input":"2024-04-20T16:19:55.705242Z","iopub.status.idle":"2024-04-20T16:19:55.710615Z","shell.execute_reply.started":"2024-04-20T16:19:55.705215Z","shell.execute_reply":"2024-04-20T16:19:55.709712Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"unicode_mapping = {}\n# unicode_mapping['\\u0085'] = '...' \n# unicode_mapping['\\u0091'] = \"'\"\n# unicode_mapping['\\u0092'] = \"'\"\n# unicode_mapping['\\u0093'] = '\"'\n# unicode_mapping['\\u0094'] = '\"'\n# unicode_mapping['\\u0097'] = '--'\n\n# unicode_mapping['\\u2014'] = '--'\n# unicode_mapping['\\u2019'] = \"'\"\n# unicode_mapping['\\u2026'] = '...'\n\n# unicode_mapping['\\u00e9'] = 'e'\n\nunicode_mapping['\\x85'] = '...' \nunicode_mapping['\\x91'] = \"'\"\nunicode_mapping['\\x92'] = \"'\"\nunicode_mapping['\\x93'] = '\"'\nunicode_mapping['\\x94'] = '\"'\nunicode_mapping['\\x97'] = '--'\n\nunicode_mapping['\\u2014'] = '--'\nunicode_mapping['\\u2019'] = \"'\"\nunicode_mapping['\\u2026'] = '...'\n\nunicode_mapping['\\xe9'] = 'e'","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:56.206124Z","iopub.execute_input":"2024-04-20T16:19:56.206466Z","iopub.status.idle":"2024-04-20T16:19:56.212898Z","shell.execute_reply.started":"2024-04-20T16:19:56.206438Z","shell.execute_reply":"2024-04-20T16:19:56.212000Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# replacing unicode characters in the data\ndef clean_utterance(utterance_list):\n    '''\t\n    This function takes a list of utterances and replaces the unicode with the proper characters.\n    '''\n    cleaned_utterances_list = []\n    for utterance in utterance_list:\n        for key in unicode_mapping:\n            utterance = utterance.replace(key, unicode_mapping[key])\n        cleaned_utterances_list.append(utterance)\n    return cleaned_utterances_list","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:19:57.655795Z","iopub.execute_input":"2024-04-20T16:19:57.656467Z","iopub.status.idle":"2024-04-20T16:19:57.661724Z","shell.execute_reply.started":"2024-04-20T16:19:57.656435Z","shell.execute_reply":"2024-04-20T16:19:57.660724Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# clean the val data\nn_val = len(test_data['utterances'])\nfor i in test_data.index:\n    temp = test_data.loc[i].copy()\n    cleaned_utterances = clean_utterance(temp['utterances'])\n    temp['utterances'] = cleaned_utterances\n    test_data.loc[i] = temp","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:00.289832Z","iopub.execute_input":"2024-04-20T16:20:00.290230Z","iopub.status.idle":"2024-04-20T16:20:00.632227Z","shell.execute_reply.started":"2024-04-20T16:20:00.290203Z","shell.execute_reply":"2024-04-20T16:20:00.631439Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:01.011048Z","iopub.execute_input":"2024-04-20T16:20:01.011734Z","iopub.status.idle":"2024-04-20T16:20:01.338665Z","shell.execute_reply.started":"2024-04-20T16:20:01.011704Z","shell.execute_reply":"2024-04-20T16:20:01.337653Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:18.279505Z","iopub.execute_input":"2024-04-20T16:20:18.280203Z","iopub.status.idle":"2024-04-20T16:20:18.285224Z","shell.execute_reply.started":"2024-04-20T16:20:18.280168Z","shell.execute_reply":"2024-04-20T16:20:18.284186Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model M1","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    #model consists of a EmoBERTa model and a linear layer for sequence labeling task\n    def __init__(self, num_classes):\n        super(Model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask)\n        out = outputs[0]\n        out = out[:, 0, :]\n        out = self.fc(out)\n        return out\n    \nemotion_to_idx = {'surprise': 0, 'fear': 1, 'sadness': 2, 'disgust': 3, 'anger': 4, 'neutral': 5, 'joy': 6}\nidx_to_emotion = {0: 'surprise', 1: 'fear', 2: 'sadness', 3: 'disgust', 4: 'anger', 5: 'neutral', 6: 'joy'}\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:19.190555Z","iopub.execute_input":"2024-04-20T16:20:19.191396Z","iopub.status.idle":"2024-04-20T16:20:19.198429Z","shell.execute_reply.started":"2024-04-20T16:20:19.191364Z","shell.execute_reply":"2024-04-20T16:20:19.197511Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test_dataset = Dataset(test_data, tokenizer, 128)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:20.711196Z","iopub.execute_input":"2024-04-20T16:20:20.711811Z","iopub.status.idle":"2024-04-20T16:20:20.962323Z","shell.execute_reply.started":"2024-04-20T16:20:20.711775Z","shell.execute_reply":"2024-04-20T16:20:20.961463Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# load model M1\n\nmodel = Model(num_classes)\n# model.load_state_dict(torch.load('M1.pth', map_location=torch.device('cpu')))\n# loading the model on the device if device is gpu\nmodel.load_state_dict(torch.load('/kaggle/input/nlpa4-withmodels/M1.pth', map_location=device))\nmodel.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:31.441771Z","iopub.execute_input":"2024-04-20T16:20:31.442141Z","iopub.status.idle":"2024-04-20T16:20:36.081026Z","shell.execute_reply.started":"2024-04-20T16:20:31.442112Z","shell.execute_reply":"2024-04-20T16:20:36.080015Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate(model, val_loader, criterion, num_classes):   \n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        predictions = []\n        true_labels = []\n        for i, data in enumerate(tqdm(val_loader)):\n            input_ids = data['input_ids'].to(device)\n            attention_masks = data['attention_masks'].to(device)\n            labels = data['labels'].to(device) #labels are one-hot encoded\n            outputs = model(input_ids, attention_masks)\n            loss = criterion(outputs.view(-1, num_classes), labels)\n            val_loss += loss.item()\n            predictions.append(torch.argmax(outputs, dim=1))\n            true_labels.append(torch.argmax(labels, dim=1))\n        predictions = torch.cat(predictions, dim=0)\n        true_labels = torch.cat(true_labels, dim=0)\n        # print(predictions)\n        # print(true_labels)\n        f1_micro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='micro')\n        f1_macro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n        f1_weighted = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n        val_loss /= len(val_loader)\n        # val_losses.append(val_loss)\n        # print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')\n    return val_loss, f1_micro, f1_macro, f1_weighted","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:49.190983Z","iopub.execute_input":"2024-04-20T16:20:49.191341Z","iopub.status.idle":"2024-04-20T16:20:49.201527Z","shell.execute_reply.started":"2024-04-20T16:20:49.191314Z","shell.execute_reply":"2024-04-20T16:20:49.200333Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"loss, f1_micro, f1_macro, f1_weighted = evaluate(model, test_loader, criterion, num_classes)\n\nprint(f'Loss: {loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:20:50.824513Z","iopub.execute_input":"2024-04-20T16:20:50.824867Z","iopub.status.idle":"2024-04-20T16:21:24.686308Z","shell.execute_reply.started":"2024-04-20T16:20:50.824839Z","shell.execute_reply":"2024-04-20T16:21:24.685397Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"100%|██████████| 874/874 [00:33<00:00, 25.84it/s]","output_type":"stream"},{"name":"stdout","text":"Loss: 0.3063106957990335, F1 Micro: 0.884549356223176, F1 Macro: 0.8625106878464001, F1 Weighted: 0.8838961470917037\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model M2","metadata":{}},{"cell_type":"code","source":"class Model2(nn.Module):\n    def __init__(self, num_classes):\n        super(Model2, self).__init__()\n        self.embedding = nn.Embedding(50265, 768)\n        self.gru = nn.GRU(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(768*2, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.embedding(input_ids)\n        out, _ = self.gru(out)\n        out = out[:, 0, :]\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:24:41.946137Z","iopub.execute_input":"2024-04-20T16:24:41.946838Z","iopub.status.idle":"2024-04-20T16:24:41.953325Z","shell.execute_reply.started":"2024-04-20T16:24:41.946806Z","shell.execute_reply":"2024-04-20T16:24:41.952353Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model2 = Model2(num_classes)\n# loading the model on the device \nmodel2.load_state_dict(torch.load('/kaggle/input/nlpa4-withmodels/M2.pth', map_location=device))\nmodel2.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:25:52.341282Z","iopub.execute_input":"2024-04-20T16:25:52.341996Z","iopub.status.idle":"2024-04-20T16:25:53.193893Z","shell.execute_reply.started":"2024-04-20T16:25:52.341960Z","shell.execute_reply":"2024-04-20T16:25:53.192992Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Model2(\n  (embedding): Embedding(50265, 768)\n  (gru): GRU(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n  (fc): Linear(in_features=1536, out_features=7, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"loss, f1_micro, f1_macro, f1_weighted = evaluate(model2, test_loader, criterion, num_classes)\n\nprint(f'Loss: {loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T16:26:19.993843Z","iopub.execute_input":"2024-04-20T16:26:19.994534Z","iopub.status.idle":"2024-04-20T16:26:40.986090Z","shell.execute_reply.started":"2024-04-20T16:26:19.994498Z","shell.execute_reply":"2024-04-20T16:26:40.985199Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"100%|██████████| 874/874 [00:20<00:00, 41.68it/s]","output_type":"stream"},{"name":"stdout","text":"Loss: 1.1738981021487194, F1 Micro: 0.5394849785407725, F1 Macro: 0.33780431640347536, F1 Weighted: 0.4950421213811303\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}