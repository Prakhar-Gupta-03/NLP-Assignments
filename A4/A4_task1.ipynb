{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#autotokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json('train_file.json')\n",
    "val_data = pd.read_json('val_file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6740\n",
      "843\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_3492</td>\n",
       "      <td>[Phoebe, Eric, Phoebe, Eric, Phoebe]</td>\n",
       "      <td>[surprise, fear, surprise, sadness, disgust]</td>\n",
       "      <td>[You-youyou had sex with Ursula?!, Uh, a litt...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_3952</td>\n",
       "      <td>[Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...</td>\n",
       "      <td>[disgust, disgust, anger, sadness, surprise, a...</td>\n",
       "      <td>[Dad, please don't pick your teeth out here!, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_3198</td>\n",
       "      <td>[Older Scientist, Ross, Ross, Joey, Ross, Ross...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n",
       "      <td>[Dr. Geller, there's a seat over here., Thank ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_2834</td>\n",
       "      <td>[Monica, Monica, Monica]</td>\n",
       "      <td>[neutral, surprise, neutral]</td>\n",
       "      <td>[So, how'd the lasagne go over?, Really?!, Good.]</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_453</td>\n",
       "      <td>[Kate, The Director, Kate]</td>\n",
       "      <td>[joy, sadness, sadness]</td>\n",
       "      <td>[Become a drama critic!, I am hurt!  A plague ...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          episode                                           speakers  \\\n",
       "0  utterance_3492               [Phoebe, Eric, Phoebe, Eric, Phoebe]   \n",
       "1  utterance_3952  [Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...   \n",
       "2  utterance_3198  [Older Scientist, Ross, Ross, Joey, Ross, Ross...   \n",
       "3  utterance_2834                           [Monica, Monica, Monica]   \n",
       "4   utterance_453                         [Kate, The Director, Kate]   \n",
       "\n",
       "                                            emotions  \\\n",
       "0       [surprise, fear, surprise, sadness, disgust]   \n",
       "1  [disgust, disgust, anger, sadness, surprise, a...   \n",
       "2  [neutral, neutral, neutral, neutral, neutral, ...   \n",
       "3                       [neutral, surprise, neutral]   \n",
       "4                            [joy, sadness, sadness]   \n",
       "\n",
       "                                          utterances  \\\n",
       "0  [You-you\n",
       "you had sex with Ursula?!, Uh, a litt...   \n",
       "1  [Dad, please don't pick your teeth out here!, ...   \n",
       "2  [Dr. Geller, there's a seat over here., Thank ...   \n",
       "3  [So, how'd the lasagne go over?, Really?!, Good.]   \n",
       "4  [Become a drama critic!, I am hurt!  A plague ...   \n",
       "\n",
       "                                            triggers  \n",
       "0                          [1.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2                [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]  \n",
       "3                                    [0.0, 0.0, 1.0]  \n",
       "4                                    [0.0, 0.0, 1.0]  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = set()\n",
    "for i in train_data['emotions']:\n",
    "    for j in i:\n",
    "        st.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Emotions in Data:\n",
      "{'surprise', 'disgust', 'joy', 'fear', 'anger', 'sadness', 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Emotions in Data:\")\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join utterances with @ sign\n",
    "train_data['utterances'] = train_data['utterances'].apply(lambda x: '@'.join(x))\n",
    "train_data['speakers'] = train_data['speakers'].apply(lambda x: '@'.join(x))\n",
    "val_data['utterances'] = val_data['utterances'].apply(lambda x: '@'.join(x))\n",
    "val_data['speakers'] = val_data['speakers'].apply(lambda x: '@'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates \n",
    "train_data = train_data.drop_duplicates(subset=['speakers', 'utterances'], keep='first')\n",
    "val_data = val_data.drop_duplicates(subset=['speakers', 'utterances'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['utterances'] = train_data['utterances'].apply(lambda x: x.split('@'))\n",
    "train_data['speakers'] = train_data['speakers'].apply(lambda x: x.split('@'))\n",
    "val_data['utterances'] = val_data['utterances'].apply(lambda x: x.split('@'))\n",
    "val_data['speakers'] = val_data['speakers'].apply(lambda x: x.split('@'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index as we dropped rows\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You-youyou had sex with Ursula?!\n",
      "Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and\n",
      "You didn't notice she was wearing different clothes?!\n",
      "Well I was just so excited to see you.\n",
      "Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\n"
     ]
    }
   ],
   "source": [
    "# print((train_data['utterances'][0]))\n",
    "n = len(train_data['utterances'][0])\n",
    "for i in range(n):\n",
    "    print((train_data['utterances'][0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_mapping = {}\n",
    "# unicode_mapping['\\u0085'] = '...' \n",
    "# unicode_mapping['\\u0091'] = \"'\"\n",
    "# unicode_mapping['\\u0092'] = \"'\"\n",
    "# unicode_mapping['\\u0093'] = '\"'\n",
    "# unicode_mapping['\\u0094'] = '\"'\n",
    "# unicode_mapping['\\u0097'] = '--'\n",
    "\n",
    "# unicode_mapping['\\u2014'] = '--'\n",
    "# unicode_mapping['\\u2019'] = \"'\"\n",
    "# unicode_mapping['\\u2026'] = '...'\n",
    "\n",
    "# unicode_mapping['\\u00e9'] = 'e'\n",
    "\n",
    "unicode_mapping['\\x85'] = '...' \n",
    "unicode_mapping['\\x91'] = \"'\"\n",
    "unicode_mapping['\\x92'] = \"'\"\n",
    "unicode_mapping['\\x93'] = '\"'\n",
    "unicode_mapping['\\x94'] = '\"'\n",
    "unicode_mapping['\\x97'] = '--'\n",
    "\n",
    "unicode_mapping['\\u2014'] = '--'\n",
    "unicode_mapping['\\u2019'] = \"'\"\n",
    "unicode_mapping['\\u2026'] = '...'\n",
    "\n",
    "unicode_mapping['\\xe9'] = 'e'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing unicode characters in the data\n",
    "def clean_utterance(utterance_list):\n",
    "    '''\t\n",
    "    This function takes a list of utterances and replaces the unicode with the proper characters.\n",
    "    '''\n",
    "    cleaned_utterances_list = []\n",
    "    for utterance in utterance_list:\n",
    "        for key in unicode_mapping:\n",
    "            utterance = utterance.replace(key, unicode_mapping[key])\n",
    "        cleaned_utterances_list.append(utterance)\n",
    "    return cleaned_utterances_list\n",
    "\n",
    "# clean the train data\n",
    "n_train = len(train_data['utterances'])\n",
    "for i in train_data.index:\n",
    "    temp = train_data.loc[i].copy()\n",
    "    cleaned_utterances = clean_utterance(temp['utterances'])\n",
    "    temp['utterances'] = cleaned_utterances\n",
    "    train_data.loc[i] = temp\n",
    "\n",
    "# clean the val data\n",
    "n_val = len(val_data['utterances'])\n",
    "for i in val_data.index:\n",
    "    temp = val_data.loc[i].copy()\n",
    "    cleaned_utterances = clean_utterance(temp['utterances'])\n",
    "    temp['utterances'] = cleaned_utterances\n",
    "    val_data.loc[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You-you...you had sex with Ursula?!', 'Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and', \"You didn't notice she was wearing different clothes?!\", 'Well I was just so excited to see you.', \"Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\"]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['utterances'][0])\n",
    "# print(len(train_data['utterances'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>speakers</th>\n",
       "      <th>emotions</th>\n",
       "      <th>utterances</th>\n",
       "      <th>triggers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utterance_3492</td>\n",
       "      <td>[Phoebe, Eric, Phoebe, Eric, Phoebe]</td>\n",
       "      <td>[surprise, fear, surprise, sadness, disgust]</td>\n",
       "      <td>[You-you...you had sex with Ursula?!, Uh, a li...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utterance_3952</td>\n",
       "      <td>[Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...</td>\n",
       "      <td>[disgust, disgust, anger, sadness, surprise, a...</td>\n",
       "      <td>[Dad, please don't pick your teeth out here!, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utterance_3198</td>\n",
       "      <td>[Older Scientist, Ross, Ross, Joey, Ross, Ross...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n",
       "      <td>[Dr. Geller, there's a seat over here., Thank ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utterance_2834</td>\n",
       "      <td>[Monica, Monica, Monica]</td>\n",
       "      <td>[neutral, surprise, neutral]</td>\n",
       "      <td>[So, how'd the lasagne go over?, Really?!, Good.]</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utterance_453</td>\n",
       "      <td>[Kate, The Director, Kate]</td>\n",
       "      <td>[joy, sadness, sadness]</td>\n",
       "      <td>[Become a drama critic!, I am hurt!  A plague ...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          episode                                           speakers  \\\n",
       "0  utterance_3492               [Phoebe, Eric, Phoebe, Eric, Phoebe]   \n",
       "1  utterance_3952  [Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...   \n",
       "2  utterance_3198  [Older Scientist, Ross, Ross, Joey, Ross, Ross...   \n",
       "3  utterance_2834                           [Monica, Monica, Monica]   \n",
       "4   utterance_453                         [Kate, The Director, Kate]   \n",
       "\n",
       "                                            emotions  \\\n",
       "0       [surprise, fear, surprise, sadness, disgust]   \n",
       "1  [disgust, disgust, anger, sadness, surprise, a...   \n",
       "2  [neutral, neutral, neutral, neutral, neutral, ...   \n",
       "3                       [neutral, surprise, neutral]   \n",
       "4                            [joy, sadness, sadness]   \n",
       "\n",
       "                                          utterances  \\\n",
       "0  [You-you...you had sex with Ursula?!, Uh, a li...   \n",
       "1  [Dad, please don't pick your teeth out here!, ...   \n",
       "2  [Dr. Geller, there's a seat over here., Thank ...   \n",
       "3  [So, how'd the lasagne go over?, Really?!, Good.]   \n",
       "4  [Become a drama critic!, I am hurt!  A plague ...   \n",
       "\n",
       "                                            triggers  \n",
       "0                          [1.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2                [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]  \n",
       "3                                    [0.0, 0.0, 1.0]  \n",
       "4                                    [0.0, 0.0, 1.0]  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808, 5)\n"
     ]
    }
   ],
   "source": [
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    #model consists of a EmoBERTa model and a linear layer for sequence labeling task\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        out = outputs[0]\n",
    "        out = out[:, 0, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def get_emotion_to_idx(data):\n",
    "    emotion_to_idx = {}\n",
    "    idx_to_emotion = {}\n",
    "    idx = 0\n",
    "    for i in data['emotions']:\n",
    "        for j in i:\n",
    "            if j not in emotion_to_idx:\n",
    "                emotion_to_idx[j] = idx\n",
    "                idx_to_emotion[idx] = j\n",
    "                idx += 1\n",
    "    return emotion_to_idx, idx_to_emotion\n",
    "\n",
    "emotion_to_idx, idx_to_emotion = get_emotion_to_idx(train_data)\n",
    "num_classes = len(emotion_to_idx)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.process(data, tokenizer)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #tokenize the input\n",
    "        input = self.data['input'][index]\n",
    "        target = self.data['target'][index]\n",
    "        encoding = self.tokenizer(input, return_tensors='pt', padding='max_length', max_length=self.max_len, truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'target': torch.tensor(target)}\n",
    "        \n",
    "    def process(self, data, tokenizer):\n",
    "        new_data = {'input': [], 'target': []}\n",
    "        for i in range(len(data)):\n",
    "            newinput = data.loc[i].copy()\n",
    "            stringlist = []\n",
    "            for j in range(len(newinput['utterances'])):\n",
    "                stringlist.append(newinput['speakers'][j] + ': ' + newinput['utterances'][j])\n",
    "            for j in range(len(newinput['utterances'])):\n",
    "                #join till jth utterance\n",
    "                temp = ' '.join(stringlist[:j])\n",
    "                temp += '</s></s>'\n",
    "                #add jth utterance\n",
    "                temp += newinput['speakers'][j] + ': ' + newinput['utterances'][j]\n",
    "                temp += '</s></s>'\n",
    "                #add later utterances\n",
    "                temp += ' '.join(stringlist[j+1:])\n",
    "                temp = '<s> ' + temp + '</s>'\n",
    "                new_data['input'].append(temp)\n",
    "                target = [0]*num_classes\n",
    "                target[emotion_to_idx[newinput['emotions'][j]]] = 1.0\n",
    "                new_data['target'].append(target)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    targets = []\n",
    "    for i in batch:\n",
    "        input_ids.append(i['input_ids'])\n",
    "        attention_masks.append(i['attention_mask'])\n",
    "        targets.append(i['target'])\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    attention_masks = torch.stack(attention_masks, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_masks': attention_masks,\n",
    "        'labels': targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train_data, tokenizer, 128)\n",
    "val_dataset = Dataset(val_data, tokenizer, 128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = Model(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_classes, num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # f1_micros = []\n",
    "    # f1_macros = []\n",
    "    # f1_weighteds = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        avg_f1_micro = 0\n",
    "        avg_f1_macro = 0\n",
    "        avg_f1_weighted = 0\n",
    "        #tqdm\n",
    "        for i, data in enumerate(tqdm(train_loader)): \n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_masks = data['attention_masks'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            loss = criterion(outputs.view(-1, num_classes), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        #     predictions = torch.argmax(outputs, dim=1)\n",
    "        #     labels = torch.argmax(labels, dim=1)\n",
    "        #     f1_micro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='micro')\n",
    "        #     f1_macro = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "        #     f1_weighted = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n",
    "        #     print(f1_micro, f1_macro, f1_weighted)\n",
    "        #     avg_f1_micro += f1_micro\n",
    "        #     avg_f1_macro += f1_macro\n",
    "        #     avg_f1_weighted += f1_weighted\n",
    "        # avg_f1_micro /= len(train_loader)\n",
    "        # avg_f1_macro /= len(train_loader)\n",
    "        # avg_f1_weighted /= len(train_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        # f1_micros.append(avg_f1_micro)\n",
    "        # f1_macros.append(avg_f1_macro)\n",
    "        # f1_weighteds.append(avg_f1_weighted)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}')\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for i, data in enumerate(val_loader):\n",
    "                input_ids = data['input_ids'].to(device)\n",
    "                attention_masks = data['attention_masks'].to(device)\n",
    "                labels = data['labels'].to(device) #labels are one-hot encoded\n",
    "                outputs = model(input_ids, attention_masks)\n",
    "                loss = criterion(outputs.view(-1, num_classes), labels)\n",
    "                val_loss += loss.item()\n",
    "                predictions.append(torch.argmax(outputs, dim=1))\n",
    "                true_labels.append(torch.argmax(labels, dim=1))\n",
    "            predictions = torch.cat(predictions, dim=0)\n",
    "            true_labels = torch.cat(true_labels, dim=0)\n",
    "            print(predictions)\n",
    "            print(true_labels)\n",
    "            f1_micro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='micro')\n",
    "            f1_macro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n",
    "            f1_weighted = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4561 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 0.00041299325231657926\n",
      "tensor([5, 5, 5, 5, 5, 5, 5, 5])\n",
      "tensor([4, 5, 5, 0, 4, 3, 5, 5])\n",
      "Epoch 1/1, Val Loss: 0.002144333563352886, F1 Micro: 0.5, F1 Macro: 0.16666666666666666, F1 Weighted: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(model, train_loader, val_loader, criterion, optimizer, num_classes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
