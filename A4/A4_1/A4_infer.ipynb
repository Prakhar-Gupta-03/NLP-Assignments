{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:51.793271Z","iopub.status.busy":"2024-04-20T16:19:51.792312Z","iopub.status.idle":"2024-04-20T16:19:51.798966Z","shell.execute_reply":"2024-04-20T16:19:51.798044Z","shell.execute_reply.started":"2024-04-20T16:19:51.793223Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics import f1_score\n","from tqdm import tqdm, trange"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:52.401607Z","iopub.status.busy":"2024-04-20T16:19:52.401255Z","iopub.status.idle":"2024-04-20T16:19:52.405901Z","shell.execute_reply":"2024-04-20T16:19:52.404899Z","shell.execute_reply.started":"2024-04-20T16:19:52.401578Z"},"trusted":true},"outputs":[],"source":["num_classes = 7"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:52.841077Z","iopub.status.busy":"2024-04-20T16:19:52.840718Z","iopub.status.idle":"2024-04-20T16:19:52.852786Z","shell.execute_reply":"2024-04-20T16:19:52.851870Z","shell.execute_reply.started":"2024-04-20T16:19:52.841048Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, data, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = self.process(data, tokenizer)\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data['input'])\n","\n","    def __getitem__(self, index):\n","        #tokenize the input\n","        input = self.data['input'][index]\n","        target = self.data['target'][index]\n","        encoding = self.tokenizer(input, return_tensors='pt', padding='max_length', max_length=self.max_len, truncation=True)\n","        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'target': torch.tensor(target)}\n","        \n","    def process(self, data, tokenizer):\n","        new_data = {'input': [], 'target': []}\n","        for i in range(len(data)):\n","            newinput = data.loc[i].copy()\n","            stringlist = []\n","            for j in range(len(newinput['utterances'])):\n","                stringlist.append(newinput['speakers'][j] + ': ' + newinput['utterances'][j])\n","            for j in range(len(newinput['utterances'])):\n","                #join till jth utterance\n","                temp = ' '.join(stringlist[:j])\n","                temp += '</s></s>'\n","                #add jth utterance\n","                temp += newinput['speakers'][j] + ': ' + newinput['utterances'][j]\n","                temp += '</s></s>'\n","                #add later utterances\n","                temp += ' '.join(stringlist[j+1:])\n","                temp = '<s> ' + temp + '</s>'\n","                new_data['input'].append(temp)\n","                target = [0]*num_classes\n","                target[emotion_to_idx[newinput['emotions'][j]]] = 1.0\n","                new_data['target'].append(target)\n","        return new_data"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:53.266996Z","iopub.status.busy":"2024-04-20T16:19:53.266636Z","iopub.status.idle":"2024-04-20T16:19:53.273463Z","shell.execute_reply":"2024-04-20T16:19:53.272397Z","shell.execute_reply.started":"2024-04-20T16:19:53.266968Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    input_ids = []\n","    attention_masks = []\n","    targets = []\n","    for i in batch:\n","        input_ids.append(i['input_ids'])\n","        attention_masks.append(i['attention_mask'])\n","        targets.append(i['target'])\n","    input_ids = torch.stack(input_ids, dim=0)\n","    attention_masks = torch.stack(attention_masks, dim=0)\n","    targets = torch.stack(targets, dim=0)\n","    return {\n","        'input_ids': input_ids,\n","        'attention_masks': attention_masks,\n","        'labels': targets\n","    }"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:53.660796Z","iopub.status.busy":"2024-04-20T16:19:53.660431Z","iopub.status.idle":"2024-04-20T16:19:53.682719Z","shell.execute_reply":"2024-04-20T16:19:53.681803Z","shell.execute_reply.started":"2024-04-20T16:19:53.660768Z"},"trusted":true},"outputs":[],"source":["# input csv\n","test_data = pd.read_json('/kaggle/input/nlp-a4infer/val_file.json')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:54.200463Z","iopub.status.busy":"2024-04-20T16:19:54.199594Z","iopub.status.idle":"2024-04-20T16:19:54.208183Z","shell.execute_reply":"2024-04-20T16:19:54.207253Z","shell.execute_reply.started":"2024-04-20T16:19:54.200429Z"},"trusted":true},"outputs":[],"source":["test_data['utterances'] = test_data['utterances'].apply(lambda x: '@'.join(x))\n","test_data['speakers'] = test_data['speakers'].apply(lambda x: '@'.join(x))"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:54.827594Z","iopub.status.busy":"2024-04-20T16:19:54.827254Z","iopub.status.idle":"2024-04-20T16:19:54.834977Z","shell.execute_reply":"2024-04-20T16:19:54.834085Z","shell.execute_reply.started":"2024-04-20T16:19:54.827567Z"},"trusted":true},"outputs":[],"source":["test_data = test_data.drop_duplicates(subset=['speakers', 'utterances'], keep='first')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:55.280474Z","iopub.status.busy":"2024-04-20T16:19:55.280143Z","iopub.status.idle":"2024-04-20T16:19:55.289828Z","shell.execute_reply":"2024-04-20T16:19:55.288834Z","shell.execute_reply.started":"2024-04-20T16:19:55.280449Z"},"trusted":true},"outputs":[],"source":["test_data['utterances'] = test_data['utterances'].apply(lambda x: x.split('@'))\n","test_data['speakers'] = test_data['speakers'].apply(lambda x: x.split('@'))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:55.705242Z","iopub.status.busy":"2024-04-20T16:19:55.704871Z","iopub.status.idle":"2024-04-20T16:19:55.710615Z","shell.execute_reply":"2024-04-20T16:19:55.709712Z","shell.execute_reply.started":"2024-04-20T16:19:55.705215Z"},"trusted":true},"outputs":[],"source":["test_data = test_data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:56.206466Z","iopub.status.busy":"2024-04-20T16:19:56.206124Z","iopub.status.idle":"2024-04-20T16:19:56.212898Z","shell.execute_reply":"2024-04-20T16:19:56.212000Z","shell.execute_reply.started":"2024-04-20T16:19:56.206438Z"},"trusted":true},"outputs":[],"source":["unicode_mapping = {}\n","# unicode_mapping['\\u0085'] = '...' \n","# unicode_mapping['\\u0091'] = \"'\"\n","# unicode_mapping['\\u0092'] = \"'\"\n","# unicode_mapping['\\u0093'] = '\"'\n","# unicode_mapping['\\u0094'] = '\"'\n","# unicode_mapping['\\u0097'] = '--'\n","\n","# unicode_mapping['\\u2014'] = '--'\n","# unicode_mapping['\\u2019'] = \"'\"\n","# unicode_mapping['\\u2026'] = '...'\n","\n","# unicode_mapping['\\u00e9'] = 'e'\n","\n","unicode_mapping['\\x85'] = '...' \n","unicode_mapping['\\x91'] = \"'\"\n","unicode_mapping['\\x92'] = \"'\"\n","unicode_mapping['\\x93'] = '\"'\n","unicode_mapping['\\x94'] = '\"'\n","unicode_mapping['\\x97'] = '--'\n","\n","unicode_mapping['\\u2014'] = '--'\n","unicode_mapping['\\u2019'] = \"'\"\n","unicode_mapping['\\u2026'] = '...'\n","\n","unicode_mapping['\\xe9'] = 'e'"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:19:57.656467Z","iopub.status.busy":"2024-04-20T16:19:57.655795Z","iopub.status.idle":"2024-04-20T16:19:57.661724Z","shell.execute_reply":"2024-04-20T16:19:57.660724Z","shell.execute_reply.started":"2024-04-20T16:19:57.656435Z"},"trusted":true},"outputs":[],"source":["# replacing unicode characters in the data\n","def clean_utterance(utterance_list):\n","    '''\t\n","    This function takes a list of utterances and replaces the unicode with the proper characters.\n","    '''\n","    cleaned_utterances_list = []\n","    for utterance in utterance_list:\n","        for key in unicode_mapping:\n","            utterance = utterance.replace(key, unicode_mapping[key])\n","        cleaned_utterances_list.append(utterance)\n","    return cleaned_utterances_list"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:00.290230Z","iopub.status.busy":"2024-04-20T16:20:00.289832Z","iopub.status.idle":"2024-04-20T16:20:00.632227Z","shell.execute_reply":"2024-04-20T16:20:00.631439Z","shell.execute_reply.started":"2024-04-20T16:20:00.290203Z"},"trusted":true},"outputs":[],"source":["# clean the val data\n","n_val = len(test_data['utterances'])\n","for i in test_data.index:\n","    temp = test_data.loc[i].copy()\n","    cleaned_utterances = clean_utterance(temp['utterances'])\n","    temp['utterances'] = cleaned_utterances\n","    test_data.loc[i] = temp"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:01.011734Z","iopub.status.busy":"2024-04-20T16:20:01.011048Z","iopub.status.idle":"2024-04-20T16:20:01.338665Z","shell.execute_reply":"2024-04-20T16:20:01.337653Z","shell.execute_reply.started":"2024-04-20T16:20:01.011704Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:18.280203Z","iopub.status.busy":"2024-04-20T16:20:18.279505Z","iopub.status.idle":"2024-04-20T16:20:18.285224Z","shell.execute_reply":"2024-04-20T16:20:18.284186Z","shell.execute_reply.started":"2024-04-20T16:20:18.280168Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Model M1"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:19.191396Z","iopub.status.busy":"2024-04-20T16:20:19.190555Z","iopub.status.idle":"2024-04-20T16:20:19.198429Z","shell.execute_reply":"2024-04-20T16:20:19.197511Z","shell.execute_reply.started":"2024-04-20T16:20:19.191364Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    #model consists of a EmoBERTa model and a linear layer for sequence labeling task\n","    def __init__(self, num_classes):\n","        super(Model, self).__init__()\n","        self.roberta = RobertaModel.from_pretrained('roberta-base')\n","        self.fc = nn.Linear(768, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.roberta(input_ids, attention_mask)\n","        out = outputs[0]\n","        out = out[:, 0, :]\n","        out = self.fc(out)\n","        return out\n","    \n","emotion_to_idx = {'surprise': 0, 'fear': 1, 'sadness': 2, 'disgust': 3, 'anger': 4, 'neutral': 5, 'joy': 6}\n","idx_to_emotion = {0: 'surprise', 1: 'fear', 2: 'sadness', 3: 'disgust', 4: 'anger', 5: 'neutral', 6: 'joy'}\n","\n","    "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:20.711811Z","iopub.status.busy":"2024-04-20T16:20:20.711196Z","iopub.status.idle":"2024-04-20T16:20:20.962323Z","shell.execute_reply":"2024-04-20T16:20:20.961463Z","shell.execute_reply.started":"2024-04-20T16:20:20.711775Z"},"trusted":true},"outputs":[],"source":["test_dataset = Dataset(test_data, tokenizer, 128)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:31.442141Z","iopub.status.busy":"2024-04-20T16:20:31.441771Z","iopub.status.idle":"2024-04-20T16:20:36.081026Z","shell.execute_reply":"2024-04-20T16:20:36.080015Z","shell.execute_reply.started":"2024-04-20T16:20:31.442112Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# load model M1\n","\n","model = Model(num_classes)\n","# model.load_state_dict(torch.load('M1.pth', map_location=torch.device('cpu')))\n","# loading the model on the device if device is gpu\n","model.load_state_dict(torch.load('/kaggle/input/nlpa4-withmodels/M1.pth', map_location=device))\n","model.to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:49.191341Z","iopub.status.busy":"2024-04-20T16:20:49.190983Z","iopub.status.idle":"2024-04-20T16:20:49.201527Z","shell.execute_reply":"2024-04-20T16:20:49.200333Z","shell.execute_reply.started":"2024-04-20T16:20:49.191314Z"},"trusted":true},"outputs":[],"source":["def evaluate(model, val_loader, criterion, num_classes):   \n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        predictions = []\n","        true_labels = []\n","        for i, data in enumerate(tqdm(val_loader)):\n","            input_ids = data['input_ids'].to(device)\n","            attention_masks = data['attention_masks'].to(device)\n","            labels = data['labels'].to(device) #labels are one-hot encoded\n","            outputs = model(input_ids, attention_masks)\n","            loss = criterion(outputs.view(-1, num_classes), labels)\n","            val_loss += loss.item()\n","            predictions.append(torch.argmax(outputs, dim=1))\n","            true_labels.append(torch.argmax(labels, dim=1))\n","        predictions = torch.cat(predictions, dim=0)\n","        true_labels = torch.cat(true_labels, dim=0)\n","        # print(predictions)\n","        # print(true_labels)\n","        f1_micro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='micro')\n","        f1_macro = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='macro')\n","        f1_weighted = f1_score(true_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n","        val_loss /= len(val_loader)\n","        # val_losses.append(val_loss)\n","        # print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')\n","    return val_loss, f1_micro, f1_macro, f1_weighted"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:20:50.824867Z","iopub.status.busy":"2024-04-20T16:20:50.824513Z","iopub.status.idle":"2024-04-20T16:21:24.686308Z","shell.execute_reply":"2024-04-20T16:21:24.685397Z","shell.execute_reply.started":"2024-04-20T16:20:50.824839Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 874/874 [00:33<00:00, 25.84it/s]"]},{"name":"stdout","output_type":"stream","text":["Loss: 0.3063106957990335, F1 Micro: 0.884549356223176, F1 Macro: 0.8625106878464001, F1 Weighted: 0.8838961470917037\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["loss, f1_micro, f1_macro, f1_weighted = evaluate(model, test_loader, criterion, num_classes)\n","\n","print(f'Loss: {loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')"]},{"cell_type":"markdown","metadata":{},"source":["### Model M2"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:24:41.946838Z","iopub.status.busy":"2024-04-20T16:24:41.946137Z","iopub.status.idle":"2024-04-20T16:24:41.953325Z","shell.execute_reply":"2024-04-20T16:24:41.952353Z","shell.execute_reply.started":"2024-04-20T16:24:41.946806Z"},"trusted":true},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Model2, self).__init__()\n","        self.embedding = nn.Embedding(50265, 768)\n","        self.gru = nn.GRU(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(768*2, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        out = self.embedding(input_ids)\n","        out, _ = self.gru(out)\n","        out = out[:, 0, :]\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:25:52.341996Z","iopub.status.busy":"2024-04-20T16:25:52.341282Z","iopub.status.idle":"2024-04-20T16:25:53.193893Z","shell.execute_reply":"2024-04-20T16:25:53.192992Z","shell.execute_reply.started":"2024-04-20T16:25:52.341960Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Model2(\n","  (embedding): Embedding(50265, 768)\n","  (gru): GRU(768, 768, num_layers=2, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=1536, out_features=7, bias=True)\n",")"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["model2 = Model2(num_classes)\n","# loading the model on the device \n","model2.load_state_dict(torch.load('/kaggle/input/nlpa4-withmodels/M2.pth', map_location=device))\n","model2.to(device)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T16:26:19.994534Z","iopub.status.busy":"2024-04-20T16:26:19.993843Z","iopub.status.idle":"2024-04-20T16:26:40.986090Z","shell.execute_reply":"2024-04-20T16:26:40.985199Z","shell.execute_reply.started":"2024-04-20T16:26:19.994498Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 874/874 [00:20<00:00, 41.68it/s]"]},{"name":"stdout","output_type":"stream","text":["Loss: 1.1738981021487194, F1 Micro: 0.5394849785407725, F1 Macro: 0.33780431640347536, F1 Weighted: 0.4950421213811303\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["loss, f1_micro, f1_macro, f1_weighted = evaluate(model2, test_loader, criterion, num_classes)\n","\n","print(f'Loss: {loss}, F1 Micro: {f1_micro}, F1 Macro: {f1_macro}, F1 Weighted: {f1_weighted}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## TASK 2 INFERENCE"]},{"cell_type":"markdown","metadata":{},"source":["#### MODEL 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import json\n","from tqdm import tqdm\n","test = json.load(open(\"/kaggle/input/data-json/val_file.json\"))\n","\n","dialogue_ids = []\n","speaker = []\n","emotion = []\n","utterance = []\n","erf_label = []\n","\n","for i in range(len(test)):\n","    dialogue_ids.append(test[i]['episode'])\n","    speaker.append(test[i]['speakers'])\n","    emotion.append(test[i]['emotions'])\n","    utterance.append(test[i]['utterances'])\n","    erf_label.append(test[i]['triggers'])\n","df_test = pd.DataFrame(list(zip(dialogue_ids, speaker, emotion, utterance)), columns =['Dialogue_ID', 'Speaker', 'Emotion', 'Utterance'])\n","\n","unicode_mapping = {}\n","unicode_mapping['\\x85'] = '...' \n","unicode_mapping['\\x91'] = \"'\"\n","unicode_mapping['\\x92'] = \"'\"\n","unicode_mapping['\\x93'] = '\"'\n","unicode_mapping['\\x94'] = '\"'\n","unicode_mapping['\\x97'] = '--'\n","\n","unicode_mapping['\\u2014'] = '--'\n","unicode_mapping['\\u2019'] = \"'\"\n","unicode_mapping['\\u2026'] = '...'\n","\n","unicode_mapping['\\xe9'] = 'e'\n","\n","\n","def clean_utterance(utterance_list):\n","    '''\t\n","    This function takes a list of utterances and replaces the unicode with the proper characters.\n","    '''\n","    cleaned_utterances_list = []\n","    for utterance in utterance_list:\n","        for key in unicode_mapping:\n","            utterance = utterance.replace(key, unicode_mapping[key])\n","        cleaned_utterances_list.append(utterance)\n","    return cleaned_utterances_list\n","\n","test_uttr = df_test['Utterance'].apply(lambda x: clean_utterance(x))\n","df_test['Utterance'] = test_uttr\n","# print(df_test.head())\n","\n","\n","# making the dataloader\n","from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n","from transformers import AutoTokenizer, AutoModel\n","import torch.nn as nn\n","import torch\n","# import dataset\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","class Dataset(Dataset):\n","    def __init__(self, data, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = self.process(data, tokenizer)\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data['input'])\n","\n","    def __getitem__(self, index):\n","        #tokenize the input\n","        input = self.data['input'][index]\n","#         if target == 'None':\n","#             target = -1\n","#         print(target)\n","        encoding = self.tokenizer(input, return_tensors='pt', padding='max_length', max_length=self.max_len, truncation=True)\n","        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten()}\n","        \n","    def process(self, data, tokenizer):\n","        new_data = {'input': []}\n","        for i in range(len(data)):\n","            newinput = data.loc[i].copy()\n","            stringlist = []\n","            for j in range(len(newinput['Utterance'])):\n","                stringlist.append(newinput['Speaker'][j] + ': ' + newinput['Utterance'][j] + \":\" + newinput['Emotion'][j])\n","            for j in range(len(newinput['Utterance'])):\n","                #join till jth utterance\n","                temp = ' '.join(stringlist[:j])\n","                temp += '</s></s>'\n","                #add jth utterance\n","                temp += newinput['Speaker'][j] + ': ' + newinput['Utterance'][j] + \":\" + newinput['Emotion'][j]\n","                temp += '</s></s>'\n","                #add later Utterence\n","                temp += ' '.join(stringlist[j+1:])\n","                temp = '<s> ' + temp + '</s>'\n","                new_data['input'].append(temp)\n","        return new_data\n","\n","def collate_fn(batch):\n","    input_ids = []\n","    attention_mask = []\n","    target = []\n","    for b in batch:\n","        input_ids.append(b['input_ids'])\n","        attention_mask.append(b['attention_mask'])\n","    input_ids = torch.stack(input_ids)\n","    attention_mask = torch.stack(attention_mask)\n","    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n","\n","test_Dataset = Dataset(df_test, tokenizer, 2)\n","# print(test_Dataset)\n","test_loader = DataLoader(test_Dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","## MODEL 1\n","class Model(nn.Module):\n","    #model consists of a EmoBERTa model and a linear layer for sequence labeling task\n","    def __init__(self, num_classes=1):\n","        super(Model, self).__init__()\n","        self.roberta = RobertaModel.from_pretrained('roberta-base')\n","        self.fc = nn.Linear(768, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.roberta(input_ids, attention_mask)\n","        out = outputs[0]\n","        out = out[:, 0, :]\n","        out = self.fc(out)\n","        # make sure output is either 0 or 1\n","        out = torch.sigmoid(out)\n","#         out = torch.round(out)\n","        return out\n","# load model using torch\n","# model = Model(1)\n","model = torch.load('/kaggle/input/model-final/M3.pth')\n","\n","# # Load the state dictionary into the model\n","# model.load_state_dict(state_dict)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","print(device)\n","# make predictions\n","predictions = []\n","\n","for batch in tqdm(test_loader):\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    with torch.no_grad():\n","        output = model(input_ids, attention_mask)\n","    predictions.append(output)\n","    \n","# calculate f1 score\n","from sklearn.metrics import f1_score\n","import numpy as np\n","\n","# # convert predictions to numpy array\n","# predictions = torch.round(predictions)\n","\n","predictions = [int(i) for i in predictions]\n","predictions = np.array(predictions)\n","erf_labels = []\n","for i in erf_label:\n","    for j in i:\n","        if j!=1.0 and j!=0.0:\n","            erf_labels.append(0.0)\n","        else:\n","            erf_labels.append(j)\n","\n","print(len(erf_labels))\n","\n","\n","f1_micro = f1_score(predictions, erf_labels, average='micro')\n","f1_macro = f1_score(predictions, erf_labels, average='macro')\n","f1_weighted = f1_score(predictions, erf_labels, average='weighted')\n","print(\"RESULTS FOR M3\")\n","print(\"F1 MICRO for model M3:\",f1_micro)\n","print(\"F1 MACRO for model M3:\",f1_macro)\n","print(\"F1 WEIGHTED for model M3:\",f1_weighted)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### MODEL 4"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Model2(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Model2, self).__init__()\n","        self.embedding = nn.Embedding(50265, 500)\n","        self.gru = nn.GRU(500, 500, num_layers=1, batch_first=False, bidirectional=True)\n","        self.fc = nn.Linear(500*2, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        out = self.embedding(input_ids)\n","        out, _ = self.gru(out)\n","        out = out[:, 0, :]\n","        out = self.fc(out)\n","        out = torch.sigmoid(out)\n","#         out = torch.round(out)\n","        return out\n","    \n","model2 = torch.load('/kaggle/input/model-final/M4.pth')\n","\n","# Load the state dictionary into the model\n","# model.load_state_dict(state_dict)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model2.to(device)\n","print(device)\n","# make predictions\n","predictions = []\n","\n","for batch in tqdm(test_loader):\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    with torch.no_grad():\n","        output = model2(input_ids, attention_mask)\n","    predictions.append(output)\n","    \n","# calculate f1 score\n","from sklearn.metrics import f1_score\n","import numpy as np\n","\n","# convert predictions to numpy array\n","predictions = [int(i) for i in predictions]\n","predictions = np.array(predictions)\n","# print(predictions)\n","erf_labels = []\n","for i in erf_label:\n","    for j in i:\n","        if j!=1.0 and j!=0.0:\n","            erf_labels.append(0.0)\n","        else:\n","            erf_labels.append(j)\n","\n","print(len(erf_labels))\n","\n","\n","f1_micro = f1_score(predictions, erf_labels, average='micro')\n","f1_macro = f1_score(predictions, erf_labels, average='macro')\n","f1_weighted = f1_score(predictions, erf_labels, average='weighted')\n","print(\"RESULTS FOR M4\")\n","print(\"F1 MICRO for model M4:\",f1_micro)\n","print(\"F1 MACRO for model M4:\",f1_macro)\n","print(\"F1 WEIGHTED for model M4:\",f1_weighted)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4840102,"sourceId":8176614,"sourceType":"datasetVersion"},{"datasetId":4840113,"sourceId":8176629,"sourceType":"datasetVersion"}],"dockerImageVersionId":30703,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
