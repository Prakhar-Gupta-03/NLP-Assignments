{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, DistilBertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    model = 'BERT Base Uncased Task 2', \n",
    "    batch_size = 32, \n",
    "    learning_rate = 1e-5, \n",
    "    optimizer = 'AdamW',\n",
    "    loss_function = 'BCELoss', \n",
    "    epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open('train_file.json'))\n",
    "test = json.load(open('val_file.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size:', len(train))\n",
    "print('Test size:', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    dialogue_ids = []\n",
    "    speaker = []\n",
    "    emotion = []\n",
    "    utterance = []\n",
    "    erf_label = []\n",
    "    for i in range(len(data)):\n",
    "        dialogue_ids.append(data[i]['episode'])\n",
    "        speaker.append(data[i]['speakers'])\n",
    "        emotion.append(data[i]['emotions'])\n",
    "        utterance.append(data[i]['utterances'])\n",
    "        erf_label.append(data[i]['triggers'])\n",
    "    df_data = pd.DataFrame(list(zip(dialogue_ids, speaker, emotion, utterance, erf_label)), columns =['Dialogue_ID', 'Speaker', 'Emotion', 'Utterance', 'ERF_Label'])\n",
    "    return df_data\n",
    "\n",
    "df_train = prepare_data(train)\n",
    "df_test = prepare_data(test)\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_mapping = {}\n",
    "unicode_mapping['\\x85'] = '...' \n",
    "unicode_mapping['\\x91'] = \"'\"\n",
    "unicode_mapping['\\x92'] = \"'\"\n",
    "unicode_mapping['\\x93'] = '\"'\n",
    "unicode_mapping['\\x94'] = '\"'\n",
    "unicode_mapping['\\x97'] = '--'\n",
    "\n",
    "unicode_mapping['\\u2014'] = '--'\n",
    "unicode_mapping['\\u2019'] = \"'\"\n",
    "unicode_mapping['\\u2026'] = '...'\n",
    "\n",
    "unicode_mapping['\\xe9'] = 'e'\n",
    "\n",
    "def clean_utterance(utterance_list):\n",
    "    '''\t\n",
    "    This function takes a list of utterances and replaces the unicode with the proper characters.\n",
    "    input: list of utterances\n",
    "    output: list of cleaned utterances\n",
    "    '''\n",
    "    cleaned_utterances_list = []\n",
    "    for utterance in utterance_list:\n",
    "        for key in unicode_mapping:\n",
    "            utterance = utterance.replace(key, unicode_mapping[key])\n",
    "        cleaned_utterances_list.append(utterance)\n",
    "    return cleaned_utterances_list\n",
    "\n",
    "train_uttr = df_train['Utterance'].apply(lambda x: clean_utterance(x))\n",
    "df_train['Utterance'] = train_uttr\n",
    "test_uttr = df_test['Utterance'].apply(lambda x: clean_utterance(x))\n",
    "df_test['Utterance'] = test_uttr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erf_labelling(data):\n",
    "    '''\t\n",
    "    This function takes a list of dialogue, and labels the utterances with ERF labels.\n",
    "    input: list of dialogue\n",
    "    output: list of dialogue with ERF labels\n",
    "    '''\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data['ERF_Label'][i])):\n",
    "            if data['ERF_Label'][i][j] != 1.0 and data['ERF_Label'][i][j] != 0.0:\n",
    "                data['ERF_Label'][i][j] = 0.0\n",
    "    return data\n",
    "\n",
    "df_train = erf_labelling(df_train)\n",
    "df_test = erf_labelling(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['Utterance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length episode\n",
    "max_len = 0\n",
    "for i in range(len(df_train)):\n",
    "    max_len = max(max_len, len(df_train['Utterance'][i]))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.process(data, tokenizer)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #tokenize the input\n",
    "        input = self.data['input'][index]\n",
    "        target = self.data['target'][index]\n",
    "        encoding = self.tokenizer(input, return_tensors='pt', padding='max_length', max_length=self.max_len, truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'target':torch.tensor(target)}\n",
    "        \n",
    "    def process(self, data, tokenizer):\n",
    "        new_data = {'input': [], 'target': []}\n",
    "        for i in range(len(data)):\n",
    "            newinput = data.loc[i].copy()\n",
    "            stringlist = []\n",
    "            for j in range(len(newinput['Utterance'])):\n",
    "                stringlist.append(newinput['Speaker'][j] + ': ' + newinput['Utterance'][j] + \":\" + newinput['Emotion'][j])\n",
    "            for j in range(len(newinput['Utterance'])):\n",
    "                #join till jth utterance\n",
    "                temp = ' '.join(stringlist[:j])\n",
    "                temp += '</s></s>'\n",
    "                #add jth utterance\n",
    "                temp += newinput['Speaker'][j] + ': ' + newinput['Utterance'][j] + \":\" + newinput['Emotion'][j]\n",
    "                temp += '</s></s>'\n",
    "                #add later Utterence\n",
    "                temp += ' '.join(stringlist[j+1:])\n",
    "                temp = '<s> ' + temp + '</s>'\n",
    "                new_data['input'].append(temp)\n",
    "                target = newinput['ERF_Label'][j]\n",
    "                # print(target)\n",
    "                new_data['target'].append(target)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    target = []\n",
    "    for b in batch:\n",
    "        input_ids.append(b['input_ids'])\n",
    "        attention_mask.append(b['attention_mask'])\n",
    "        target.append(b['target'])\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "    target = torch.stack(target)\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target': target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Dataset = Dataset(df_train, tokenizer, 2)\n",
    "train_loader = DataLoader(train_Dataset, batch_size=model_config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "test_Dataset = Dataset(df_test, tokenizer, 2)\n",
    "test_loader = DataLoader(test_Dataset, batch_size=model_config['batch_size'], shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    This class defines the model architecture using BERT.\n",
    "    It utilizes the BERT model's encoder to get the embeddings of the input text \n",
    "    and then passes it through a linear layer to get the output. \n",
    "    The linear layer acts as a classifier head to encodings taken from the BERT model.\n",
    "    '''\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.bart = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(self.bart.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart(input_ids, attention_mask)\n",
    "        out = outputs[0]\n",
    "        out = out[:, 0, :]\n",
    "        out = self.classifier(out)\n",
    "        # make sure output is either 0 or 1\n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=model_config['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"f669722ccde3fc9df322c58c0943f6a8cd01a084\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='assignment-4', entity='nlp-assignments', config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, minibatch):\n",
    "    model.train()\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = criterion(output.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        minibatch_log = {\n",
    "            'train_minibatch_loss' : loss.item(), \n",
    "            'minibatch' : minibatch\n",
    "        }\n",
    "        wandb.log(minibatch_log)\n",
    "        minibatch += 1\n",
    "    return minibatch, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, optimizer, criterion, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        targets = []\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = criterion(output.view(-1), target)\n",
    "            val_loss += loss.item()\n",
    "            outputs.extend(output.cpu().numpy())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "    outputs = np.array(outputs)\n",
    "    targets = np.array(targets)\n",
    "    outputs = np.round(outputs)\n",
    "    micro_f1 = f1_score(targets, outputs, average='micro')\n",
    "    macro_f1 = f1_score(targets, outputs, average='macro')\n",
    "    weighted_f1 = f1_score(targets, outputs, average='weighted')    \n",
    "    val_loss /= len(dataloader)\n",
    "    return val_loss, micro_f1, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader,criterion,optimizer,num_class,epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    minibatch = 1\n",
    "    wandb.define_metric('epoch')\n",
    "    wandb.define_metric('train_epoch_loss', step_metric='epoch')\n",
    "    wandb.define_metric('val_epoch_loss', step_metric='epoch')\n",
    "    wandb.define_metric('f1_micro', step_metric='epoch')\n",
    "    wandb.define_metric('f1_macro', step_metric='epoch')\n",
    "    wandb.define_metric('f1_weighted', step_metric='epoch')\n",
    "    wandb.define_metric('train_f1_micro', step_metric='epoch')\n",
    "    wandb.define_metric('train_f1_macro', step_metric='epoch')\n",
    "    wandb.define_metric('train_f1_weighted', step_metric='epoch')\n",
    "    wandb.define_metric('minibatch')\n",
    "    wandb.define_metric('train_minibatch_loss', step_metric='minibatch')\n",
    "    for epoch in range(epochs):\n",
    "        minibatch, model = train_epoch(model, train_loader, criterion, optimizer, minibatch)\n",
    "        val_loss, val_f1_micro, val_f1_macro, val_f1_weighted = evaluation(model, test_loader, optimizer, criterion, epoch)\n",
    "        train_loss, train_f1_micro, train_f1_macro, train_f1_weighted = evaluation(model, train_loader, optimizer, criterion, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(val_loss)\n",
    "        log_dict = {\n",
    "        'epoch': epoch,\n",
    "        'train_epoch_loss': train_loss,\n",
    "        'val_epoch_loss': val_loss,\n",
    "        'f1_micro': val_f1_micro,\n",
    "        'f1_macro': val_f1_macro,\n",
    "        'f1_weighted': val_f1_weighted,\n",
    "        'train_f1_micro': train_f1_micro,\n",
    "        'train_f1_macro': train_f1_macro,\n",
    "        'train_f1_weighted': train_f1_weighted\n",
    "        }\n",
    "        wandb.log(log_dict)\n",
    "    return train_losses,test_losses         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = train(model,train_loader,test_loader,criterion,optimizer,1,3)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'Task2_BERT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss,label):\n",
    "    plt.plot(loss, label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(test_losses,'val')\n",
    "plot_loss(train_losses,'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpA4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
