{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7801151,"sourceType":"datasetVersion","datasetId":4567922}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Importing required libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image as im\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset, SubsetRandomSampler\nfrom torchvision import utils\nfrom torchvision.transforms import v2 as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom torchtext.vocab import GloVe","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:39.570878Z","iopub.execute_input":"2024-03-10T13:16:39.571338Z","iopub.status.idle":"2024-03-10T13:16:39.581136Z","shell.execute_reply.started":"2024-03-10T13:16:39.571303Z","shell.execute_reply":"2024-03-10T13:16:39.579527Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"with open('NER_TRAIN_JUDGEMENT.json') as file:\n    data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:39.974028Z","iopub.execute_input":"2024-03-10T13:16:39.975285Z","iopub.status.idle":"2024-03-10T13:16:40.625053Z","shell.execute_reply.started":"2024-03-10T13:16:39.975239Z","shell.execute_reply":"2024-03-10T13:16:40.623598Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"with open('NER_TEST_JUDGEMENT.json') as file:\n    test_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:40.627133Z","iopub.execute_input":"2024-03-10T13:16:40.627518Z","iopub.status.idle":"2024-03-10T13:16:40.653193Z","shell.execute_reply.started":"2024-03-10T13:16:40.627487Z","shell.execute_reply":"2024-03-10T13:16:40.651743Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train, val = train_test_split(data, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:40.654851Z","iopub.execute_input":"2024-03-10T13:16:40.655184Z","iopub.status.idle":"2024-03-10T13:16:40.666478Z","shell.execute_reply.started":"2024-03-10T13:16:40.655156Z","shell.execute_reply":"2024-03-10T13:16:40.664990Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"#### BIO Encoding of data","metadata":{}},{"cell_type":"code","source":"unique_labels = set()\nfor i in range(len(data)):\n    for annotation in data[i]['annotations'][0]['result']:\n        label = annotation['value']['labels'][0]\n        unique_labels.add(label)\nprint(unique_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:41.325955Z","iopub.execute_input":"2024-03-10T13:16:41.326377Z","iopub.status.idle":"2024-03-10T13:16:41.350156Z","shell.execute_reply.started":"2024-03-10T13:16:41.326344Z","shell.execute_reply":"2024-03-10T13:16:41.348909Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"{'PROVISION', 'STATUTE', 'GPE', 'OTHER_PERSON', 'ORG', 'CASE_NUMBER', 'RESPONDENT', 'COURT', 'DATE', 'PRECEDENT', 'WITNESS', 'PETITIONER', 'JUDGE'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Cleaning Data - Replacing escape sequences with spaces","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    special_chars = ['\\x05', '\\t', '\\n', '\\x0c', '\\x11', '\\x12', '\\x13', '\\x14', '\\x16', '\\x1a', '\\x80', '\\x9d', '\\xa0', '\\xad', '\\uf076']\n    for char in special_chars:\n        text = text.replace(char, ' ')\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:42.288584Z","iopub.execute_input":"2024-03-10T13:16:42.289033Z","iopub.status.idle":"2024-03-10T13:16:42.295727Z","shell.execute_reply.started":"2024-03-10T13:16:42.288999Z","shell.execute_reply":"2024-03-10T13:16:42.294250Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"#### Storing all the label boundaries in a list","metadata":{}},{"cell_type":"code","source":"def border_index(annotations):\n    border_indices = []\n    for annotation in annotations[0]['result']:\n        start = annotation['value']['start']\n        end = annotation['value']['end']\n        label = annotation['value']['labels'][0]\n        label = label.upper()\n        border_indices.append([start, end, label])\n    border_indices.sort(key=lambda x: x[0])\n    return border_indices","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:43.517571Z","iopub.execute_input":"2024-03-10T13:16:43.518125Z","iopub.status.idle":"2024-03-10T13:16:43.528045Z","shell.execute_reply.started":"2024-03-10T13:16:43.518084Z","shell.execute_reply":"2024-03-10T13:16:43.526422Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"#### Adding spaces on the boundaries of labels where there is no space","metadata":{}},{"cell_type":"code","source":"def border_spacing(text, border_indices):\n    i = 0\n    while i < len(text):\n        for border in border_indices:\n            if (i==border[0] or i==border[1]):\n                index = border_indices.index(border)\n                if (i==border[0] and i!=0 and text[i-1]!=' '):\n                    text = text[:i] + ' ' + text[i:]\n                    for j in range(index, len(border_indices)):\n                        if border_indices[j][0] >= i:\n                            border_indices[j][0] += 1\n                        if border_indices[j][1] >= i:\n                            border_indices[j][1] += 1\n                if (i==border[1] and i!=len(text)-1 and text[i]!=' '):\n                    text = text[:i] + ' ' + text[i:]\n                    for j in range(index, len(border_indices)):\n                        if border_indices[j][0] >= i:\n                            border_indices[j][0] += 1\n                        if border_indices[j][1] >= i:\n                            border_indices[j][1] += 1\n                i += 1\n        i += 1\n    return text, border_indices","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:44.295243Z","iopub.execute_input":"2024-03-10T13:16:44.295649Z","iopub.status.idle":"2024-03-10T13:16:44.310656Z","shell.execute_reply.started":"2024-03-10T13:16:44.295618Z","shell.execute_reply":"2024-03-10T13:16:44.309218Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"#### Performing Tokenization by space and BIO Encoding","metadata":{}},{"cell_type":"code","source":"def bio_encoding(text, border_indices):\n    tokens = text.split()\n    labels = ['O'] * len(tokens)\n    for annotation in border_indices:\n        start = annotation[0]\n        end = annotation[1]\n        label = annotation[2]\n        label = label.upper()\n        label_start_token = None\n        label_end_token = None\n        curr_token_index = 0\n        i = 0\n        while (i < len(text)):\n            if (text[i] == ' '):\n                i += 1\n            else:\n                curr_word = ''\n                if (i == start):\n                    label_start_token = curr_token_index\n                    while (i < end):\n                        current_word = ''\n                        if (text[i] == ' '):\n                            while (text[i] == ' '):\n                                i += 1\n                        else: \n                            while (i < len(text) and text[i] != ' '):\n                                current_word += text[i]\n                                i += 1\n                            if (tokens[curr_token_index] == current_word):\n                                curr_token_index += 1\n                    label_end_token = curr_token_index\n                else: \n                    while (i < len(text) and text[i] != ' '):\n                        curr_word += text[i]\n                        i += 1\n                    if (tokens[curr_token_index] == curr_word):\n                        curr_token_index += 1\n        if (label_end_token == None):\n            label_end_token = len(tokens) - 1\n        if (label_start_token == None):\n            continue\n        for i in range(label_start_token, label_end_token):\n            if i == label_start_token:\n                labels[i] = 'B_' + label\n            else:\n                labels[i] = 'I_' + label\n    return labels","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:45.063088Z","iopub.execute_input":"2024-03-10T13:16:45.063506Z","iopub.status.idle":"2024-03-10T13:16:45.078486Z","shell.execute_reply.started":"2024-03-10T13:16:45.063476Z","shell.execute_reply":"2024-03-10T13:16:45.077297Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"#### BIO Encoding of data","metadata":{}},{"cell_type":"code","source":"def convert_to_bio(data):\n    processed_data = {}\n    for i in range(len(data)):\n        id = data[i]['id']\n        annotations = data[i]['annotations']\n        text = data[i]['data']['text']\n        text = clean_text(text)\n        border_indices = border_index(annotations)\n        text, border_indices = border_spacing(text, border_indices)\n        labels = bio_encoding(text, border_indices)\n        processed_data[id] = {'text': text, 'labels': labels}\n    return processed_data","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:45.782892Z","iopub.execute_input":"2024-03-10T13:16:45.783330Z","iopub.status.idle":"2024-03-10T13:16:45.791330Z","shell.execute_reply.started":"2024-03-10T13:16:45.783298Z","shell.execute_reply":"2024-03-10T13:16:45.789746Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"#### Saving the data","metadata":{}},{"cell_type":"code","source":"processed_train = convert_to_bio(train)\nprocessed_val = convert_to_bio(val)\nprocessed_test = convert_to_bio(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:46.648181Z","iopub.execute_input":"2024-03-10T13:16:46.648594Z","iopub.status.idle":"2024-03-10T13:16:50.529917Z","shell.execute_reply.started":"2024-03-10T13:16:46.648563Z","shell.execute_reply":"2024-03-10T13:16:50.528751Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# dumping the processed data\nwith open('NER_TRAIN_JUDGEMENT_PROCESSED.json', 'w') as file:\n    json.dump(processed_train, file, indent=4)\nwith open('NER_VAL_JUDGEMENT_PROCESSED.json', 'w') as file:\n    json.dump(processed_val, file, indent=4)\nwith open('NER_TEST_JUDGEMENT_PROCESSED.json', 'w') as file:\n    json.dump(processed_test, file, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:50.531676Z","iopub.execute_input":"2024-03-10T13:16:50.531995Z","iopub.status.idle":"2024-03-10T13:16:51.014780Z","shell.execute_reply.started":"2024-03-10T13:16:50.531969Z","shell.execute_reply":"2024-03-10T13:16:51.013055Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"#### Loading data","metadata":{}},{"cell_type":"code","source":"with open('NER_TRAIN_JUDGEMENT_PROCESSED.json') as file:\n    train_data = json.load(file)\nwith open('NER_VAL_JUDGEMENT_PROCESSED.json') as file:\n    val_data = json.load(file)\nwith open('NER_TEST_JUDGEMENT_PROCESSED.json') as file:\n    test_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:51.016463Z","iopub.execute_input":"2024-03-10T13:16:51.016982Z","iopub.status.idle":"2024-03-10T13:16:51.126994Z","shell.execute_reply.started":"2024-03-10T13:16:51.016938Z","shell.execute_reply":"2024-03-10T13:16:51.125868Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"#### Preparing the data","metadata":{}},{"cell_type":"code","source":"# plotting data distribution of number of tokens in each sentence\nimport matplotlib.pyplot as plt\nlabel_counts = []\nfor key in train_data.keys():\n    label_counts.append(len(train_data[key]['labels']))\nplt.hist(label_counts, bins=30, alpha=0.5, color='b', edgecolor='black', linewidth=1.2, histtype='bar', align='mid', orientation='vertical', rwidth=0.8, label='Number of tokens')\nplt.title('Distribution of number of tokens in each sentence')\nplt.xlabel('Number of tokens')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:51.129634Z","iopub.execute_input":"2024-03-10T13:16:51.130231Z","iopub.status.idle":"2024-03-10T13:16:51.524560Z","shell.execute_reply.started":"2024-03-10T13:16:51.130187Z","shell.execute_reply":"2024-03-10T13:16:51.523396Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"train, val, test = [], [], []\nfor i in train_data:\n    train.append([train_data[i]['text'], train_data[i]['labels']])\nfor i in val_data:\n    val.append([val_data[i]['text'], val_data[i]['labels']])\nfor i in test_data:\n    test.append([test_data[i]['text'], test_data[i]['labels']])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:51.526262Z","iopub.execute_input":"2024-03-10T13:16:51.526621Z","iopub.status.idle":"2024-03-10T13:16:51.550659Z","shell.execute_reply.started":"2024-03-10T13:16:51.526592Z","shell.execute_reply":"2024-03-10T13:16:51.549011Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"print(f'Training data size: {len(train)}')\nprint(f'Validation data size: {len(val)}')\nprint(f'Test data size: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:53.663551Z","iopub.execute_input":"2024-03-10T13:16:53.664059Z","iopub.status.idle":"2024-03-10T13:16:53.670169Z","shell.execute_reply.started":"2024-03-10T13:16:53.664023Z","shell.execute_reply":"2024-03-10T13:16:53.668896Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Training data size: 8019\nValidation data size: 1416\nTest data size: 949\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n    text, labels = train[i][0], train[i][1]\n    print(f\"Text: {text}\\nLabels: {labels}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:54.862023Z","iopub.execute_input":"2024-03-10T13:16:54.862422Z","iopub.status.idle":"2024-03-10T13:16:54.870428Z","shell.execute_reply.started":"2024-03-10T13:16:54.862394Z","shell.execute_reply":"2024-03-10T13:16:54.868790Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Text: Therefore, while interpreting statutory provisions, the courts should keep in mind the objectives or purpose for which statute has been enacted.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The petitioner in W.P.No.15821 of 2008 was never considered for appointment under the National Rural Employment Guarantee Scheme either through Employment Exchange sponsorship or by Outsourcing Agencies.\nLabels: ['O', 'O', 'O', 'B_CASE_NUMBER', 'I_CASE_NUMBER', 'I_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The factum of accident, allegation of rash and negligent driving causing death of Sukendra Pal Singh were denied.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_OTHER_PERSON', 'I_OTHER_PERSON', 'I_OTHER_PERSON', 'O', 'O']\n\nText: ..36..    W.A.No.655/2012 & others Meaning thereby that except interview by the Commission, entire procedure for recruitment as emergency appointment was followed.\nLabels: ['O', 'B_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The law on this issue is well settled and the law is that though the provisions of Evidence Act are not applicable, but in a given situation the help of the principles of Evidence Act in the proceedings before the assessing authorities can be taken.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data = train + val + test\n# Finding number of unique words in the dataset\nword_count = {}\nfor i in range(len(data)):\n    words = data[i][0].split()\n    for word in words:\n        if word not in word_count:\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\nprint(f\"Number of unique words in the dataset: {len(word_count)}\")\nprint(f\"Words and their counts: {list(word_count.items())[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:55.494707Z","iopub.execute_input":"2024-03-10T13:16:55.495162Z","iopub.status.idle":"2024-03-10T13:16:55.702970Z","shell.execute_reply.started":"2024-03-10T13:16:55.495126Z","shell.execute_reply":"2024-03-10T13:16:55.701727Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset: 37866\nWords and their counts: [('Therefore,', 97), ('while', 164), ('interpreting', 6), ('statutory', 76), ('provisions,', 10)]\n","output_type":"stream"}]},{"cell_type":"code","source":"word_list = list(word_count.keys())\n# adding 'PAD' and 'UNK' to the word list\nword_list.append('PAD')\nword_list.append('UNK')\nword_count['PAD'] = 0\nword_count['UNK'] = 0\nprint(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:57.809502Z","iopub.execute_input":"2024-03-10T13:16:57.809952Z","iopub.status.idle":"2024-03-10T13:16:57.819119Z","shell.execute_reply.started":"2024-03-10T13:16:57.809917Z","shell.execute_reply":"2024-03-10T13:16:57.817785Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset after adding 'PAD' and 'UNK': 37868\n","output_type":"stream"}]},{"cell_type":"code","source":"# Word-to-index and index-to-word mapping from the dataset\nword_to_index = {word:idx for idx, word in enumerate(word_list)}\nindex_to_word = {idx:word for word, idx in word_to_index.items()}\nprint(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\nprint(f\"Index-to-word: {list(index_to_word.items())[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:16:59.551469Z","iopub.execute_input":"2024-03-10T13:16:59.552849Z","iopub.status.idle":"2024-03-10T13:16:59.586936Z","shell.execute_reply.started":"2024-03-10T13:16:59.552802Z","shell.execute_reply":"2024-03-10T13:16:59.585635Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Word-to-index: [('Therefore,', 0), ('while', 1), ('interpreting', 2), ('statutory', 3), ('provisions,', 4)]\nIndex-to-word: [(0, 'Therefore,'), (1, 'while'), (2, 'interpreting'), (3, 'statutory'), (4, 'provisions,')]\n","output_type":"stream"}]},{"cell_type":"code","source":"# finding all the unique labels in the dataset\nlabel_count = {}\nfor i in range(len(data)):\n    labels = data[i][1]\n    for label in labels:\n        if label not in label_count:\n            label_count[label] = 1\n        else:\n            label_count[label] += 1\nprint(f\"Number of unique labels in the dataset: {len(label_count)}\")\nprint(f\"Labels and their counts: {list(label_count.items())}\")\nprint(f\"Labels and their counts: {list(label_count.items())}\")\nlabel_to_idx = {label:idx for idx, label in enumerate(label_count.keys())}\nidx_to_label = {idx:label for label, idx in label_to_idx.items()}\nprint(f\"Label-to-index: {list(label_to_idx.items())}\")\nprint(f\"Index-to-label: {list(idx_to_label.items())}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:02.974681Z","iopub.execute_input":"2024-03-10T13:17:02.975143Z","iopub.status.idle":"2024-03-10T13:17:03.095198Z","shell.execute_reply.started":"2024-03-10T13:17:02.975107Z","shell.execute_reply":"2024-03-10T13:17:03.094328Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Number of unique labels in the dataset: 27\nLabels and their counts: [('O', 294339), ('B_CASE_NUMBER', 1161), ('I_CASE_NUMBER', 3975), ('B_ORG', 1600), ('I_ORG', 2945), ('B_OTHER_PERSON', 2929), ('I_OTHER_PERSON', 2252), ('B_STATUTE', 2026), ('I_STATUTE', 3659), ('B_PROVISION', 2642), ('I_PROVISION', 4326), ('B_COURT', 1471), ('I_COURT', 2854), ('B_WITNESS', 939), ('B_PRECEDENT', 1528), ('I_PRECEDENT', 13435), ('B_DATE', 2106), ('B_PETITIONER', 473), ('I_PETITIONER', 384), ('I_WITNESS', 810), ('B_GPE', 1581), ('B_RESPONDENT', 329), ('I_RESPONDENT', 448), ('I_DATE', 1480), ('B_JUDGE', 575), ('I_JUDGE', 398), ('I_GPE', 327)]\nLabels and their counts: [('O', 294339), ('B_CASE_NUMBER', 1161), ('I_CASE_NUMBER', 3975), ('B_ORG', 1600), ('I_ORG', 2945), ('B_OTHER_PERSON', 2929), ('I_OTHER_PERSON', 2252), ('B_STATUTE', 2026), ('I_STATUTE', 3659), ('B_PROVISION', 2642), ('I_PROVISION', 4326), ('B_COURT', 1471), ('I_COURT', 2854), ('B_WITNESS', 939), ('B_PRECEDENT', 1528), ('I_PRECEDENT', 13435), ('B_DATE', 2106), ('B_PETITIONER', 473), ('I_PETITIONER', 384), ('I_WITNESS', 810), ('B_GPE', 1581), ('B_RESPONDENT', 329), ('I_RESPONDENT', 448), ('I_DATE', 1480), ('B_JUDGE', 575), ('I_JUDGE', 398), ('I_GPE', 327)]\nLabel-to-index: [('O', 0), ('B_CASE_NUMBER', 1), ('I_CASE_NUMBER', 2), ('B_ORG', 3), ('I_ORG', 4), ('B_OTHER_PERSON', 5), ('I_OTHER_PERSON', 6), ('B_STATUTE', 7), ('I_STATUTE', 8), ('B_PROVISION', 9), ('I_PROVISION', 10), ('B_COURT', 11), ('I_COURT', 12), ('B_WITNESS', 13), ('B_PRECEDENT', 14), ('I_PRECEDENT', 15), ('B_DATE', 16), ('B_PETITIONER', 17), ('I_PETITIONER', 18), ('I_WITNESS', 19), ('B_GPE', 20), ('B_RESPONDENT', 21), ('I_RESPONDENT', 22), ('I_DATE', 23), ('B_JUDGE', 24), ('I_JUDGE', 25), ('I_GPE', 26)]\nIndex-to-label: [(0, 'O'), (1, 'B_CASE_NUMBER'), (2, 'I_CASE_NUMBER'), (3, 'B_ORG'), (4, 'I_ORG'), (5, 'B_OTHER_PERSON'), (6, 'I_OTHER_PERSON'), (7, 'B_STATUTE'), (8, 'I_STATUTE'), (9, 'B_PROVISION'), (10, 'I_PROVISION'), (11, 'B_COURT'), (12, 'I_COURT'), (13, 'B_WITNESS'), (14, 'B_PRECEDENT'), (15, 'I_PRECEDENT'), (16, 'B_DATE'), (17, 'B_PETITIONER'), (18, 'I_PETITIONER'), (19, 'I_WITNESS'), (20, 'B_GPE'), (21, 'B_RESPONDENT'), (22, 'I_RESPONDENT'), (23, 'I_DATE'), (24, 'B_JUDGE'), (25, 'I_JUDGE'), (26, 'I_GPE')]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchtext.vocab import GloVe\nglove_vectors = GloVe(name='6B', dim=300)\nword_embeddings = np.zeros((len(word_list), 300))\nfor i in range(len(word_list)):\n    word = word_list[i]\n    idx = word_to_index[word]\n    if word in glove_vectors.stoi:\n        word_embeddings[idx] = glove_vectors[word]\n    else:\n        word_embeddings[idx] = glove_vectors['unk']\nprint(f\"Shape of word_embeddings: {word_embeddings.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:02:29.306363Z","iopub.execute_input":"2024-03-10T13:02:29.306826Z","iopub.status.idle":"2024-03-10T13:02:34.770524Z","shell.execute_reply.started":"2024-03-10T13:02:29.306791Z","shell.execute_reply":"2024-03-10T13:02:34.768868Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":".vector_cache/glove.6B.zip:   2%|▏         | 18.2M/862M [00:03<03:03, 4.60MB/s]   \n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# List of word vectors\nword_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\nword_vectors = np.array(word_vectors)\nprint(f\"Shape of word vectors: {word_vectors.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:29.210682Z","iopub.execute_input":"2024-03-10T13:18:29.211221Z","iopub.status.idle":"2024-03-10T13:18:29.295924Z","shell.execute_reply.started":"2024-03-10T13:18:29.211188Z","shell.execute_reply":"2024-03-10T13:18:29.294806Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Shape of word vectors: (37868, 300)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Setting up DataLoaders","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n        Padding the sequences to the maximum length sequence in the batch\n        Args:\n            batch: list of individual elements of the dataset\n        Returns:\n            {'text' : padded_texts, 'labels' : padded_labels}\n    \"\"\"\n    texts, labels = [item['text'] for item in batch], [item['labels'] for item in batch]\n    max_len = max([len(text) for text in texts])\n    padded_texts, padded_labels = [], []\n    for i in range(len(texts)):\n        text, label = texts[i], labels[i]\n        # padding text and label sequences\n        text = text + [word_to_index['PAD']] * (max_len - len(text))\n        label = label + [label_to_idx['O']] * (max_len - len(label))\n        padded_texts.append(text)\n        padded_labels.append(label)\n    return {'text': torch.tensor(padded_texts), 'labels': torch.tensor(padded_labels)}","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:29.830424Z","iopub.execute_input":"2024-03-10T13:18:29.830878Z","iopub.status.idle":"2024-03-10T13:18:29.841752Z","shell.execute_reply.started":"2024-03-10T13:18:29.830845Z","shell.execute_reply":"2024-03-10T13:18:29.840375Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"class NERDataset(Dataset):\n    \"\"\"\n        Custom Dataset to load the Laptop Review dataset\n        Args:\n            data: list of tuples (text, labels)\n            vocab_size: size of the vocabulary\n            embedding_size: size of the word embeddings\n            word_to_index: word-to-index mapping\n            index_to_word: index-to-word mapping\n            label_to_idx: label-to-index mapping\n    \"\"\"\n    def __init__(self, data, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx):\n        self.data = data\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.word_to_index = word_to_index\n        self.index_to_word = index_to_word\n        self.label_to_idx = label_to_idx \n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text, labels = self.data[idx][0], self.data[idx][1]\n        words = text.split()\n        # converting words and labels to indices\n        word_indices = [self.word_to_index[word] if word in self.word_to_index else self.word_to_index['UNK'] for word in words]\n        label_indices = [self.label_to_idx[label] for label in labels]\n        sample = {'text' : word_indices, 'labels' : label_indices}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:30.446246Z","iopub.execute_input":"2024-03-10T13:18:30.448105Z","iopub.status.idle":"2024-03-10T13:18:30.459220Z","shell.execute_reply.started":"2024-03-10T13:18:30.448055Z","shell.execute_reply":"2024-03-10T13:18:30.457970Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# constants\nvocab_size = len(word_to_index)\nembedding_size = 300","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:31.569471Z","iopub.execute_input":"2024-03-10T13:18:31.569913Z","iopub.status.idle":"2024-03-10T13:18:31.574802Z","shell.execute_reply.started":"2024-03-10T13:18:31.569879Z","shell.execute_reply":"2024-03-10T13:18:31.573585Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"train_dataset = NERDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\nval_dataset = NERDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\ntest_dataset = NERDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:33.663677Z","iopub.execute_input":"2024-03-10T13:18:33.664195Z","iopub.status.idle":"2024-03-10T13:18:33.671831Z","shell.execute_reply.started":"2024-03-10T13:18:33.664158Z","shell.execute_reply":"2024-03-10T13:18:33.670081Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:18:34.002099Z","iopub.execute_input":"2024-03-10T13:18:34.002890Z","iopub.status.idle":"2024-03-10T13:18:34.011605Z","shell.execute_reply.started":"2024-03-10T13:18:34.002837Z","shell.execute_reply":"2024-03-10T13:18:34.010086Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"for i, data in enumerate(train_dataloader, 0):\n    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n    for j in range(2):\n        text = data['text'][j]\n        labels = data['labels'][j]\n        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:50:04.471760Z","iopub.execute_input":"2024-03-10T13:50:04.472204Z","iopub.status.idle":"2024-03-10T13:50:04.491822Z","shell.execute_reply.started":"2024-03-10T13:50:04.472174Z","shell.execute_reply":"2024-03-10T13:50:04.490259Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"Batch 1\nText shape: torch.Size([32, 68])\nLabels shape: torch.Size([32, 68])\n\nText: The firm was dissolved and its business was discontinued with effect from February 1, 1948. PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O O O O O O O O O O O O B_DATE I_DATE I_DATE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n\nText: This was followed by a letter of intent dated Nov. 4, 1980 issued by the CMDA incorporating therein the terms and conditions agreed upon by the parties and in reply thereto the petitioner on behalf of the firm wrote a letter on Nov. 6, 1980 accepting the terms and conditions. PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O O O O O O O O O B_DATE I_DATE I_DATE O O O B_ORG O O O O O O O O O O O O O O O O O O O O O O O O O O B_DATE I_DATE I_DATE O O O O O O O O O O O O O O O O O O O O O O O\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task 1: RNN + GloVe","metadata":{}},{"cell_type":"markdown","source":"#### WandB Setup\n","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb   \nwandb.login(relogin=True)","metadata":{},"execution_count":74,"outputs":[{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'RNN',\n    embed_size = 300,\n    embedding = 'GloVe',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":76,"outputs":[{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprakhar432\u001b[0m (\u001b[33mnlp-assignments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0578327e230340d78738e6821699cc34","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"]},"metadata":{}}]},{"cell_type":"markdown","source":"#### Model Architecture\n","metadata":{}},{"cell_type":"code","source":"class RNNModel(nn.Module):\n    \"\"\"\n        Model architecture to perform Sequence Labeling on the Laptop Review dataset. RNN, LSTM or GRU model is initialized based on the model configuration parameters.\n        Args: \n            vocab_size: size of the vocabulary\n            embed_size: size of the word embeddings\n            hidden_size: size of the hidden state\n            pretrained_embeddings: pre-trained word embeddings\n            model_config: dictionary containing model configuration parameters\n    \"\"\"\n    def __init__(self, vocab_size, embed_size, hidden_size, pretrained_embeddings, model_config):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(embeddings=pretrained_embeddings, freeze=True)\n        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], nonlinearity=model_config['activation'], batch_first=True, dropout=model_config['dropout'])\n        if (model_config['model'] == 'LSTM'):\n            self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], batch_first=True, dropout=model_config['dropout'])\n        if (model_config['model'] == 'GRU'):\n            self.rnn = nn.GRU(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], batch_first=True, dropout=model_config['dropout'])\n        self.fc = nn.Linear(hidden_size, 27)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        output, hidden = self.rnn(x)\n        output = self.fc(output)\n        return output, hidden\n\n# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:27:43.207490Z","iopub.execute_input":"2024-03-10T13:27:43.207965Z","iopub.status.idle":"2024-03-10T13:27:43.362484Z","shell.execute_reply.started":"2024-03-10T13:27:43.207930Z","shell.execute_reply":"2024-03-10T13:27:43.361624Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"37868 37868 300 128\nInput shape: torch.Size([32, 61])\nLabel shape: torch.Size([32, 61])\nOutput shape: torch.Size([32, 61, 27])\nHidden shape: torch.Size([1, 32, 128])\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/1076253624.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  word_vectors = torch.tensor(word_vectors)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion):\n    model.eval()\n    loss = 0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for i, data in enumerate(dataloader, 0):\n            text, labels = data['text'], data['labels']\n            output, hidden = model(text)\n            output = output.view(-1, 27)\n            labels = labels.view(-1)\n            loss += criterion(output, labels).item()\n            y_true += labels.tolist()\n            y_pred += torch.argmax(output, 1).tolist()\n    accuracy = (np.array(y_true) == np.array(y_pred)).mean()\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return accuracy, macro_f1, loss","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:22.456664Z","iopub.execute_input":"2024-03-10T13:48:22.457094Z","iopub.status.idle":"2024-03-10T13:48:22.467947Z","shell.execute_reply.started":"2024-03-10T13:48:22.457062Z","shell.execute_reply":"2024-03-10T13:48:22.466950Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config):\n    wandb.define_metric('epoch')\n    wandb.define_metric('minibatch_epoch')\n    wandb.define_metric('train_loss', step_metric='epoch')\n    wandb.define_metric('val_loss', step_metric='epoch')    \n    wandb.define_metric('train_f1', step_metric='epoch')\n    wandb.define_metric('val_f1', step_metric='epoch')\n    wandb.define_metric('train_acc', step_metric='epoch')\n    wandb.define_metric('val_acc', step_metric='epoch')\n    wandb.define_metric('minibatch_loss', step_metric='minibatch_epoch')\n    wandb.define_metric('minibatch_acc', step_metric='minibatch_epoch')\n    wandb.define_metric('minibatch_f1', step_metric='minibatch_epoch')\n    minibatch = 0\n    for epoch in tqdm.tqdm(range(model_config['epochs'])):\n        model.train()\n        for i, data in enumerate(train_dataloader, 0):\n            text, labels = data['text'], data['labels']\n            optimizer.zero_grad()\n            outputs, hidden = model(text)\n            outputs = outputs.view(-1, 27)\n            labels = labels.view(-1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            y_true = labels.tolist()\n            y_pred = torch.argmax(outputs, 1).tolist()\n            minibatch_acc = (np.array(y_true) == np.array(y_pred)).mean()\n            minibatch_f1 = f1_score(y_true, y_pred, average='weighted')\n            # logging\n            log = {}\n            log[\"minibatch_epoch\"] = minibatch\n            log[\"minibatch_loss\"] = loss.item()\n            log[\"minibatch_acc\"] = minibatch_acc\n            log[\"minibatch_f1\"] = minibatch_f1\n            wandb.log(log)\n            minibatch += 1\n        # logging\n        accuracy, f1, loss = evaluate_model(model, train_dataloader, criterion)\n        epoch_log = {}\n        epoch_log[\"epoch\"] = epoch\n        epoch_log[\"train_loss\"] = loss\n        epoch_log[\"train_f1\"] = f1\n        epoch_log[\"train_acc\"] = accuracy\n        accuracy, f1, loss = evaluate_model(model, val_dataloader, criterion)\n        epoch_log[\"val_loss\"] = loss\n        epoch_log[\"val_f1\"] = f1\n        epoch_log[\"val_acc\"] = accuracy\n        wandb.log(epoch_log)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:22.955004Z","iopub.execute_input":"2024-03-10T13:48:22.955653Z","iopub.status.idle":"2024-03-10T13:48:22.973154Z","shell.execute_reply.started":"2024-03-10T13:48:22.955619Z","shell.execute_reply":"2024-03-10T13:48:22.971987Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(model, dataloader, criterion):\n    model.eval()\n    loss = 0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for i, data in enumerate(dataloader, 0):\n            text, labels = data['text'], data['labels']\n            outputs, hidden = model(text)\n            outputs = outputs.view(-1, 27)\n            labels = labels.view(-1)\n            loss += criterion(outputs, labels).item()\n            y_true += labels.tolist()\n            y_pred += torch.argmax(outputs, 1).tolist()\n    accuracy = (np.array(y_true) == np.array(y_pred)).mean()\n    precision = precision_score(y_true, y_pred, average='weighted')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    from sklearn.metrics import classification_report\n    classification_report = classification_report(y_true, y_pred)\n    return accuracy, precision, macro_f1, loss, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:25.272346Z","iopub.execute_input":"2024-03-10T13:48:25.273231Z","iopub.status.idle":"2024-03-10T13:48:25.284948Z","shell.execute_reply.started":"2024-03-10T13:48:25.273197Z","shell.execute_reply":"2024-03-10T13:48:25.283518Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'RNN_GloVe_Task1_Exp2.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 1: GRU + GloVe","metadata":{}},{"cell_type":"markdown","source":"#### WandB Setup","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb   \nwandb.login()","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprakhar432\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'GRU',\n    embed_size = 300,\n    embedding = 'GloVe',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'GRU_GloVe_Task1_Exp1.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 1: LSTM + GloVe","metadata":{}},{"cell_type":"markdown","source":"#### WandB Setup\n","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb\nwandb.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'LSTM',\n    embed_size = 300,\n    embedding = 'GloVe',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\")\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'LSTM_GloVe_Task1_Exp1.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 1: RNN + FastText","metadata":{}},{"cell_type":"markdown","source":"#### Preparing the data","metadata":{}},{"cell_type":"code","source":"train, val, test = [], [], []\nfor i in train_data:\n    train.append([train_data[i]['text'], train_data[i]['labels']])\nfor i in val_data:\n    val.append([val_data[i]['text'], val_data[i]['labels']])\nfor i in test_data:\n    test.append([test_data[i]['text'], test_data[i]['labels']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training data size: {len(train)}')\nprint(f'Validation data size: {len(val)}')\nprint(f'Test data size: {len(test)}')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Training data size: 8019\n\nValidation data size: 1416\n\nTest data size: 949\n"}]},{"cell_type":"code","source":"for i in range(5):\n    text, labels = train[i][0], train[i][1]\n    print(f\"Text: {text}\\nLabels: {labels}\\n\")","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Text: Therefore, while interpreting statutory provisions, the courts should keep in mind the objectives or purpose for which statute has been enacted.\n\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n\n\nText: The petitioner in W.P.No.15821 of 2008 was never considered for appointment under the National Rural Employment Guarantee Scheme either through Employment Exchange sponsorship or by Outsourcing Agencies.\n\nLabels: ['O', 'O', 'O', 'B_CASE_NUMBER', 'I_CASE_NUMBER', 'I_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n\n\nText: The factum of accident, allegation of rash and negligent driving causing death of Sukendra Pal Singh were denied.\n\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_OTHER_PERSON', 'I_OTHER_PERSON', 'I_OTHER_PERSON', 'O', 'O']\n\n\n\nText: ..36..    W.A.No.655/2012 & others Meaning thereby that except interview by the Commission, entire procedure for recruitment as emergency appointment was followed.\n\nLabels: ['O', 'B_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n\n\nText: The law on this issue is well settled and the law is that though the provisions of Evidence Act are not applicable, but in a given situation the help of the principles of Evidence Act in the proceedings before the assessing authorities can be taken.\n\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n\n"}]},{"cell_type":"code","source":"data = train + val + test\n# Finding number of unique words in the dataset\nword_count = {}\nfor i in range(len(data)):\n    words = data[i][0].split()\n    for word in words:\n        if word not in word_count:\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\nprint(f\"Number of unique words in the dataset: {len(word_count)}\")\nprint(f\"Words and their counts: {list(word_count.items())[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:23:54.441999Z","iopub.execute_input":"2024-03-10T13:23:54.442407Z","iopub.status.idle":"2024-03-10T13:23:54.639008Z","shell.execute_reply.started":"2024-03-10T13:23:54.442378Z","shell.execute_reply":"2024-03-10T13:23:54.637649Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset: 37866\nWords and their counts: [('Therefore,', 97), ('while', 164), ('interpreting', 6), ('statutory', 76), ('provisions,', 10)]\n","output_type":"stream"}]},{"cell_type":"code","source":"word_list = list(word_count.keys())\n# adding 'PAD' and 'UNK' to the word list\nword_list.append('PAD')\nword_list.append('UNK')\nword_count['PAD'] = 0\nword_count['UNK'] = 0\nprint(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:23:54.675434Z","iopub.execute_input":"2024-03-10T13:23:54.676098Z","iopub.status.idle":"2024-03-10T13:23:54.684939Z","shell.execute_reply.started":"2024-03-10T13:23:54.676064Z","shell.execute_reply":"2024-03-10T13:23:54.683737Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset after adding 'PAD' and 'UNK': 37868\n","output_type":"stream"}]},{"cell_type":"code","source":"# Word-to-index and index-to-word mapping from the dataset\nword_to_index = {word:idx for idx, word in enumerate(word_list)}\nindex_to_word = {idx:word for word, idx in word_to_index.items()}\nprint(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\nprint(f\"Index-to-word: {list(index_to_word.items())[:5]}\")\n# finding all the unique labels in the dataset\nlabel_count = {}\nfor i in range(len(data)):\n    labels = data[i][1]\n    for label in labels:\n        if label not in label_count:\n            label_count[label] = 1\n        else:\n            label_count[label] += 1\nprint(f\"Number of unique labels in the dataset: {len(label_count)}\")\nprint(f\"Labels and their counts: {list(label_count.items())}\")\nprint(f\"Labels and their counts: {list(label_count.items())}\")\nlabel_to_idx = {label:idx for idx, label in enumerate(label_count.keys())}\nidx_to_label = {idx:label for label, idx in label_to_idx.items()}\nprint(f\"Label-to-index: {list(label_to_idx.items())}\")\nprint(f\"Index-to-label: {list(idx_to_label.items())}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:23:56.934092Z","iopub.execute_input":"2024-03-10T13:23:56.934472Z","iopub.status.idle":"2024-03-10T13:23:57.077041Z","shell.execute_reply.started":"2024-03-10T13:23:56.934443Z","shell.execute_reply":"2024-03-10T13:23:57.075590Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Word-to-index: [('Therefore,', 0), ('while', 1), ('interpreting', 2), ('statutory', 3), ('provisions,', 4)]\nIndex-to-word: [(0, 'Therefore,'), (1, 'while'), (2, 'interpreting'), (3, 'statutory'), (4, 'provisions,')]\nNumber of unique labels in the dataset: 27\nLabels and their counts: [('O', 294339), ('B_CASE_NUMBER', 1161), ('I_CASE_NUMBER', 3975), ('B_ORG', 1600), ('I_ORG', 2945), ('B_OTHER_PERSON', 2929), ('I_OTHER_PERSON', 2252), ('B_STATUTE', 2026), ('I_STATUTE', 3659), ('B_PROVISION', 2642), ('I_PROVISION', 4326), ('B_COURT', 1471), ('I_COURT', 2854), ('B_WITNESS', 939), ('B_PRECEDENT', 1528), ('I_PRECEDENT', 13435), ('B_DATE', 2106), ('B_PETITIONER', 473), ('I_PETITIONER', 384), ('I_WITNESS', 810), ('B_GPE', 1581), ('B_RESPONDENT', 329), ('I_RESPONDENT', 448), ('I_DATE', 1480), ('B_JUDGE', 575), ('I_JUDGE', 398), ('I_GPE', 327)]\nLabels and their counts: [('O', 294339), ('B_CASE_NUMBER', 1161), ('I_CASE_NUMBER', 3975), ('B_ORG', 1600), ('I_ORG', 2945), ('B_OTHER_PERSON', 2929), ('I_OTHER_PERSON', 2252), ('B_STATUTE', 2026), ('I_STATUTE', 3659), ('B_PROVISION', 2642), ('I_PROVISION', 4326), ('B_COURT', 1471), ('I_COURT', 2854), ('B_WITNESS', 939), ('B_PRECEDENT', 1528), ('I_PRECEDENT', 13435), ('B_DATE', 2106), ('B_PETITIONER', 473), ('I_PETITIONER', 384), ('I_WITNESS', 810), ('B_GPE', 1581), ('B_RESPONDENT', 329), ('I_RESPONDENT', 448), ('I_DATE', 1480), ('B_JUDGE', 575), ('I_JUDGE', 398), ('I_GPE', 327)]\nLabel-to-index: [('O', 0), ('B_CASE_NUMBER', 1), ('I_CASE_NUMBER', 2), ('B_ORG', 3), ('I_ORG', 4), ('B_OTHER_PERSON', 5), ('I_OTHER_PERSON', 6), ('B_STATUTE', 7), ('I_STATUTE', 8), ('B_PROVISION', 9), ('I_PROVISION', 10), ('B_COURT', 11), ('I_COURT', 12), ('B_WITNESS', 13), ('B_PRECEDENT', 14), ('I_PRECEDENT', 15), ('B_DATE', 16), ('B_PETITIONER', 17), ('I_PETITIONER', 18), ('I_WITNESS', 19), ('B_GPE', 20), ('B_RESPONDENT', 21), ('I_RESPONDENT', 22), ('I_DATE', 23), ('B_JUDGE', 24), ('I_JUDGE', 25), ('I_GPE', 26)]\nIndex-to-label: [(0, 'O'), (1, 'B_CASE_NUMBER'), (2, 'I_CASE_NUMBER'), (3, 'B_ORG'), (4, 'I_ORG'), (5, 'B_OTHER_PERSON'), (6, 'I_OTHER_PERSON'), (7, 'B_STATUTE'), (8, 'I_STATUTE'), (9, 'B_PROVISION'), (10, 'I_PROVISION'), (11, 'B_COURT'), (12, 'I_COURT'), (13, 'B_WITNESS'), (14, 'B_PRECEDENT'), (15, 'I_PRECEDENT'), (16, 'B_DATE'), (17, 'B_PETITIONER'), (18, 'I_PETITIONER'), (19, 'I_WITNESS'), (20, 'B_GPE'), (21, 'B_RESPONDENT'), (22, 'I_RESPONDENT'), (23, 'I_DATE'), (24, 'B_JUDGE'), (25, 'I_JUDGE'), (26, 'I_GPE')]\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchtext.vocab import FastText\nfasttext_vectors = FastText(language=\"en\")\nword_embeddings = np.zeros((len(word_list), 300))\nfor i in range(len(word_list)):\n    word = word_list[i]\n    idx = word_to_index[word]\n    if word in glove_vectors.stoi:\n        word_embeddings[idx] = fasttext_vectors[word]\n    else:\n        word_embeddings[idx] = fasttext_vectors['unk']\nprint(f\"Shape of word_embeddings: {word_embeddings.shape}\")","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Shape of word_embeddings: (37868, 300)\n"}]},{"cell_type":"code","source":"# List of word vectors\nword_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\nword_vectors = np.array(word_vectors)\nprint(f\"Shape of word vectors: {word_vectors.shape}\")","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Shape of word vectors: (37868, 300)\n"}]},{"cell_type":"code","source":"# constants\nvocab_size = len(word_to_index)\nembedding_size = 300","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = NERDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\nval_dataset = NERDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\ntest_dataset = NERDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:19.076610Z","iopub.execute_input":"2024-03-10T13:24:19.077054Z","iopub.status.idle":"2024-03-10T13:24:19.084153Z","shell.execute_reply.started":"2024-03-10T13:24:19.077015Z","shell.execute_reply":"2024-03-10T13:24:19.082885Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:20.413537Z","iopub.execute_input":"2024-03-10T13:24:20.413987Z","iopub.status.idle":"2024-03-10T13:24:20.420836Z","shell.execute_reply.started":"2024-03-10T13:24:20.413956Z","shell.execute_reply":"2024-03-10T13:24:20.419622Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"for i, data in enumerate(train_dataloader, 0):\n    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n    for j in range(2):\n        text = data['text'][j]\n        labels = data['labels'][j]\n        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:20.868870Z","iopub.execute_input":"2024-03-10T13:24:20.869268Z","iopub.status.idle":"2024-03-10T13:24:20.901305Z","shell.execute_reply.started":"2024-03-10T13:24:20.869238Z","shell.execute_reply":"2024-03-10T13:24:20.900404Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Batch 1\nText shape: torch.Size([32, 68])\nLabels shape: torch.Size([32, 68])\n\nText: As Criminal Appeal No.472 of 1992 has been allowed by us, we do not see any reason, at this point of time, to enhance the sentence awarded to accused no.1 by the trial Court for the offence punishable under Section 498-A of IPC. PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O B_CASE_NUMBER I_CASE_NUMBER I_CASE_NUMBER I_CASE_NUMBER I_CASE_NUMBER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_PROVISION I_PROVISION O B_STATUTE O O O O O O O O O O O O O O O O O O O O O O O O O\n\nText: Even before this court there has been no controversy raised with regard to any such distinction at the Bar. PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### WandB Setup\n","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb   \nwandb.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'RNN',\n    embed_size = 300,\n    embedding = 'FastText',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'RNN_FastText_Task1_Exp1.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 1: GRU + FastText","metadata":{}},{"cell_type":"markdown","source":"#### WandB Setup","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb\nwandb.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'GRU',\n    embed_size = 300,\n    embedding = 'FastText',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'GRU_FastText_Task1_Exp1.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task 1: LSTM + FastText","metadata":{}},{"cell_type":"markdown","source":"#### WandB Setup","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb\nwandb.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'LSTM',\n    embed_size = 300,\n    embedding = 'FastText',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Architecture","metadata":{}},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\")\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'LSTM_FastText_Task1_Exp1.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preparing the data","metadata":{}},{"cell_type":"code","source":"train, val, test = [], [], []\nfor i in train_data:\n    train.append([train_data[i]['text'], train_data[i]['labels']])\nfor i in val_data:\n    val.append([val_data[i]['text'], val_data[i]['labels']])\nfor i in test_data:\n    test.append([test_data[i]['text'], test_data[i]['labels']])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:28.435622Z","iopub.execute_input":"2024-03-10T13:17:28.436036Z","iopub.status.idle":"2024-03-10T13:17:28.457241Z","shell.execute_reply.started":"2024-03-10T13:17:28.436006Z","shell.execute_reply":"2024-03-10T13:17:28.456154Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"print(f'Training data size: {len(train)}')\nprint(f'Validation data size: {len(val)}')\nprint(f'Test data size: {len(test)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:28.908616Z","iopub.execute_input":"2024-03-10T13:17:28.909036Z","iopub.status.idle":"2024-03-10T13:17:28.914755Z","shell.execute_reply.started":"2024-03-10T13:17:28.908997Z","shell.execute_reply":"2024-03-10T13:17:28.913370Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Training data size: 8019\nValidation data size: 1416\nTest data size: 949\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(5):\n    text, labels = train[i][0], train[i][1]\n    print(f\"Text: {text}\\nLabels: {labels}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:30.236665Z","iopub.execute_input":"2024-03-10T13:17:30.237512Z","iopub.status.idle":"2024-03-10T13:17:30.244844Z","shell.execute_reply.started":"2024-03-10T13:17:30.237477Z","shell.execute_reply":"2024-03-10T13:17:30.243375Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Text: Therefore, while interpreting statutory provisions, the courts should keep in mind the objectives or purpose for which statute has been enacted.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The petitioner in W.P.No.15821 of 2008 was never considered for appointment under the National Rural Employment Guarantee Scheme either through Employment Exchange sponsorship or by Outsourcing Agencies.\nLabels: ['O', 'O', 'O', 'B_CASE_NUMBER', 'I_CASE_NUMBER', 'I_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'I_ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The factum of accident, allegation of rash and negligent driving causing death of Sukendra Pal Singh were denied.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_OTHER_PERSON', 'I_OTHER_PERSON', 'I_OTHER_PERSON', 'O', 'O']\n\nText: ..36..    W.A.No.655/2012 & others Meaning thereby that except interview by the Commission, entire procedure for recruitment as emergency appointment was followed.\nLabels: ['O', 'B_CASE_NUMBER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\nText: The law on this issue is well settled and the law is that though the provisions of Evidence Act are not applicable, but in a given situation the help of the principles of Evidence Act in the proceedings before the assessing authorities can be taken.\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_STATUTE', 'I_STATUTE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data = train + val + test\n# Finding number of unique words in the dataset\nword_count = {}\nfor i in range(len(data)):\n    words = data[i][0].split()\n    for word in words:\n        if word not in word_count:\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\nprint(f\"Number of unique words in the dataset: {len(word_count)}\")\nprint(f\"Words and their counts: {list(word_count.items())[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:31.215915Z","iopub.execute_input":"2024-03-10T13:17:31.216412Z","iopub.status.idle":"2024-03-10T13:17:31.409341Z","shell.execute_reply.started":"2024-03-10T13:17:31.216374Z","shell.execute_reply":"2024-03-10T13:17:31.408161Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset: 37866\nWords and their counts: [('Therefore,', 97), ('while', 164), ('interpreting', 6), ('statutory', 76), ('provisions,', 10)]\n","output_type":"stream"}]},{"cell_type":"code","source":"word_list = list(word_count.keys())\n# adding 'PAD' and 'UNK' to the word list\nword_list.append('PAD')\nword_list.append('UNK')\nword_count['PAD'] = 0\nword_count['UNK'] = 0\nprint(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:32.389675Z","iopub.execute_input":"2024-03-10T13:17:32.390866Z","iopub.status.idle":"2024-03-10T13:17:32.401548Z","shell.execute_reply.started":"2024-03-10T13:17:32.390820Z","shell.execute_reply":"2024-03-10T13:17:32.399993Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Number of unique words in the dataset after adding 'PAD' and 'UNK': 37868\n","output_type":"stream"}]},{"cell_type":"code","source":"# Word-to-index and index-to-word mapping from the dataset\nword_to_index = {word:idx for idx, word in enumerate(word_list)}\nindex_to_word = {idx:word for word, idx in word_to_index.items()}\nlabel_to_idx = {'O': 0, 'B': 1, 'I': 2}\nprint(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\nprint(f\"Index-to-word: {list(index_to_word.items())[:5]}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:17:33.558502Z","iopub.execute_input":"2024-03-10T13:17:33.558939Z","iopub.status.idle":"2024-03-10T13:17:33.598390Z","shell.execute_reply.started":"2024-03-10T13:17:33.558906Z","shell.execute_reply":"2024-03-10T13:17:33.597035Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Word-to-index: [('Therefore,', 0), ('while', 1), ('interpreting', 2), ('statutory', 3), ('provisions,', 4)]\nIndex-to-word: [(0, 'Therefore,'), (1, 'while'), (2, 'interpreting'), (3, 'statutory'), (4, 'provisions,')]\n","output_type":"stream"}]},{"cell_type":"code","source":"import gensim\nimport torch\nimport gensim.downloader as api\nwv = api.load('word2vec-google-news-300')\n\n# Load word embeddings\nword_embeddings = []\nfor word in word_list:\n    try:\n        word_embeddings.append(wv[word])\n    except:\n        word_embeddings.append(wv['unk'])\nword_embeddings = np.array(word_embeddings)\nprint(f\"Shape of word embeddings: {word_embeddings.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:54.233213Z","iopub.execute_input":"2024-03-10T13:24:54.233643Z","iopub.status.idle":"2024-03-10T13:24:54.447115Z","shell.execute_reply.started":"2024-03-10T13:24:54.233609Z","shell.execute_reply":"2024-03-10T13:24:54.445750Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Shape of word embeddings: (37868, 300)\n","output_type":"stream"}]},{"cell_type":"code","source":"# List of word vectors\nword_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\nword_vectors = np.array(word_vectors)\nprint(f\"Shape of word vectors: {word_vectors.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:54.816963Z","iopub.execute_input":"2024-03-10T13:24:54.817380Z","iopub.status.idle":"2024-03-10T13:24:54.895628Z","shell.execute_reply.started":"2024-03-10T13:24:54.817350Z","shell.execute_reply":"2024-03-10T13:24:54.894265Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Shape of word vectors: (37868, 300)\n","output_type":"stream"}]},{"cell_type":"code","source":"# constants\nvocab_size = len(word_to_index)\nembedding_size = 300","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:57.003467Z","iopub.execute_input":"2024-03-10T13:24:57.004258Z","iopub.status.idle":"2024-03-10T13:24:57.009909Z","shell.execute_reply.started":"2024-03-10T13:24:57.004223Z","shell.execute_reply":"2024-03-10T13:24:57.008719Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train_dataset = NERDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\nval_dataset = NERDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\ntest_dataset = NERDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:57.253393Z","iopub.execute_input":"2024-03-10T13:24:57.253798Z","iopub.status.idle":"2024-03-10T13:24:57.259943Z","shell.execute_reply.started":"2024-03-10T13:24:57.253767Z","shell.execute_reply":"2024-03-10T13:24:57.258654Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:59.119320Z","iopub.execute_input":"2024-03-10T13:24:59.119775Z","iopub.status.idle":"2024-03-10T13:24:59.125789Z","shell.execute_reply.started":"2024-03-10T13:24:59.119741Z","shell.execute_reply":"2024-03-10T13:24:59.124522Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"for i, data in enumerate(train_dataloader, 0):\n    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n    for j in range(2):\n        text = data['text'][j]\n        labels = data['labels'][j]\n        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:24:59.713906Z","iopub.execute_input":"2024-03-10T13:24:59.714369Z","iopub.status.idle":"2024-03-10T13:24:59.729398Z","shell.execute_reply.started":"2024-03-10T13:24:59.714332Z","shell.execute_reply":"2024-03-10T13:24:59.727867Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Batch 1\nText shape: torch.Size([32, 54])\nLabels shape: torch.Size([32, 54])\n\nText: He knows the objector and belongs to the same State ( Kerala ). PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O O O O O O O O O O O B_GPE O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n\nText: The material placed on record fully establishes that the assessment of the petitioner's work and conduct was bad and the adverse remarks for the year 1994-95 were sufficient for the impugned action. PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\nLabels: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Task1: RNN + Word2Vec","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb   \nwandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:26:44.770317Z","iopub.execute_input":"2024-03-10T13:26:44.770729Z","iopub.status.idle":"2024-03-10T13:26:48.315503Z","shell.execute_reply.started":"2024-03-10T13:26:44.770671Z","shell.execute_reply":"2024-03-10T13:26:48.314123Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'RNN',\n    embed_size = 300,\n    embedding = 'Word2Vec',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:26:50.717818Z","iopub.execute_input":"2024-03-10T13:26:50.718222Z","iopub.status.idle":"2024-03-10T13:26:50.724967Z","shell.execute_reply.started":"2024-03-10T13:26:50.718193Z","shell.execute_reply":"2024-03-10T13:26:50.723762Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:26:53.245341Z","iopub.execute_input":"2024-03-10T13:26:53.246549Z","iopub.status.idle":"2024-03-10T13:27:24.318528Z","shell.execute_reply.started":"2024-03-10T13:26:53.246495Z","shell.execute_reply":"2024-03-10T13:27:24.317333Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahcir\u001b[0m (\u001b[33mnlp-assignments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240310_132653-9wxu3st8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8' target=\"_blank\">comfy-capybara-29</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8</a>"},"metadata":{}},{"execution_count":103,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7aa2079adf90>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:28:27.779401Z","iopub.execute_input":"2024-03-10T13:28:27.779815Z","iopub.status.idle":"2024-03-10T13:28:27.838180Z","shell.execute_reply.started":"2024-03-10T13:28:27.779783Z","shell.execute_reply":"2024-03-10T13:28:27.837244Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"37868 37868 300 128\nInput shape: torch.Size([32, 66])\nLabel shape: torch.Size([32, 66])\nOutput shape: torch.Size([32, 66, 27])\nHidden shape: torch.Size([1, 32, 128])\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  word_vectors = torch.tensor(word_vectors)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:28:29.998857Z","iopub.execute_input":"2024-03-10T13:28:29.999501Z","iopub.status.idle":"2024-03-10T13:28:30.006949Z","shell.execute_reply.started":"2024-03-10T13:28:29.999467Z","shell.execute_reply":"2024-03-10T13:28:30.005333Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:28:58.735194Z","iopub.execute_input":"2024-03-10T13:28:58.735626Z","iopub.status.idle":"2024-03-10T13:47:58.987268Z","shell.execute_reply.started":"2024-03-10T13:28:58.735590Z","shell.execute_reply":"2024-03-10T13:47:58.985902Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [19:00<00:00, 11.40s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:38.856647Z","iopub.execute_input":"2024-03-10T13:48:38.857073Z","iopub.status.idle":"2024-03-10T13:48:39.480184Z","shell.execute_reply.started":"2024-03-10T13:48:38.857044Z","shell.execute_reply":"2024-03-10T13:48:39.478786Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"Test accuracy: 0.9517\nTest precision: 0.9485\nTest macro F1: 0.5221\nTest loss: 9.7472\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98     57483\n           1       0.40      0.26      0.31       121\n           2       0.61      0.47      0.53       344\n           3       0.43      0.31      0.36       159\n           4       0.43      0.29      0.35       310\n           5       0.55      0.57      0.56       276\n           6       0.59      0.59      0.59       195\n           7       0.69      0.60      0.64       222\n           8       0.75      0.68      0.71       383\n           9       0.90      0.86      0.88       258\n          10       0.86      0.77      0.82       439\n          11       0.83      0.73      0.78       178\n          12       0.84      0.73      0.78       326\n          13       0.44      0.48      0.46        58\n          14       0.50      0.37      0.42       177\n          15       0.85      0.63      0.72      1793\n          16       0.80      0.78      0.79       222\n          17       0.25      0.44      0.32         9\n          18       0.24      0.45      0.31        11\n          19       0.55      0.39      0.46        54\n          20       0.43      0.45      0.44       183\n          21       0.22      0.40      0.29         5\n          22       0.17      0.22      0.19         9\n          23       0.69      0.67      0.68       102\n          24       0.08      0.25      0.12         8\n          25       0.27      0.57      0.36         7\n          26       0.23      0.23      0.23        47\n\n    accuracy                           0.95     63379\n   macro avg       0.54      0.53      0.52     63379\nweighted avg       0.95      0.95      0.95     63379\n\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:47.100926Z","iopub.execute_input":"2024-03-10T13:48:47.102086Z","iopub.status.idle":"2024-03-10T13:48:53.178163Z","shell.execute_reply.started":"2024-03-10T13:48:47.102044Z","shell.execute_reply":"2024-03-10T13:48:53.177222Z"},"trusted":true},"execution_count":117,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test loss</td><td>▁</td></tr><tr><td>Test macro F1</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_acc</td><td>▁▁▂▃▅▂▄▅▆▄▃▅▅▄▇▅▇▆▆▇▇▇▇▇█▇▇▇███▇██▇██▇██</td></tr><tr><td>minibatch_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_f1</td><td>▁▃▃▄▆▄▅▆▆▅▄▆▆▅▇▆▇▇▆▇▇▇▇▇█▇█████▇████████</td></tr><tr><td>minibatch_loss</td><td>█▆▆▄▃▅▄▃▃▄▄▃▃▄▂▃▂▂▃▂▂▂▂▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇███████████</td></tr><tr><td>train_f1</td><td>▁▃▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇▇███████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇█▇██████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▅▆▆▇▇█████████████████████▇▇█▇▇█▇▇█▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▅▄▅▅▅▅▅▅▅▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.9517</td></tr><tr><td>Test loss</td><td>9.74718</td></tr><tr><td>Test macro F1</td><td>0.5221</td></tr><tr><td>Test precision</td><td>0.94852</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99458</td></tr><tr><td>minibatch_epoch</td><td>25099</td></tr><tr><td>minibatch_f1</td><td>0.9946</td></tr><tr><td>minibatch_loss</td><td>0.01763</td></tr><tr><td>train_acc</td><td>0.9963</td></tr><tr><td>train_f1</td><td>0.95753</td></tr><tr><td>train_loss</td><td>3.57693</td></tr><tr><td>val_acc</td><td>0.95793</td></tr><tr><td>val_f1</td><td>0.54738</td></tr><tr><td>val_loss</td><td>11.39213</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">comfy-capybara-29</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/9wxu3st8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240310_132653-9wxu3st8/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'RNN_Word2Vec_Task1_Exp1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:48:53.179605Z","iopub.execute_input":"2024-03-10T13:48:53.179982Z","iopub.status.idle":"2024-03-10T13:48:53.246211Z","shell.execute_reply.started":"2024-03-10T13:48:53.179951Z","shell.execute_reply":"2024-03-10T13:48:53.244927Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"#### Task 1: GRU + Word2Vec","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb\nwandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:50:44.198321Z","iopub.execute_input":"2024-03-10T13:50:44.198761Z","iopub.status.idle":"2024-03-10T13:51:01.193338Z","shell.execute_reply.started":"2024-03-10T13:50:44.198728Z","shell.execute_reply":"2024-03-10T13:51:01.192007Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'GRU',\n    embed_size = 300,\n    embedding = 'Word2Vec',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:51:17.196383Z","iopub.execute_input":"2024-03-10T13:51:17.196851Z","iopub.status.idle":"2024-03-10T13:51:17.203983Z","shell.execute_reply.started":"2024-03-10T13:51:17.196815Z","shell.execute_reply":"2024-03-10T13:51:17.202641Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:51:30.294582Z","iopub.execute_input":"2024-03-10T13:51:30.295067Z","iopub.status.idle":"2024-03-10T13:52:07.431433Z","shell.execute_reply.started":"2024-03-10T13:51:30.295032Z","shell.execute_reply":"2024-03-10T13:52:07.430432Z"},"trusted":true},"execution_count":123,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113899888889339, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e46711c1284c8195b57b9a02c909df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240310_135130-y99dqwt4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4' target=\"_blank\">denim-monkey-30</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4</a>"},"metadata":{}},{"execution_count":123,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7aa2079ac640>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:52:07.433103Z","iopub.execute_input":"2024-03-10T13:52:07.433605Z","iopub.status.idle":"2024-03-10T13:52:07.519434Z","shell.execute_reply.started":"2024-03-10T13:52:07.433576Z","shell.execute_reply":"2024-03-10T13:52:07.518510Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"37868 37868 300 128\nInput shape: torch.Size([32, 68])\nLabel shape: torch.Size([32, 68])\nOutput shape: torch.Size([32, 68, 27])\nHidden shape: torch.Size([1, 32, 128])\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  word_vectors = torch.tensor(word_vectors)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:52:07.521053Z","iopub.execute_input":"2024-03-10T13:52:07.521768Z","iopub.status.idle":"2024-03-10T13:52:07.533250Z","shell.execute_reply.started":"2024-03-10T13:52:07.521734Z","shell.execute_reply":"2024-03-10T13:52:07.531668Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:52:07.535969Z","iopub.execute_input":"2024-03-10T13:52:07.537123Z","iopub.status.idle":"2024-03-10T14:25:55.001096Z","shell.execute_reply.started":"2024-03-10T13:52:07.537086Z","shell.execute_reply":"2024-03-10T14:25:54.999777Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [33:47<00:00, 20.27s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:25:55.002804Z","iopub.execute_input":"2024-03-10T14:25:55.003649Z","iopub.status.idle":"2024-03-10T14:25:55.849678Z","shell.execute_reply.started":"2024-03-10T14:25:55.003605Z","shell.execute_reply":"2024-03-10T14:25:55.847512Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Test accuracy: 0.9527\nTest precision: 0.9514\nTest macro F1: 0.5482\nTest loss: 12.0151\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.98     57483\n           1       0.48      0.43      0.45       121\n           2       0.63      0.53      0.58       344\n           3       0.34      0.32      0.33       159\n           4       0.50      0.40      0.44       310\n           5       0.57      0.53      0.55       276\n           6       0.59      0.52      0.55       195\n           7       0.64      0.65      0.65       222\n           8       0.79      0.68      0.73       383\n           9       0.86      0.88      0.87       258\n          10       0.88      0.76      0.81       439\n          11       0.81      0.75      0.78       178\n          12       0.83      0.70      0.76       326\n          13       0.35      0.50      0.41        58\n          14       0.49      0.47      0.48       177\n          15       0.84      0.69      0.76      1793\n          16       0.73      0.81      0.77       222\n          17       0.21      0.44      0.29         9\n          18       0.35      0.55      0.43        11\n          19       0.40      0.39      0.40        54\n          20       0.48      0.51      0.50       183\n          21       0.21      0.60      0.32         5\n          22       0.17      0.33      0.22         9\n          23       0.66      0.76      0.71       102\n          24       0.19      0.50      0.28         8\n          25       0.38      0.71      0.50         7\n          26       0.28      0.26      0.27        47\n\n    accuracy                           0.95     63379\n   macro avg       0.54      0.58      0.55     63379\nweighted avg       0.95      0.95      0.95     63379\n\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:25:55.851258Z","iopub.execute_input":"2024-03-10T14:25:55.851722Z","iopub.status.idle":"2024-03-10T14:26:00.842964Z","shell.execute_reply.started":"2024-03-10T14:25:55.851651Z","shell.execute_reply":"2024-03-10T14:26:00.841890Z"},"trusted":true},"execution_count":128,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test loss</td><td>▁</td></tr><tr><td>Test macro F1</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_acc</td><td>▁▃▄▅▅▄▆▆▆▇▆▆▆▆▇▇▇███████████▇▇████████▇█</td></tr><tr><td>minibatch_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_f1</td><td>▁▄▅▅▅▅▆▇▆▇▆▇▆▇▇█▇████████████▇██████████</td></tr><tr><td>minibatch_loss</td><td>█▆▄▄▃▄▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▆▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▄▄▅▅▆▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▇▇██████▇▇▇▇▇▇▇▇▇▆▇▇▇▆▆▆▇▆▇▆▆▇▆▇▆▇▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▅▆▆▇█████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▆▂▂▁▁▁▁▁▁▂▂▂▃▃▃▄▄▅▅▆▅▆▆▆▆▇▆▇▇▇▇███▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.95267</td></tr><tr><td>Test loss</td><td>12.01513</td></tr><tr><td>Test macro F1</td><td>0.54819</td></tr><tr><td>Test precision</td><td>0.95142</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99774</td></tr><tr><td>minibatch_epoch</td><td>25099</td></tr><tr><td>minibatch_f1</td><td>0.99747</td></tr><tr><td>minibatch_loss</td><td>0.00808</td></tr><tr><td>train_acc</td><td>0.99877</td></tr><tr><td>train_f1</td><td>0.98755</td></tr><tr><td>train_loss</td><td>1.1196</td></tr><tr><td>val_acc</td><td>0.95793</td></tr><tr><td>val_f1</td><td>0.55559</td></tr><tr><td>val_loss</td><td>15.17349</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">denim-monkey-30</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/y99dqwt4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240310_135130-y99dqwt4/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'GRU_Word2Vec_Task1_Exp1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:26:00.920528Z","iopub.execute_input":"2024-03-10T14:26:00.921297Z","iopub.status.idle":"2024-03-10T14:26:00.990273Z","shell.execute_reply.started":"2024-03-10T14:26:00.921260Z","shell.execute_reply":"2024-03-10T14:26:00.989120Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Task 1: LSTM + Word2Vec","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport wandb\nwandb.login(relogin=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:30:31.295403Z","iopub.execute_input":"2024-03-10T14:30:31.295873Z","iopub.status.idle":"2024-03-10T14:31:04.885752Z","shell.execute_reply.started":"2024-03-10T14:30:31.295841Z","shell.execute_reply":"2024-03-10T14:31:04.884226Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"model_config = dict(\n    task = 1, \n    model = 'LSTM',\n    embed_size = 300,\n    embedding = 'Word2Vec',\n    hidden_size = 128,\n    learning_rate = 0.001,\n    batch_size = 32,\n    epochs = 100, \n    padding = 'max_post', \n    loss = 'CrossEntropyLoss',\n    optimizer = 'Adam',\n    num_hidden = 1,\n    dropout = 0, \n    activation = 'tanh'\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:31:26.323564Z","iopub.execute_input":"2024-03-10T14:31:26.324020Z","iopub.status.idle":"2024-03-10T14:31:26.331166Z","shell.execute_reply.started":"2024-03-10T14:31:26.323988Z","shell.execute_reply":"2024-03-10T14:31:26.329861Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:31:36.384403Z","iopub.execute_input":"2024-03-10T14:31:36.384875Z","iopub.status.idle":"2024-03-10T14:32:08.291097Z","shell.execute_reply.started":"2024-03-10T14:31:36.384841Z","shell.execute_reply":"2024-03-10T14:32:08.290144Z"},"trusted":true},"execution_count":134,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240310_143136-gwfjrogd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd' target=\"_blank\">generous-leaf-31</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd</a>"},"metadata":{}},{"execution_count":134,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7aa207ac2c80>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the model\nvocab_size = len(word_to_index)\nembed_size = model_config['embed_size'] # Size of the word embeddings\nhidden_size = model_config['hidden_size'] # Size of the hidden state\nword_vectors = torch.tensor(word_vectors)\nword_vectors = word_vectors.float()\nprint(vocab_size, len(word_vectors), embed_size, hidden_size)\nmodel = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\nfor i, data in enumerate(train_dataloader, 0):\n    text = data['text']\n    print(f\"Input shape: {text.size()}\")\n    print(f\"Label shape: {data['labels'].size()}\")\n    output, hidden = model(text)\n    print(f\"Output shape: {output.size()}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:32:08.293064Z","iopub.execute_input":"2024-03-10T14:32:08.293730Z","iopub.status.idle":"2024-03-10T14:32:08.520550Z","shell.execute_reply.started":"2024-03-10T14:32:08.293680Z","shell.execute_reply":"2024-03-10T14:32:08.519278Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/36252246.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  word_vectors = torch.tensor(word_vectors)\n","output_type":"stream"},{"name":"stdout","text":"37868 37868 300 128\nInput shape: torch.Size([32, 68])\nLabel shape: torch.Size([32, 68])\nOutput shape: torch.Size([32, 68, 27])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:32:08.522230Z","iopub.execute_input":"2024-03-10T14:32:08.523101Z","iopub.status.idle":"2024-03-10T14:32:08.538042Z","shell.execute_reply.started":"2024-03-10T14:32:08.523057Z","shell.execute_reply":"2024-03-10T14:32:08.536522Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:32:08.541181Z","iopub.execute_input":"2024-03-10T14:32:08.542263Z","iopub.status.idle":"2024-03-10T15:07:30.086575Z","shell.execute_reply.started":"2024-03-10T14:32:08.542213Z","shell.execute_reply":"2024-03-10T15:07:30.085421Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [35:21<00:00, 21.22s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\nprint(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\nprint(f\"Classification report:\\n{classification_report}\")\nwandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:30.088812Z","iopub.execute_input":"2024-03-10T15:07:30.089290Z","iopub.status.idle":"2024-03-10T15:07:30.990131Z","shell.execute_reply.started":"2024-03-10T15:07:30.089247Z","shell.execute_reply":"2024-03-10T15:07:30.988745Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"Test accuracy: 0.9531\nTest precision: 0.9504\nTest macro F1: 0.5455\nTest loss: 10.7841\n\nClassification report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.99      0.98     57483\n           1       0.41      0.30      0.34       121\n           2       0.63      0.48      0.55       344\n           3       0.37      0.31      0.34       159\n           4       0.48      0.32      0.38       310\n           5       0.58      0.58      0.58       276\n           6       0.58      0.57      0.58       195\n           7       0.73      0.65      0.69       222\n           8       0.80      0.67      0.73       383\n           9       0.87      0.86      0.87       258\n          10       0.87      0.74      0.80       439\n          11       0.86      0.77      0.81       178\n          12       0.85      0.75      0.80       326\n          13       0.45      0.40      0.42        58\n          14       0.49      0.41      0.45       177\n          15       0.83      0.67      0.74      1793\n          16       0.82      0.80      0.81       222\n          17       0.25      0.56      0.34         9\n          18       0.43      0.55      0.48        11\n          19       0.55      0.39      0.46        54\n          20       0.47      0.51      0.49       183\n          21       0.20      0.40      0.27         5\n          22       0.14      0.22      0.17         9\n          23       0.62      0.69      0.65       102\n          24       0.21      0.62      0.31         8\n          25       0.21      0.57      0.31         7\n          26       0.39      0.36      0.37        47\n\n    accuracy                           0.95     63379\n   macro avg       0.56      0.56      0.55     63379\nweighted avg       0.95      0.95      0.95     63379\n\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:30.991501Z","iopub.execute_input":"2024-03-10T15:07:30.991956Z","iopub.status.idle":"2024-03-10T15:07:39.179897Z","shell.execute_reply.started":"2024-03-10T15:07:30.991923Z","shell.execute_reply":"2024-03-10T15:07:39.178916Z"},"trusted":true},"execution_count":139,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test loss</td><td>▁</td></tr><tr><td>Test macro F1</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_acc</td><td>▁▅▄▄▄▆▅▃▅▆▅▆▆▆▆▇▇▇▇▇▇▇█▇████████████████</td></tr><tr><td>minibatch_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>minibatch_f1</td><td>▁▅▅▅▄▇▆▄▆▇▆▆▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>minibatch_loss</td><td>█▄▄▄▅▂▃▄▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>train_f1</td><td>▁▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇▇█████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▄▅▆▆▆▇▇████████████████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.95312</td></tr><tr><td>Test loss</td><td>10.78413</td></tr><tr><td>Test macro F1</td><td>0.54553</td></tr><tr><td>Test precision</td><td>0.95039</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>25099</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00152</td></tr><tr><td>train_acc</td><td>0.99895</td></tr><tr><td>train_f1</td><td>0.98898</td></tr><tr><td>train_loss</td><td>1.23548</td></tr><tr><td>val_acc</td><td>0.95881</td></tr><tr><td>val_f1</td><td>0.55594</td></tr><tr><td>val_loss</td><td>13.18905</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">generous-leaf-31</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/gwfjrogd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240310_143136-gwfjrogd/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# saving the model\ntorch.save(model.state_dict(), 'LSTM_Word2Vec_Task1_Exp1.pth')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:07:39.181770Z","iopub.execute_input":"2024-03-10T15:07:39.182552Z","iopub.status.idle":"2024-03-10T15:07:39.252000Z","shell.execute_reply.started":"2024-03-10T15:07:39.182515Z","shell.execute_reply":"2024-03-10T15:07:39.250787Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}