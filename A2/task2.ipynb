{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Laptop_Review_Train.json') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Laptop_Review_Val.json') as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Laptop_Review_Test.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIO Encoding of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_label_encoding(tokens, aspects):\n",
    "    labels = ['O'] * len(tokens)\n",
    "    for aspect in aspects: \n",
    "        start = aspect['from']\n",
    "        end = aspect['to']\n",
    "        labels[start] = 'B'\n",
    "        for i in range(start+1, end):\n",
    "            labels[i] = 'I'\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over each sentence and encoding it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio(data):\n",
    "    processed_data = {}\n",
    "    for i in range(len(data)):\n",
    "        words = data[i]['words']\n",
    "        text = data[i]['raw_words']\n",
    "        aspects = data[i]['aspects']\n",
    "        labels = bio_label_encoding(words, aspects)\n",
    "        processed_data[i] = {\n",
    "            'text': text,\n",
    "            'labels': labels\n",
    "        }\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data and saving it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = convert_to_bio(train_data)\n",
    "processed_val_data = convert_to_bio(val_data)\n",
    "processed_test_data = convert_to_bio(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping the processed data\n",
    "with open('Laptop_Review_Train_Processed.json', 'w') as f:\n",
    "    json.dump(processed_train_data, f, indent=4)\n",
    "with open('Laptop_Review_Val_Processed.json', 'w') as f:\n",
    "    json.dump(processed_val_data, f, indent=4)\n",
    "with open('Laptop_Review_Test_Processed.json', 'w') as f:\n",
    "    json.dump(processed_test_data, f, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Laptop_Review_Train_Processed.json') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('Laptop_Review_Val_Processed.json') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('Laptop_Review_Test_Processed.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLp0lEQVR4nO3deVhU5f8+8HvYUWRwUDZlc0dUVNxwSRMSl1xSMv2a4V4JKmqWVopRrqXigqAt7mJKuVTugFpGqCgapaaGSipQToC4IDLP749+nI8joCyDMxzu13Wd63Ke85xz3s+ZEW7ONgohhAARERGRTBnpuwAiIiKiysSwQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrAjY3PnzoVCoXgu2+rRowd69OghvT5y5AgUCgViYmKey/ZHjRoFNze357Kt8srNzcW4cePg4OAAhUKBkJAQfZdUJj169ECLFi30XUapbdq0Cc2aNYOpqSlsbGwqdVsKhQLBwcGVug1dUigUmDt3rr7LqJDn/TOGqjaGnSpi/fr1UCgU0mRhYQEnJyf4+/tjxYoVuHPnjk62c/PmTcydOxfJyck6WZ8uGXJtpTF//nysX78eb7/9NjZt2oSRI0fquyTZunDhAkaNGoWGDRvi888/x9q1a0vsu3fv3ir/i5+qjvnz52PXrl36LqPaMdF3AVQ2YWFhcHd3R35+PtLT03HkyBGEhIRg6dKl2LNnD1q1aiX1/fDDDzFz5swyrf/mzZv46KOP4ObmhtatW5d6uYMHD5ZpO+XxtNo+//xzaDSaSq+hIuLi4tCpUyeEhobquxTZO3LkCDQaDZYvX45GjRo9te/evXsRERFRrQLP/fv3YWLCH//6MH/+fAQEBGDQoEH6LqVa4ae9iunTpw/atWsnvZ41axbi4uLw8ssvY8CAATh//jwsLS0BACYmJpX+A+3evXuoUaMGzMzMKnU7z2JqaqrX7ZdGZmYmmjdvru8yDJpGo8HDhw9hYWFRofVkZmYCQKWfvqqqKrp/iaoansaSgZ49e2L27Nm4du0aNm/eLLUXd83OoUOH0LVrV9jY2MDKygpNmzbF+++/D+C/v4bbt28PABg9erR0ymz9+vUA/nfNRlJSEl544QXUqFFDWvbJa3YKFRQU4P3334eDgwNq1qyJAQMGIC0tTauPm5sbRo0aVWTZx9f5rNqKu2bn7t27mD59OpydnWFubo6mTZvis88+gxBCq1/h9Ra7du1CixYtYG5uDk9PT+zfv7/4Hf6EzMxMjB07Fvb29rCwsICXlxc2bNggzS+8tiA1NRU//PCDVPvVq1dLXGdpayrpWqXi3vvCde7YsQPNmzeHpaUlfHx88OuvvwIA1qxZg0aNGsHCwgI9evQosb6kpCR07twZlpaWcHd3R1RUVJE+eXl5CA0NRaNGjWBubg5nZ2e8++67yMvLK7amLVu2wNPTE+bm5s/c76tXr5b6Ojk5ISgoCFlZWdJ8Nzc36ehZ3bp1n3p9yqhRoxARESHVUjgVKu1nqDiffPIJjIyMsHLlSqlt37596NatG2rWrIlatWqhX79++O2334rUZGVlhRs3bmDQoEGwsrJC3bp18c4776CgoECr77Zt2+Dt7Y1atWrB2toaLVu2xPLly59Z25P7pPDzcvnyZYwaNQo2NjZQKpUYPXo07t2798z1AUBiYiJ69+4NpVKJGjVqoHv37jh+/LhWn2vXrmHixIlo2rQpLC0tYWtri1dffbXYz1pWVhamTp0KNzc3mJubo379+njjjTfwzz//aPXTaDSYN28e6tevDwsLC/j6+uLy5cvPrPfOnTsICQmR1m9nZ4eXXnoJp0+fLvO4Srv/FAoF7t69iw0bNkiftcd/9t24cQNjxoyBvb299H/+q6++0tpW4c+T7du3l2rciYmJ6Nu3L2rXro2aNWuiVatWRT4jFy5cQEBAAFQqFSwsLNCuXTvs2bPnmfuwShFUJaxbt04AECdPnix2flpamgAgAgICpLbQ0FDx+FuckpIizMzMRLt27cTy5ctFVFSUeOedd8QLL7wghBAiPT1dhIWFCQBiwoQJYtOmTWLTpk3iypUrQgghunfvLhwcHETdunXFpEmTxJo1a8SuXbuked27d5e2FR8fLwCIli1bilatWomlS5eKmTNnCgsLC9GkSRNx7949qa+rq6sIDAwsMqbH1/ms2gIDA4Wrq6u0rEajET179hQKhUKMGzdOrFq1SvTv318AECEhIVrbASC8vLyEo6Oj+Pjjj0V4eLho0KCBqFGjhvjnn3+e+r7cu3dPeHh4CFNTUzF16lSxYsUK0a1bNwFAhIeHS7Vv2rRJ1KlTR7Ru3VqqPTc3t8T1lramJ8dd6Mn3vnCdrVq1Es7OzmLhwoVi4cKFQqlUChcXF7Fq1SrRvHlzsWTJEvHhhx8KMzMz8eKLLxZ5P5ycnISdnZ0IDg4WK1asEF27dhUAxJdffin1KygoEL169RI1atQQISEhYs2aNSI4OFiYmJiIgQMHFqnJw8ND1K1bV3z00UciIiJCnDlzpsT9UjguPz8/sXLlShEcHCyMjY1F+/btxcOHD4UQQuzcuVO88sorAoCIjIwUmzZtEmfPni12fT///LN46aWXBADpfdm0aZMQouyfoaCgIOn1Bx98IBQKhVi7dq3UtnHjRqFQKETv3r3FypUrxaJFi4Sbm5uwsbERqampUr/AwEBhYWEhPD09xZgxY0RkZKQYMmSIACBWr14t9Tt48KAAIHx9fUVERISIiIgQwcHB4tVXXy1x/z1eb2hoaJH92qZNGzF48GCxevVqMW7cOAFAvPvuu89cX2xsrDAzMxM+Pj5iyZIlYtmyZaJVq1bCzMxMJCYmSv127NghvLy8xJw5c8TatWvF+++/L2rXri1cXV3F3bt3pX537twRLVq0EMbGxmL8+PEiMjJSfPzxx6J9+/bS56PwZ0ybNm2Et7e3WLZsmZg7d66oUaOG6NChwzNr/r//+z9hZmYmpk2bJr744guxaNEi0b9/f7F58+Yyj6u0+2/Tpk3C3NxcdOvWTfqs/fzzz0KI/35O1K9fXzg7O4uwsDARGRkpBgwYIACIZcuWSesoy7gPHjwozMzMhKurqwgNDRWRkZFi8uTJws/PT+qTkpIilEqlaN68uVi0aJFYtWqVeOGFF4RCoRDffvvtM/djVcGwU0U8K+wIIYRSqRRt2rSRXj/5C2/ZsmUCgPj7779LXMfJkycFALFu3boi87p37y4AiKioqGLnFRd26tWrJ3JycqT27du3CwBi+fLlUltpws6zanvyl/6uXbsEAPHJJ59o9QsICBAKhUJcvnxZagMgzMzMtNrOnj0rAIiVK1cW2dbjwsPDBQCtH5APHz4UPj4+wsrKSmvsrq6uol+/fk9dX1lrKmvYMTc31/rFumbNGgFAODg4aNU6a9YsAUCrb+H7v2TJEqktLy9PtG7dWtjZ2UlhY9OmTcLIyEj8+OOPWtuPiooSAMTx48e1ajIyMhK//fbbM/dJZmamMDMzE7169RIFBQVS+6pVqwQA8dVXXxUZ/9M+64WCgoKK7Cshyv4ZKgw706dPF0ZGRmL9+vXS/Dt37ggbGxsxfvx4rXWlp6cLpVKp1R4YGCgAiLCwMK2+hb/cCk2ZMkVYW1uLR48ePXOMTyop7IwZM0ar3yuvvCJsbW2fui6NRiMaN24s/P39hUajkdrv3bsn3N3dxUsvvaTV9qSEhAQBQGzcuFFqmzNnjgBQ7C/bwm0U/ozx8PAQeXl50vzly5cLAOLXX399at1KpVIroFZkXGXZfzVr1iz2593YsWOFo6NjkT+whg0bJpRKpbTvSjvuR48eCXd3d+Hq6ir+/fffImMr5OvrK1q2bCkePHigNb9z586icePGJe6fqoansWTEysrqqXdlFV6/sHv37nJfzGtubo7Ro0eXuv8bb7yBWrVqSa8DAgLg6OiIvXv3lmv7pbV3714YGxtj8uTJWu3Tp0+HEAL79u3Tavfz80PDhg2l161atYK1tTX+/PPPZ27HwcEBw4cPl9pMTU0xefJk5Obm4ujRo+UeQ3lrehpfX1+t014dO3YEAAwZMkTrfSpsf3JbJiYmePPNN6XXZmZmePPNN5GZmYmkpCQAwI4dO+Dh4YFmzZrhn3/+kaaePXsCAOLj47XW2b1791Jdy3T48GE8fPgQISEhMDL634+u8ePHw9raGj/88ENpdkGplfUzJIRAcHAwli9fjs2bNyMwMFCad+jQIWRlZWH48OFa+8TY2BgdO3Yssk8A4K233tJ63a1bN633w8bGBnfv3sWhQ4d0MdwSt3n79m3k5OSUuExycjIuXbqE//u//8Pt27elsd29exe+vr44duyY9POm8HpCAMjPz8ft27fRqFEj2NjYaJ0++uabb+Dl5YVXXnmlyPaePD07evRorWsGu3XrBqDoZ/dJNjY2SExMxM2bNys8rkLl2X/Af5+db775Bv3794cQQusz4u/vj+zs7CKn15417jNnziA1NRUhISFFrl0r3IdqtRpxcXEYOnQo7ty5I23z9u3b8Pf3x6VLl3Djxo2n1l5V8AJlGcnNzYWdnV2J81977TV88cUXGDduHGbOnAlfX18MHjwYAQEBWr88nqZevXpluhi5cePGWq8VCgUaNWr01OtVdOHatWtwcnLS+gUOAB4eHtL8x7m4uBRZR+3atfHvv/8+czuNGzcusv9K2k5ZlLemsqxTqVQCAJydnYttf3JbTk5OqFmzplZbkyZNAABXr15Fp06dcOnSJZw/fx5169YttobCi4cLubu7l6r2wn3ZtGlTrXYzMzM0aNCgQvu6pO2V5TO0ceNG5ObmIjIyUiv8AsClS5cAQAp8T7K2ttZ6bWFhUWT/PfneT5w4Edu3b0efPn1Qr1499OrVC0OHDkXv3r3LMEptT34+ateuDeC/z8GTNRYqHNvj4e5J2dnZqF27Nu7fv48FCxZg3bp1uHHjhta1T9nZ2dK/r1y5giFDhlS45qdZvHgxAgMD4ezsDG9vb/Tt2xdvvPEGGjRoUOZxlaaWkvYfAPz999/IysrC2rVrS3xMwpP/b5417itXrgDAU5+NdfnyZQghMHv2bMyePbvE7darV6/EdVQVDDsy8ddffyE7O/upt9laWlri2LFjiI+Pxw8//ID9+/fj66+/Rs+ePXHw4EEYGxs/czuP/2WmKyU9+LCgoKBUNelCSdt5/Ifx81aamp6278qyTl2OX6PRoGXLlli6dGmx858MVpXxmdKHLl26IDk5GatWrcLQoUOhUqmkeYVHADZt2gQHB4ciyz5512RpPvd2dnZITk7GgQMHsG/fPuzbtw/r1q3DG2+8oXWBfFmU53NQOLZPP/20xMdVWFlZAQAmTZqEdevWISQkBD4+PlAqlVAoFBg2bFi5jzaX97M7dOhQdOvWDTt37sTBgwfx6aefYtGiRfj222/Rp0+fMo2rorUUbuv1118vMVw9/liRimyruO2+88478Pf3L7bPsx7dUFUw7MjEpk2bAKDED2whIyMj+Pr6wtfXF0uXLsX8+fPxwQcfID4+Hn5+fjp/4nLhX0eFhBC4fPmy1n/c2rVra91NU+jatWvSX1lAyb/Yi+Pq6orDhw/jzp07Wn+ZX7hwQZqvC66urjh37hw0Go3W0R1db6ckT9t3leHmzZu4e/eu1tGdP/74AwCk02MNGzbE2bNn4evrq9PPU+G+vHjxotbn4uHDh0hNTYWfn1+51ltSjWX9DDVq1AiLFy9Gjx490Lt3b8TGxkrLFZ6OtLOzK3edxTEzM0P//v3Rv39/aDQaTJw4EWvWrMHs2bOf2y+pwrFZW1s/c2wxMTEIDAzEkiVLpLYHDx4U+Qw3bNgQKSkpOq/1SY6Ojpg4cSImTpyIzMxMtG3bFvPmzUOfPn3KNK6yKO7zVrduXdSqVQsFBQU621Zh/SkpKSWus/D/kampqU7HaIh4zY4MxMXF4eOPP4a7uztGjBhRYj+1Wl2krfAvlsJbggt/iRX3C7Q8Nm7cqHUdUUxMDG7duoU+ffpIbQ0bNsQvv/yChw8fSm3ff/99kVvUy1Jb3759UVBQgFWrVmm1L1u2DAqFQmv7FdG3b1+kp6fj66+/ltoePXqElStXwsrKCt27d9fJdkrSsGFDZGdn49y5c1LbrVu3sHPnzkrZ3qNHj7BmzRrp9cOHD7FmzRrUrVsX3t7eAP77i/nGjRv4/PPPiyx///593L17t1zb9vPzg5mZGVasWKH11+uXX36J7Oxs9OvXr1zrLelzVZ7PUKtWrbB3716cP38e/fv3x/379wH890eItbU15s+fj/z8/CLL/f3332Wu+/bt21qvjYyMpD8inrzFvzJ5e3ujYcOG+Oyzz5Cbm1tk/uNjMzY2LnLkYeXKlUWORA4ZMgRnz54t9nOsi6OtBQUFWqfNgP+CqJOTk7TvyjKusqhZs2aRz5qxsTGGDBmCb775ptiQV55ttW3bFu7u7ggPDy+yvcJ9aGdnhx49emDNmjW4deuWTrZrqHhkp4rZt28fLly4gEePHiEjIwNxcXE4dOgQXF1dsWfPnqc+LCwsLAzHjh1Dv3794OrqiszMTKxevRr169dH165dAfz3y9PGxgZRUVGoVasWatasiY4dO5b6uoonqVQqdO3aFaNHj0ZGRgbCw8PRqFEjjB8/Xuozbtw4xMTEoHfv3hg6dCiuXLmCzZs3a12cW9ba+vfvjxdffBEffPABrl69Ci8vLxw8eBC7d+9GSEhIkXWX14QJE7BmzRqMGjUKSUlJcHNzQ0xMDI4fP47w8PAi13vo2rBhw/Dee+/hlVdeweTJk3Hv3j1ERkaiSZMmRS5o1AUnJycsWrQIV69eRZMmTfD1118jOTkZa9eulR7sOHLkSGzfvh1vvfUW4uPj0aVLFxQUFODChQvYvn07Dhw4oPVgzNKqW7cuZs2ahY8++gi9e/fGgAEDcPHiRaxevRrt27fH66+/Xq4xFYa0yZMnw9/fH8bGxhg2bFi5P0OdOnXC7t270bdvXwQEBGDXrl2wtrZGZGQkRo4cibZt22LYsGGoW7curl+/jh9++AFdunQpEqqeZdy4cVCr1ejZsyfq16+Pa9euYeXKlWjdurV0XdHzYGRkhC+++AJ9+vSBp6cnRo8ejXr16uHGjRuIj4+HtbU1vvvuOwDAyy+/jE2bNkGpVKJ58+ZISEjA4cOHYWtrq7XOGTNmICYmBq+++irGjBkDb29vqNVq7NmzB1FRUfDy8qpQzXfu3EH9+vUREBAALy8vWFlZ4fDhwzh58qR01Kks4yoLb29vHD58GEuXLoWTkxPc3d3RsWNHLFy4EPHx8ejYsSPGjx+P5s2bQ61W4/Tp0zh8+HCxf6w+jZGRESIjI9G/f3+0bt0ao0ePhqOjIy5cuIDffvsNBw4cAABERESga9euaNmyJcaPH48GDRogIyMDCQkJ+Ouvv3D27Nkyj9EgPee7v6icCm89L5zMzMyEg4ODeOmll8Ty5cu1bhsu9OTtx7GxsWLgwIHCyclJmJmZCScnJzF8+HDxxx9/aC23e/du0bx5c2FiYqJ1q3f37t2Fp6dnsfWVdOt5dHS0mDVrlrCzsxOWlpaiX79+4tq1a0WWX7JkiahXr54wNzcXXbp0EadOnSqyzqfVVtwt2Hfu3BFTp04VTk5OwtTUVDRu3Fh8+umnWrddClH0GSmFSrol/kkZGRli9OjRok6dOsLMzEy0bNmy2Nvjy3rreWlrOnjwoGjRooUwMzMTTZs2FZs3by7x1vMn15mamioAiE8//VSrvfD927Fjh9RW+P6fOnVK+Pj4CAsLC+Hq6ipWrVpVpM6HDx+KRYsWCU9PT2Fubi5q164tvL29xUcffSSys7OfOc6nWbVqlWjWrJkwNTUV9vb24u233y5ya21Zbj1/9OiRmDRpkqhbt65QKBRa+60in6Hdu3cLExMT8dprr0m3ysfHxwt/f3+hVCqFhYWFaNiwoRg1apQ4deqUtFxgYKCoWbNmkTqffE9jYmJEr169hJ2dnTAzMxMuLi7izTffFLdu3XrmmFHCredP7q/CnzuPP4KgJGfOnBGDBw8Wtra2wtzcXLi6uoqhQ4eK2NhYqc+///4r/V+xsrIS/v7+4sKFC8V+rm/fvi2Cg4NFvXr1hJmZmahfv74IDAyUbs0u7jMqxP8+08X9HyyUl5cnZsyYIby8vEStWrVEzZo1hZeXl9ZzjMoyrrLsvwsXLogXXnhBWFpaCgBa487IyBBBQUHC2dlZmJqaCgcHB+Hr66v1vKayjvunn34SL730kjTOVq1aFXmkxpUrV8Qbb7whHBwchKmpqahXr554+eWXRUxMTIn7sKpRCKHHKzCJiIiIKhmv2SEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWdNr2Dl27Bj69+8PJycnKBQK7Nq1q8S+b731FhQKBcLDw7Xa1Wo1RowYAWtra9jY2GDs2LHFPgCKiIiIqie9PlTw7t278PLywpgxYzB48OAS++3cuRO//PILnJyciswbMWIEbt26hUOHDiE/Px+jR4/GhAkTsHXr1lLXodFocPPmTdSqVUvnX5dARERElUMIgTt37sDJyenpX2it5+f8SACInTt3Fmn/66+/RL169URKSopwdXUVy5Ytk+b9/vvvAoA4efKk1LZv3z6hUCjEjRs3Sr3ttLQ0rQf2ceLEiRMnTpyqzpSWlvbU3/MG/XURGo0GI0eOxIwZM+Dp6VlkfkJCAmxsbLQePe/n5wcjIyMkJibilVdeKXa9eXl5Wt8dI/7/cxXT0tJgbW2t41EQERFRZcjJyYGzs/Mzv5rHoMPOokWLYGJigsmTJxc7Pz09HXZ2dlptJiYmUKlUSE9PL3G9CxYswEcffVSk3drammGHiIioinnWJSgGezdWUlISli9fjvXr1+v8OppZs2YhOztbmp78dm0iIiKSD4MNOz/++CMyMzPh4uICExMTmJiY4Nq1a5g+fTrc3NwAAA4ODsjMzNRa7tGjR1Cr1XBwcChx3ebm5tJRHB7NISIikjeDPY01cuRI+Pn5abX5+/tj5MiRGD16NADAx8cHWVlZSEpKgre3NwAgLi4OGo0GHTt2fO41ExERkeHRa9jJzc3F5cuXpdepqalITk6GSqWCi4sLbG1ttfqbmprCwcEBTZs2BQB4eHigd+/eGD9+PKKiopCfn4/g4GAMGzas2NvUiYiIqPrR62msU6dOoU2bNmjTpg0AYNq0aWjTpg3mzJlT6nVs2bIFzZo1g6+vL/r27YuuXbti7dq1lVUyERERVTEKUXjfdTWWk5MDpVKJ7OxsXr9DRERURZT297fBXqBMREREpAsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrBvvdWFQ1RUdHQ61Wl2kZlUqF4cOHV1JFRERU3THskE6p1WrExKhhaakqVf/799UICKjkooiIqFpj2CGds7RUoUOHoFL1PXEiopKrISKi6o7X7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkazpNewcO3YM/fv3h5OTExQKBXbt2iXNy8/Px3vvvYeWLVuiZs2acHJywhtvvIGbN29qrUOtVmPEiBGwtraGjY0Nxo4di9zc3Oc8EiIiIjJUeg07d+/ehZeXFyIiIorMu3fvHk6fPo3Zs2fj9OnT+Pbbb3Hx4kUMGDBAq9+IESPw22+/4dChQ/j+++9x7NgxTJgw4XkNgYiIiAyciT433qdPH/Tp06fYeUqlEocOHdJqW7VqFTp06IDr16/DxcUF58+fx/79+3Hy5Em0a9cOALBy5Ur07dsXn332GZycnCp9DERERGTYqtQ1O9nZ2VAoFLCxsQEAJCQkwMbGRgo6AODn5wcjIyMkJiaWuJ68vDzk5ORoTURERCRPVSbsPHjwAO+99x6GDx8Oa2trAEB6ejrs7Oy0+pmYmEClUiE9Pb3EdS1YsABKpVKanJ2dK7V2IiIi0p8qEXby8/MxdOhQCCEQGRlZ4fXNmjUL2dnZ0pSWlqaDKomIiMgQ6fWandIoDDrXrl1DXFycdFQHABwcHJCZmanV/9GjR1Cr1XBwcChxnebm5jA3N6+0momIiMhwGPSRncKgc+nSJRw+fBi2trZa8318fJCVlYWkpCSpLS4uDhqNBh07dnze5RIREZEB0uuRndzcXFy+fFl6nZqaiuTkZKhUKjg6OiIgIACnT5/G999/j4KCAuk6HJVKBTMzM3h4eKB3794YP348oqKikJ+fj+DgYAwbNox3YhEREREAPYedU6dO4cUXX5ReT5s2DQAQGBiIuXPnYs+ePQCA1q1bay0XHx+PHj16AAC2bNmC4OBg+Pr6wsjICEOGDMGKFSueS/1ERERk+PQadnr06AEhRInznzavkEqlwtatW3VZFhEREcmIQV+zQ0RERFRRDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsm+i6ASBeio6OhVqvLtIxKpcLw4cMrqSIiIjIUDDskC2q1GjExalhaqkrV//59NQICKrkoIiIyCAw7JBuWlip06BBUqr4nTkRUcjVERGQoeM0OERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJml6/CPTYsWP49NNPkZSUhFu3bmHnzp0YNGiQNF8IgdDQUHz++efIyspCly5dEBkZicaNG0t91Go1Jk2ahO+++w5GRkYYMmQIli9fDisrKz2MSB6io6OhVqvLtIxKpcLw4cMrqSIiIqLy02vYuXv3Lry8vDBmzBgMHjy4yPzFixdjxYoV2LBhA9zd3TF79mz4+/vj999/h4WFBQBgxIgRuHXrFg4dOoT8/HyMHj0aEyZMwNatW5/3cGRDrVYjJkYNS0tVqfrfv69GQEAlF0VERFROeg07ffr0QZ8+fYqdJ4RAeHg4PvzwQwwcOBAAsHHjRtjb22PXrl0YNmwYzp8/j/379+PkyZNo164dAGDlypXo27cvPvvsMzg5OT23sciNpaUKHToElarviRMRlVwNERFR+RnsNTupqalIT0+Hn5+f1KZUKtGxY0ckJCQAABISEmBjYyMFHQDw8/ODkZEREhMTS1x3Xl4ecnJytCYiIiKSJ4MNO+np6QAAe3t7rXZ7e3tpXnp6Ouzs7LTmm5iYQKVSSX2Ks2DBAiiVSmlydnbWcfVERERkKAw27FSmWbNmITs7W5rS0tL0XRIRERFVEoMNOw4ODgCAjIwMrfaMjAxpnoODAzIzM7XmP3r0CGq1WupTHHNzc1hbW2tNREREJE8GG3bc3d3h4OCA2NhYqS0nJweJiYnw8fEBAPj4+CArKwtJSUlSn7i4OGg0GnTs2PG510xERESGR693Y+Xm5uLy5cvS69TUVCQnJ0OlUsHFxQUhISH45JNP0LhxY+nWcycnJ+lZPB4eHujduzfGjx+PqKgo5OfnIzg4GMOGDeOdWERERARAz2Hn1KlTePHFF6XX06ZNAwAEBgZi/fr1ePfdd3H37l1MmDABWVlZ6Nq1K/bv3y89YwcAtmzZguDgYPj6+koPFVyxYsVzHwsREREZJr2GnR49ekAIUeJ8hUKBsLAwhIWFldhHpVLxAYJERERUIoO9ZoeIiIhIFxh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1vT6redUOaKjo6FWq8u8nEqlwvDhwyuhIiIiIv1h2JEhtVqNmBg1LC1VpV7m/n01AgIqsSgiIiI9YdiRKUtLFTp0CCp1/xMnIiqxGiIiIv3hNTtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkawYddgoKCjB79my4u7vD0tISDRs2xMcffwwhhNRHCIE5c+bA0dERlpaW8PPzw6VLl/RYNRERERkSgw47ixYtQmRkJFatWoXz589j0aJFWLx4MVauXCn1Wbx4MVasWIGoqCgkJiaiZs2a8Pf3x4MHD/RYORERERkKE30X8DQ///wzBg4ciH79+gEA3NzcEB0djRMnTgD476hOeHg4PvzwQwwcOBAAsHHjRtjb22PXrl0YNmyY3monIiIiw2DQR3Y6d+6M2NhY/PHHHwCAs2fP4qeffkKfPn0AAKmpqUhPT4efn5+0jFKpRMeOHZGQkKCXmomIiMiwGPSRnZkzZyInJwfNmjWDsbExCgoKMG/ePIwYMQIAkJ6eDgCwt7fXWs7e3l6aV5y8vDzk5eVJr3NyciqheiIiIjIEBn1kZ/v27diyZQu2bt2K06dPY8OGDfjss8+wYcOGCq13wYIFUCqV0uTs7KyjiomIiMjQGHTYmTFjBmbOnIlhw4ahZcuWGDlyJKZOnYoFCxYAABwcHAAAGRkZWstlZGRI84oza9YsZGdnS1NaWlrlDYKIiIj0yqDDzr1792BkpF2isbExNBoNAMDd3R0ODg6IjY2V5ufk5CAxMRE+Pj4lrtfc3BzW1tZaExEREcmTQV+z079/f8ybNw8uLi7w9PTEmTNnsHTpUowZMwYAoFAoEBISgk8++QSNGzeGu7s7Zs+eDScnJwwaNEi/xRMREZFBMOiws3LlSsyePRsTJ05EZmYmnJyc8Oabb2LOnDlSn3fffRd3797FhAkTkJWVha5du2L//v2wsLDQY+VERERkKAw67NSqVQvh4eEIDw8vsY9CoUBYWBjCwsKeX2FERERUZRj0NTtEREREFVWusNOgQQPcvn27SHtWVhYaNGhQ4aKIiIiIdKVcYefq1asoKCgo0p6Xl4cbN25UuCgiIiIiXSnTNTt79uyR/n3gwAEolUrpdUFBAWJjY+Hm5qaz4oiIiIgqqkxhp/B2boVCgcDAQK15pqamcHNzw5IlS3RWHBEREVFFlSnsPP4wv5MnT6JOnTqVUhQRERGRrpTr1vPU1FRd10FERERUKcr9nJ3Y2FjExsYiMzNTOuJT6KuvvqpwYURERES6UK6w89FHHyEsLAzt2rWDo6MjFAqFrusiIiIi0olyhZ2oqCisX78eI0eO1HU9RERERDpVrufsPHz4EJ07d9Z1LUREREQ6V66wM27cOGzdulXXtRARERHpXLlOYz148ABr167F4cOH0apVK5iammrNX7p0qU6KIyIiIqqocoWdc+fOoXXr1gCAlJQUrXm8WJmIiIgMSbnCTnx8vK7rICIiIqoU5bpmh4iIiKiqKNeRnRdffPGpp6vi4uLKXRARERGRLpUr7BRer1MoPz8fycnJSElJKfIFoURERET6VK6ws2zZsmLb586di9zc3AoVRERERKRLOr1m5/XXX+f3YhEREZFB0WnYSUhIgIWFhS5XSURERFQh5TqNNXjwYK3XQgjcunULp06dwuzZs3VSGBEREZEulCvsKJVKrddGRkZo2rQpwsLC0KtXL50URkRERKQL5Qo769at03UdRERERJWiXGGnUFJSEs6fPw8A8PT0RJs2bXRSFBEREZGulCvsZGZmYtiwYThy5AhsbGwAAFlZWXjxxRexbds21K1bV5c1EhEREZVbue7GmjRpEu7cuYPffvsNarUaarUaKSkpyMnJweTJk3VdIxEREVG5levIzv79+3H48GF4eHhIbc2bN0dERAQvUKZqJTo6Gmq1ukzLqFQqDB8+vJIqIiKiJ5Ur7Gg0GpiamhZpNzU1hUajqXBRRFWFWq1GTIwalpaqUvW/f1+NgIBKLoqIiLSUK+z07NkTU6ZMQXR0NJycnAAAN27cwNSpU+Hr66vTAqn6qKpHSSwtVejQIahUfU+ciKjkaoiI6EnlCjurVq3CgAED4ObmBmdnZwBAWloaWrRogc2bN+u0QKo+eJSEiIgqQ7nCjrOzM06fPo3Dhw/jwoULAAAPDw/4+fnptDiqfniUhIiIdK1Md2PFxcWhefPmyMnJgUKhwEsvvYRJkyZh0qRJaN++PTw9PfHjjz9WVq1EREREZVamsBMeHo7x48fD2tq6yDylUok333wTS5cu1VlxRERERBVVprBz9uxZ9O7du8T5vXr1QlJSUoWLIiIiItKVMoWdjIyMYm85L2RiYoK///67wkURERER6UqZwk69evWQkpJS4vxz587B0dGxwkURERER6UqZwk7fvn0xe/ZsPHjwoMi8+/fvIzQ0FC+//LLOiiMiIiKqqDLdev7hhx/i22+/RZMmTRAcHIymTZsCAC5cuICIiAgUFBTggw8+qJRCiYiIiMqjTGHH3t4eP//8M95++23MmjULQggAgEKhgL+/PyIiImBvb18phRIRERGVR5kfKujq6oq9e/fi33//xeXLlyGEQOPGjVG7du3KqI+IiIioQsr1BGUAqF27Ntq3b6/LWoiIiIh0rkwXKOvDjRs38Prrr8PW1haWlpZo2bIlTp06Jc0XQmDOnDlwdHSEpaUl/Pz8cOnSJT1WTERERIbEoMPOv//+iy5dusDU1BT79u3D77//jiVLlmidMlu8eDFWrFiBqKgoJCYmombNmvD39y/2jjEiIiKqfsp9Gut5WLRoEZydnbFu3Tqpzd3dXfq3EALh4eH48MMPMXDgQADAxo0bYW9vj127dmHYsGHPvWYiIiIyLAZ9ZGfPnj1o164dXn31VdjZ2aFNmzb4/PPPpfmpqalIT0/X+rZ1pVKJjh07IiEhocT15uXlIScnR2siIiIieTLosPPnn38iMjISjRs3xoEDB/D2229j8uTJ2LBhAwAgPT0dAIrc7m5vby/NK86CBQugVCqlydnZufIGQURERHpl0GFHo9Ggbdu2mD9/Ptq0aYMJEyZg/PjxiIqKqtB6Z82ahezsbGlKS0vTUcVERERkaAw67Dg6OqJ58+ZabR4eHrh+/ToAwMHBAcB/X1D6uIyMDGlecczNzWFtba01ERERkTwZdNjp0qULLl68qNX2xx9/wNXVFcB/Fys7ODggNjZWmp+Tk4PExET4+Pg811qJiIjIMBn03VhTp05F586dMX/+fAwdOhQnTpzA2rVrsXbtWgD/fU1FSEgIPvnkEzRu3Bju7u6YPXs2nJycMGjQIP0WT0RERAbBoMNO+/btsXPnTsyaNQthYWFwd3dHeHg4RowYIfV59913cffuXUyYMAFZWVno2rUr9u/fDwsLCz1WTkRERIbCoMMOALz88st4+eWXS5yvUCgQFhaGsLCw51gVERERVRUGH3aIKlt0dDTUanWZllGpVBg+fHglVURERLrEsEPVnlqtRkyMGpaWqlL1v39fjYCASi6KiIh0hmGHCIClpQodOgSVqu+JExGVXA0REemSQd96TkRERFRRDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrfM6OgeJTfYmIiHSDYcdA8am+REREusGwY8D4VF8iIqKK4zU7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGtVKuwsXLgQCoUCISEhUtuDBw8QFBQEW1tbWFlZYciQIcjIyNBfkURERGRQqkzYOXnyJNasWYNWrVpptU+dOhXfffcdduzYgaNHj+LmzZsYPHiwnqokIiIiQ1Mlwk5ubi5GjBiBzz//HLVr15bas7Oz8eWXX2Lp0qXo2bMnvL29sW7dOvz888/45Zdf9FgxERERGYoqEXaCgoLQr18/+Pn5abUnJSUhPz9fq71Zs2ZwcXFBQkLC8y6TiIiIDJCJvgt4lm3btuH06dM4efJkkXnp6ekwMzODjY2NVru9vT3S09NLXGdeXh7y8vKk1zk5OTqrl4iIiAyLQR/ZSUtLw5QpU7BlyxZYWFjobL0LFiyAUqmUJmdnZ52tm4iIiAyLQR/ZSUpKQmZmJtq2bSu1FRQU4NixY1i1ahUOHDiAhw8fIisrS+voTkZGBhwcHEpc76xZszBt2jTpdU5ODgMPPXfR0dFQq9VlWkalUmH48OGVVBERkTwZdNjx9fXFr7/+qtU2evRoNGvWDO+99x6cnZ1hamqK2NhYDBkyBABw8eJFXL9+HT4+PiWu19zcHObm5pVaO9GzqNVqxMSoYWmpKlX/+/fVCAio5KKIiGTIoMNOrVq10KJFC622mjVrwtbWVmofO3Yspk2bBpVKBWtra0yaNAk+Pj7o1KmTPkomKhNLSxU6dAgqVd8TJyIquRoiInky6LBTGsuWLYORkRGGDBmCvLw8+Pv7Y/Xq1foui6jS8TQYEVHpVLmwc+TIEa3XFhYWiIiIQEQE/+ql6oWnwYiISqfKhR0i+h+eBiMiejaDvvWciIiIqKIYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWGHaIiIhI1hh2iIiISNYYdoiIiEjWTPRdgJxFR0dDrVaXaRmVSoXhw4dXUkVERETVD8NOJVKr1YiJUcPSUlWq/vfvqxEQUMlFERERVTMMO5XM0lKFDh2CStX3xImISq6GiIio+mHYIaqGeIqViKoThh2iaoinWImoOmHYIaqmynuKlUeFiKiqYdghojLhUSEiqmoYdoiozHjhPRFVJXyoIBEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREcmaQYedBQsWoH379qhVqxbs7OwwaNAgXLx4UavPgwcPEBQUBFtbW1hZWWHIkCHIyMjQU8VERERkaAw67Bw9ehRBQUH45ZdfcOjQIeTn56NXr164e/eu1Gfq1Kn47rvvsGPHDhw9ehQ3b97E4MGD9Vg1ERERGRKD/iLQ/fv3a71ev3497OzskJSUhBdeeAHZ2dn48ssvsXXrVvTs2RMAsG7dOnh4eOCXX35Bp06d9FE2ERERGRCDPrLzpOzsbACASqUCACQlJSE/Px9+fn5Sn2bNmsHFxQUJCQklricvLw85OTlaExEREclTlQk7Go0GISEh6NKlC1q0aAEASE9Ph5mZGWxsbLT62tvbIz09vcR1LViwAEqlUpqcnZ0rs3QiIiLSoyoTdoKCgpCSkoJt27ZVeF2zZs1Cdna2NKWlpemgQiIiIjJEBn3NTqHg4GB8//33OHbsGOrXry+1Ozg44OHDh8jKytI6upORkQEHB4cS12dubg5zc/PKLJmIiIgMhEEf2RFCIDg4GDt37kRcXBzc3d215nt7e8PU1BSxsbFS28WLF3H9+nX4+Pg873KJiIjIABn0kZ2goCBs3boVu3fvRq1ataTrcJRKJSwtLaFUKjF27FhMmzYNKpUK1tbWmDRpEnx8fHgnFhEREQEw8LATGRkJAOjRo4dW+7p16zBq1CgAwLJly2BkZIQhQ4YgLy8P/v7+WL169XOulIiIiAyVQYcdIcQz+1hYWCAiIgIRERHPoSIiIiKqagw67BARFYqOjoZarS7TMiqVCsOHD6+kioioqmDYIaIqQa1WIyZGDUtLVan637+vRkBAJRdFRFUCww4RVRmWlip06BBUqr4nTvDUNhH9x6BvPSciIiKqKIYdIiIikjWexiKi54YXGRORPjDsENFzw4uMiUgfGHaI6LniRcZE9Lzxmh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNYYdIiIikjWGHSIiIpI1hh0iIiKSNRN9F0BEZMiio6OhVqvLtIxKpcLw4cMrqSIiKiuGHSKip1Cr1YiJUcPSUlWq/vfvqxEQUMlFEVGZMOwQkexV9OiMpaUKHToElWq5EyciylwfEVUuhh0ikj0enSGq3hh2iKha4NEZouqLYYeIyEDx4mgi3WDYISIyUDz9RqQbDDtERAaMp9+IKo4PFSQiIiJZY9ghIiIiWeNpLCIiGeLFzUT/w7BDRCRDvLiZ6H8YdoiIZIoXNxP9h2GHiIgMgr5OvfGUn/wx7BARkUHQ16k3nvKTP4YdIiLSUpEjHVX1S1d5yk/eGHaIiEhLRY508CgJGSLZhJ2IiAh8+umnSE9Ph5eXF1auXIkOHTrouywioiqpIkc6eJSEDI0sws7XX3+NadOmISoqCh07dkR4eDj8/f1x8eJF2NnZ6bs8IqqmynNKB+DFr1VJRd/jqnhxdFWsWRZhZ+nSpRg/fjxGjx4NAIiKisIPP/yAr776CjNnztRzdURUXZX1lA7A0zpVTUXf46p42q8q1lzlw87Dhw+RlJSEWbNmSW1GRkbw8/NDQkKCHisjIirbKR2Ap3Wqooq+x1XxtF9Vq7nKh51//vkHBQUFsLe312q3t7fHhQsXil0mLy8PeXl50uvs7GwAQE5Ojk5ru3//Pu7c+RfHj39Wqv4PHvyL+/drIycn57kuq89tc9nyLQtUnc8Xl+X/xeexLACsXbu2VMsVmjBhAoDn+3+posvraswVWbaiNetS4TqFEE/vKKq4GzduCADi559/1mqfMWOG6NChQ7HLhIaGCgCcOHHixIkTJxlMaWlpT80KVf7ITp06dWBsbIyMjAyt9oyMDDg4OBS7zKxZszBt2jTptUajgVqthq2tLe7cuQNnZ2ekpaXB2tq6Ums3BDk5OdVqvED1G3N1Gy9Q/cZc3cYLVL8xV7fxAqUbsxACd+7cgZOT01PXVeXDjpmZGby9vREbG4tBgwYB+C+8xMbGIjg4uNhlzM3NYW5urtVmY2MDAFAoFAAAa2vravOBAqrfeIHqN+bqNl6g+o25uo0XqH5jrm7jBZ49ZqVS+cx1VPmwAwDTpk1DYGAg2rVrhw4dOiA8PBx3796V7s4iIiKi6ksWYee1117D33//jTlz5iA9PR2tW7fG/v37i1y0TERERNWPLMIOAAQHB5d42qoszM3NERoaWuQ0l1xVt/EC1W/M1W28QPUbc3UbL1D9xlzdxgvodswKIZ51vxYRERFR1WWk7wKIiIiIKhPDDhEREckaww4RERHJGsMOERERyRrDzmMiIiLg5uYGCwsLdOzYESdOnNB3STpz7Ngx9O/fH05OTlAoFNi1a5fWfCEE5syZA0dHR1haWsLPzw+XLl3ST7E6sGDBArRv3x61atWCnZ0dBg0ahIsXL2r1efDgAYKCgmBrawsrKysMGTKkyJO4q4rIyEi0atVKeviWj48P9u3bJ82X01hLsnDhQigUCoSEhEhtchr33LlzoVAotKZmzZpJ8+U01sfduHEDr7/+OmxtbWFpaYmWLVvi1KlT0ny5/exyc3Mr8j4rFAoEBf33pZtye58LCgowe/ZsuLu7w9LSEg0bNsTHH3+s9V1XOnmPK/7tVPKwbds2YWZmJr766ivx22+/ifHjxwsbGxuRkZGh79J0Yu/eveKDDz4Q3377rQAgdu7cqTV/4cKFQqlUil27domzZ8+KAQMGCHd3d3H//n39FFxB/v7+Yt26dSIlJUUkJyeLvn37ChcXF5Gbmyv1eeutt4Szs7OIjY0Vp06dEp06dRKdO3fWY9Xlt2fPHvHDDz+IP/74Q1y8eFG8//77wtTUVKSkpAgh5DXW4pw4cUK4ubmJVq1aiSlTpkjtchp3aGio8PT0FLdu3ZKmv//+W5ovp7EWUqvVwtXVVYwaNUokJiaKP//8Uxw4cEBcvnxZ6iO3n12ZmZla7/GhQ4cEABEfHy+EkN/7PG/ePGFrayu+//57kZqaKnbs2CGsrKzE8uXLpT66eI8Zdv6/Dh06iKCgIOl1QUGBcHJyEgsWLNBjVZXjybCj0WiEg4OD+PTTT6W2rKwsYW5uLqKjo/VQoe5lZmYKAOLo0aNCiP/GZ2pqKnbs2CH1OX/+vAAgEhIS9FWmTtWuXVt88cUXsh/rnTt3ROPGjcWhQ4dE9+7dpbAjt3GHhoYKLy+vYufJbayF3nvvPdG1a9cS51eHn11TpkwRDRs2FBqNRpbvc79+/cSYMWO02gYPHixGjBghhNDde8zTWAAePnyIpKQk+Pn5SW1GRkbw8/NDQkKCHit7PlJTU5Genq41fqVSiY4dO8pm/NnZ2QAAlUoFAEhKSkJ+fr7WmJs1awYXF5cqP+aCggJs27YNd+/ehY+Pj6zHCgBBQUHo16+f1vgAeb7Hly5dgpOTExo0aIARI0bg+vXrAOQ5VgDYs2cP2rVrh1dffRV2dnZo06YNPv/8c2m+3H92PXz4EJs3b8aYMWOgUChk+T537twZsbGx+OOPPwAAZ8+exU8//YQ+ffoA0N17LJsnKFfEP//8g4KCgiJfL2Fvb48LFy7oqarnJz09HQCKHX/hvKpMo9EgJCQEXbp0QYsWLQD8N2YzMzPpC2ALVeUx//rrr/Dx8cGDBw9gZWWFnTt3onnz5khOTpbdWAtt27YNp0+fxsmTJ4vMk9t73LFjR6xfvx5NmzbFrVu38NFHH6Fbt25ISUmR3VgL/fnnn4iMjMS0adPw/vvv4+TJk5g8eTLMzMwQGBgo+59du3btQlZWFkaNGgVAfp9pAJg5cyZycnLQrFkzGBsbo6CgAPPmzcOIESMA6O73E8MOyV5QUBBSUlLw008/6buUStW0aVMkJycjOzsbMTExCAwMxNGjR/VdVqVJS0vDlClTcOjQIVhYWOi7nEpX+JcuALRq1QodO3aEq6srtm/fDktLSz1WVnk0Gg3atWuH+fPnAwDatGmDlJQUREVFITAwUM/VVb4vv/wSffr0gZOTk75LqTTbt2/Hli1bsHXrVnh6eiI5ORkhISFwcnLS6XvM01gA6tSpA2Nj4yJXtGdkZMDBwUFPVT0/hWOU4/iDg4Px/fffIz4+HvXr15faHRwc8PDhQ2RlZWn1r8pjNjMzQ6NGjeDt7Y0FCxbAy8sLy5cvl+VYgf9O3WRmZqJt27YwMTGBiYkJjh49ihUrVsDExAT29vayHHchGxsbNGnSBJcvX5bte+zo6IjmzZtrtXl4eEin7+T8s+vatWs4fPgwxo0bJ7XJ8X2eMWMGZs6ciWHDhqFly5YYOXIkpk6digULFgDQ3XvMsIP/fkl4e3sjNjZWatNoNIiNjYWPj48eK3s+3N3d4eDgoDX+nJwcJCYmVtnxCyEQHByMnTt3Ii4uDu7u7lrzvb29YWpqqjXmixcv4vr161V2zE/SaDTIy8uT7Vh9fX3x66+/Ijk5WZratWuHESNGSP+W47gL5ebm4sqVK3B0dJTte9ylS5cij4z4448/4OrqCkCeP7sKrVu3DnZ2dujXr5/UJsf3+d69ezAy0o4ixsbG0Gg0AHT4HuvkcmoZ2LZtmzA3Nxfr168Xv//+u5gwYYKwsbER6enp+i5NJ+7cuSPOnDkjzpw5IwCIpUuXijNnzohr164JIf67tc/Gxkbs3r1bnDt3TgwcOLBK37759ttvC6VSKY4cOaJ1G+e9e/ekPm+99ZZwcXERcXFx4tSpU8LHx0f4+Pjoserymzlzpjh69KhITU0V586dEzNnzhQKhUIcPHhQCCGvsT7N43djCSGvcU+fPl0cOXJEpKamiuPHjws/Pz9Rp04dkZmZKYSQ11gLnThxQpiYmIh58+aJS5cuiS1btogaNWqIzZs3S33k9rNLiP/uBnZxcRHvvfdekXlye58DAwNFvXr1pFvPv/32W1GnTh3x7rvvSn108R4z7Dxm5cqVwsXFRZiZmYkOHTqIX375Rd8l6Ux8fLwAUGQKDAwUQvx3e9/s2bOFvb29MDc3F76+vuLixYv6LboCihsrALFu3Tqpz/3798XEiRNF7dq1RY0aNcQrr7wibt26pb+iK2DMmDHC1dVVmJmZibp16wpfX18p6Aghr7E+zZNhR07jfu2114Sjo6MwMzMT9erVE6+99prW82bkNNbHfffdd6JFixbC3NxcNGvWTKxdu1Zrvtx+dgkhxIEDBwSAYscht/c5JydHTJkyRbi4uAgLCwvRoEED8cEHH4i8vDypjy7eY4UQjz2mkIiIiEhmeM0OERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhEREckaww4RERHJGsMOERERyRrDDhE9V1evXoVCoUBycrK+S5FcuHABnTp1goWFBVq3bq3Tdffo0QMhISE6XScRlQ3DDlE1M2rUKCgUCixcuFCrfdeuXVAoFHqqSr9CQ0NRs2ZNXLx4Ues7eB7H0EJUdTHsEFVDFhYWWLRoEf799199l6IzDx8+LPeyV65cQdeuXeHq6gpbW1sdVkVEhoBhh6ga8vPzg4ODAxYsWFBin7lz5xY5pRMeHg43Nzfp9ahRozBo0CDMnz8f9vb2sLGxQVhYGB49eoQZM2ZApVKhfv36WLduXZH1X7hwAZ07d4aFhQVatGiBo0ePas1PSUlBnz59YGVlBXt7e4wcORL//POPNL9Hjx4IDg5GSEgI6tSpA39//2LHodFoEBYWhvr168Pc3BytW7fG/v37pfkKhQJJSUkICwuDQqHA3Llzi6xj1KhROHr0KJYvXw6FQgGFQoGrV68CAI4ePYoOHTrA3Nwcjo6OmDlzJh49elTifv3hhx+gVCqxZcsWAEBaWhqGDh0KGxsbqFQqDBw4UFr34/v4s88+g6OjI2xtbREUFIT8/Hypz+rVq9G4cWNYWFjA3t4eAQEBJW6fqDpi2CGqhoyNjTF//nysXLkSf/31V4XWFRcXh5s3b+LYsWNYunQpQkND8fLLL6N27dpITEzEW2+9hTfffLPIdmbMmIHp06fjzJkz8PHxQf/+/XH79m0AQFZWFnr27Ik2bdrg1KlT2L9/PzIyMjB06FCtdWzYsAFmZmY4fvw4oqKiiq1v+fLlWLJkCT777DOcO3cO/v7+GDBgAC5dugQAuHXrFjw9PTF9+nTcunUL77zzTrHr8PHxwfjx43Hr1i3cunULzs7OuHHjBvr27Yv27dvj7NmziIyMxJdffolPPvmk2Fq2bt2K4cOHY8uWLRgxYgTy8/Ph7++PWrVq4ccff8Tx48dhZWWF3r17ax2pio+Px5UrVxAfH48NGzZg/fr1WL9+PQDg1KlTmDx5MsLCwnDx4kXs378fL7zwQunePKLqQrffX0pEhi4wMFAMHDhQCCFEp06dxJgxY4QQQuzcuVM8/iMhNDRUeHl5aS27bNky4erqqrUuV1dXUVBQILU1bdpUdOvWTXr96NEjUbNmTREdHS2EECI1NVUAEAsXLpT65Ofni/r164tFixYJIYT4+OOPRa9evbS2nZaWpvVN0N27dxdt2rR55nidnJzEvHnztNrat28vJk6cKL328vISoaGhT13Pk9+oLoQQ77//vmjatKnQaDRSW0REhLCyspL2SeFyq1atEkqlUhw5ckTqu2nTpiLL5+XlCUtLS3HgwAEhxP/28aNHj6Q+r776qnjttdeEEEJ88803wtraWuTk5DxzXxBVVyZ6zlpEpEeLFi1Cz549iz2aUVqenp4wMvrfQWJ7e3u0aNFCem1sbAxbW1tkZmZqLefj4yP928TEBO3atcP58+cBAGfPnkV8fDysrKyKbO/KlSto0qQJAMDb2/upteXk5ODmzZvo0qWLVnuXLl1w9uzZUo6wZOfPn4ePj4/Whd1dunRBbm4u/vrrL7i4uAAAYmJikJmZiePHj6N9+/ZS37Nnz+Ly5cuoVauW1nofPHiAK1euSK89PT1hbGwsvXZ0dMSvv/4KAHjppZfg6uqKBg0aoHfv3ujduzdeeeUV1KhRo8LjI5ILhh2iauyFF16Av78/Zs2ahVGjRmnNMzIyghBCq+3x60QKmZqaar1WKBTFtmk0mlLXlZubi/79+2PRokVF5jk6Okr/rlmzZqnXqU9t2rTB6dOn8dVXX6Fdu3ZSOMrNzYW3t7d0/c7j6tatK/37afuzVq1aOH36NI4cOYKDBw9izpw5mDt3Lk6ePAkbG5vKGxRRFcJrdoiquYULF+K7775DQkKCVnvdunWRnp6uFXh0+WycX375Rfr3o0ePkJSUBA8PDwBA27Zt8dtvv8HNzQ2NGjXSmsoScKytreHk5ITjx49rtR8/fhzNmzcvU71mZmYoKCjQavPw8EBCQoLWPjp+/Dhq1aqF+vXrS20NGzZEfHw8du/ejUmTJkntbdu2xaVLl2BnZ1dknEqlstS1mZiYwM/PD4sXL8a5c+dw9epVxMXFlWl8RHLGsENUzbVs2RIjRozAihUrtNp79OiBv//+G4sXL8aVK1cQERGBffv26Wy7ERER2LlzJy5cuICgoCD8+++/GDNmDAAgKCgIarUaw4cPx8mTJ3HlyhUcOHAAo0ePLhI4nmXGjBlYtGgRvv76a1y8eBEzZ85EcnIypkyZUqb1uLm5ITExEVevXsU///wDjUaDiRMnIi0tDZMmTcKFCxewe/duhIaGYtq0aVqn9gCgSZMmiI+PxzfffCM9r2fEiBGoU6cOBg4ciB9//BGpqak4cuQIJk+eXOoLx7///nusWLECycnJuHbtGjZu3AiNRoOmTZuWaXxEcsawQ0QICwsrcprJw8MDq1evRkREBLy8vHDixIkKXdvzpIULF2LhwoXw8vLCTz/9hD179qBOnToAIB2NKSgoQK9evdCyZUuEhITAxsamSIh4lsmTJ2PatGmYPn06WrZsif3792PPnj1o3LhxmdbzzjvvwNjYGM2bN0fdunVx/fp11KtXD3v37sWJEyfg5eWFt956C2PHjsWHH35Y7DqaNm2KuLg4REdHY/r06ahRowaOHTsGFxcXDB48GB4eHhg7diwePHgAa2vrUtVlY2ODb7/9Fj179oSHhweioqIQHR0NT0/PMo2PSM4U4smT8kREREQywiM7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQkaww7REREJGsMO0RERCRrDDtEREQka/8PCkNu4p94eaQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting data distribution of number of tokens in each sentence\n",
    "import matplotlib.pyplot as plt\n",
    "label_counts = []\n",
    "for key in train_data.keys():\n",
    "    label_counts.append(len(train_data[key]['labels']))\n",
    "plt.hist(label_counts, bins=30, alpha=0.5, color='b', edgecolor='black', linewidth=1.2, histtype='bar', align='mid', orientation='vertical', rwidth=0.8, label='Number of tokens')\n",
    "plt.title('Distribution of number of tokens in each sentence')\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for i in train_data:\n",
    "    train.append([train_data[i]['text'], train_data[i]['labels']])\n",
    "for i in val_data:\n",
    "    val.append([val_data[i]['text'], val_data[i]['labels']])\n",
    "for i in test_data:\n",
    "    test.append([test_data[i]['text'], test_data[i]['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 906\n",
      "Validation data size: 219\n",
      "Test data size: 328\n"
     ]
    }
   ],
   "source": [
    "print(f'Training data size: {len(train)}')\n",
    "print(f'Validation data size: {len(val)}')\n",
    "print(f'Test data size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I charge it at night and skip taking the cord with me because of the good battery life .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Text: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous .\n",
      "Labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Easy to start up and does not overheat as much as other laptops .\n",
      "Labels: ['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Great laptop that offers many great features !\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Text: One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text, labels = train[i][0], train[i][1]\n",
    "    print(f\"Text: {text}\\nLabels: {labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 3495\n",
      "Words and their counts: [('I', 637), ('charge', 10), ('it', 499), ('at', 70), ('night', 5)]\n"
     ]
    }
   ],
   "source": [
    "data = train + val + test\n",
    "# Finding number of unique words in the dataset\n",
    "word_count = {}\n",
    "for i in range(len(data)):\n",
    "    words = data[i][0].split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "print(f\"Number of unique words in the dataset: {len(word_count)}\")\n",
    "print(f\"Words and their counts: {list(word_count.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset after adding 'PAD' and 'UNK': 3497\n"
     ]
    }
   ],
   "source": [
    "word_list = list(word_count.keys())\n",
    "# adding 'PAD' and 'UNK' to the word list\n",
    "word_list.append('PAD')\n",
    "word_list.append('UNK')\n",
    "word_count['PAD'] = 0\n",
    "word_count['UNK'] = 0\n",
    "print(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-to-index: [('I', 0), ('charge', 1), ('it', 2), ('at', 3), ('night', 4)]\n",
      "Index-to-word: [(0, 'I'), (1, 'charge'), (2, 'it'), (3, 'at'), (4, 'night')]\n"
     ]
    }
   ],
   "source": [
    "# Word-to-index and index-to-word mapping from the dataset\n",
    "word_to_index = {word:idx for idx, word in enumerate(word_list)}\n",
    "index_to_word = {idx:word for word, idx in word_to_index.items()}\n",
    "label_to_idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "print(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\n",
    "print(f\"Index-to-word: {list(index_to_word.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word_embeddings: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = GloVe(name='6B', dim=300)\n",
    "word_embeddings = np.zeros((len(word_list), 300))\n",
    "for i in range(len(word_list)):\n",
    "    word = word_list[i]\n",
    "    idx = word_to_index[word]\n",
    "    if word in glove_vectors.stoi:\n",
    "        word_embeddings[idx] = glove_vectors[word]\n",
    "    else:\n",
    "        word_embeddings[idx] = glove_vectors['unk']\n",
    "print(f\"Shape of word_embeddings: {word_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word vectors: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "# List of word vectors\n",
    "word_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\n",
    "word_vectors = np.array(word_vectors)\n",
    "print(f\"Shape of word vectors: {word_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        Padding the sequences to the maximum length sequence in the batch\n",
    "        Args:\n",
    "            batch: list of individual elements of the dataset\n",
    "        Returns:\n",
    "            {'text' : padded_texts, 'labels' : padded_labels}\n",
    "    \"\"\"\n",
    "    texts, labels = [item['text'] for item in batch], [item['labels'] for item in batch]\n",
    "    max_len = max([len(text) for text in texts])\n",
    "    padded_texts, padded_labels = [], []\n",
    "    for i in range(len(texts)):\n",
    "        text, label = texts[i], labels[i]\n",
    "        # padding text and label sequences\n",
    "        text = text + [word_to_index['PAD']] * (max_len - len(text))\n",
    "        label = label + [label_to_idx['O']] * (max_len - len(label))\n",
    "        padded_texts.append(text)\n",
    "        padded_labels.append(label)\n",
    "    return {'text': torch.tensor(padded_texts), 'labels': torch.tensor(padded_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaptopReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom Dataset to load the Laptop Review dataset\n",
    "        Args:\n",
    "            data: list of tuples (text, labels)\n",
    "            vocab_size: size of the vocabulary\n",
    "            embedding_size: size of the word embeddings\n",
    "            word_to_index: word-to-index mapping\n",
    "            index_to_word: index-to-word mapping\n",
    "            label_to_idx: label-to-index mapping\n",
    "    \"\"\"\n",
    "    def __init__(self, data, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx):\n",
    "        self.data = data\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.word_to_index = word_to_index\n",
    "        self.index_to_word = index_to_word\n",
    "        self.label_to_idx = label_to_idx \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, labels = self.data[idx][0], self.data[idx][1]\n",
    "        words = text.split()\n",
    "        # converting words and labels to indices\n",
    "        word_indices = [self.word_to_index[word] if word in self.word_to_index else self.word_to_index['UNK'] for word in words]\n",
    "        label_indices = [self.label_to_idx[label] for label in labels]\n",
    "        sample = {'text' : word_indices, 'labels' : label_indices}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LaptopReviewDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "val_dataset = LaptopReviewDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "test_dataset = LaptopReviewDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Text shape: torch.Size([32, 46])\n",
      "Labels shape: torch.Size([32, 46])\n",
      "\n",
      "Text: It has come into good use for my finances , scheduling , my parents business expenses , and it is definitely amazing for gaming . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O O O O O B O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Text: I love the way the entire suite of software works together . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O O O O O O B I I O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n",
    "    for j in range(2):\n",
    "        text = data['text'][j]\n",
    "        labels = data['labels'][j]\n",
    "        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n",
    "        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n",
    "        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: RNN + GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb   \n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'RNN',\n",
    "    embed_size = 300,\n",
    "    embedding = 'GloVe',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprakhar432\u001b[0m (\u001b[33mnlp-assignments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fec2cf5d6c84ca7af145440368cc8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777286247, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_173523-ovt08ksg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg' target=\"_blank\">misty-jazz-8</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2491ef2e910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Model architecture to perform Sequence Labeling on the Laptop Review dataset. RNN, LSTM or GRU model is initialized based on the model configuration parameters.\n",
    "        Args: \n",
    "            vocab_size: size of the vocabulary\n",
    "            embed_size: size of the word embeddings\n",
    "            hidden_size: size of the hidden state\n",
    "            pretrained_embeddings: pre-trained word embeddings\n",
    "            model_config: dictionary containing model configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, pretrained_embeddings, model_config):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=pretrained_embeddings, freeze=True)\n",
    "        self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], nonlinearity=model_config['activation'], batch_first=True, dropout=model_config['dropout'])\n",
    "        if (model_config['model'] == 'LSTM'):\n",
    "            self.rnn = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], batch_first=True, dropout=model_config['dropout'])\n",
    "        if (model_config['model'] == 'GRU'):\n",
    "            self.rnn = nn.GRU(input_size=embed_size, hidden_size=hidden_size, num_layers=model_config['num_hidden'], batch_first=True, dropout=model_config['dropout'])\n",
    "        self.fc = nn.Linear(hidden_size, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            text, labels = data['text'], data['labels']\n",
    "            output, hidden = model(text)\n",
    "            output = output.view(-1, 3)\n",
    "            labels = labels.view(-1)\n",
    "            loss += criterion(output, labels).item()\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += torch.argmax(output, 1).tolist()\n",
    "    accuracy = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return accuracy, macro_f1, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config):\n",
    "    wandb.define_metric('epoch')\n",
    "    wandb.define_metric('minibatch_epoch')\n",
    "    wandb.define_metric('train_loss', step_metric='epoch')\n",
    "    wandb.define_metric('val_loss', step_metric='epoch')    \n",
    "    wandb.define_metric('train_f1', step_metric='epoch')\n",
    "    wandb.define_metric('val_f1', step_metric='epoch')\n",
    "    wandb.define_metric('train_acc', step_metric='epoch')\n",
    "    wandb.define_metric('val_acc', step_metric='epoch')\n",
    "    wandb.define_metric('minibatch_loss', step_metric='minibatch_epoch')\n",
    "    wandb.define_metric('minibatch_acc', step_metric='minibatch_epoch')\n",
    "    wandb.define_metric('minibatch_f1', step_metric='minibatch_epoch')\n",
    "    minibatch = 0\n",
    "    for epoch in tqdm.tqdm(range(model_config['epochs'])):\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            text, labels = data['text'], data['labels']\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(text)\n",
    "            outputs = outputs.view(-1, 3)\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_true = labels.tolist()\n",
    "            y_pred = torch.argmax(outputs, 1).tolist()\n",
    "            minibatch_acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "            minibatch_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            # logging\n",
    "            log = {}\n",
    "            log[\"minibatch_epoch\"] = minibatch\n",
    "            log[\"minibatch_loss\"] = loss.item()\n",
    "            log[\"minibatch_acc\"] = minibatch_acc\n",
    "            log[\"minibatch_f1\"] = minibatch_f1\n",
    "            wandb.log(log)\n",
    "            minibatch += 1\n",
    "        # logging\n",
    "        accuracy, f1, loss = evaluate_model(model, train_dataloader, criterion)\n",
    "        epoch_log = {}\n",
    "        epoch_log[\"epoch\"] = epoch\n",
    "        epoch_log[\"train_loss\"] = loss\n",
    "        epoch_log[\"train_f1\"] = f1\n",
    "        epoch_log[\"train_acc\"] = accuracy\n",
    "        accuracy, f1, loss = evaluate_model(model, val_dataloader, criterion)\n",
    "        epoch_log[\"val_loss\"] = loss\n",
    "        epoch_log[\"val_f1\"] = f1\n",
    "        epoch_log[\"val_acc\"] = accuracy\n",
    "        wandb.log(epoch_log)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            text, labels = data['text'], data['labels']\n",
    "            outputs, hidden = model(text)\n",
    "            outputs = outputs.view(-1, 3)\n",
    "            labels = labels.view(-1)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += torch.argmax(outputs, 1).tolist()\n",
    "    accuracy = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    from sklearn.metrics import classification_report\n",
    "    classification_report = classification_report(y_true, y_pred)\n",
    "    return accuracy, precision, macro_f1, loss, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9653\n",
      "Test precision: 0.9603\n",
      "Test macro F1: 0.6546\n",
      "Test loss: 2.5851\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     14774\n",
      "           1       0.65      0.49      0.56       463\n",
      "           2       0.63      0.32      0.42       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.75      0.60      0.65     15480\n",
      "weighted avg       0.96      0.97      0.96     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c0481048cf411e873489bc18c699b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–â–„â–…â–†â–„â–„â–„â–„â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–…â–†â–‡â–…â–…â–…â–…â–†â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–…â–„â–ƒâ–„â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ</td></tr><tr><td>train_f1</td><td>â–â–ƒâ–…â–…â–†â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–…â–„â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–</td></tr><tr><td>val_acc</td><td>â–â–„â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–†â–…â–…â–…â–†â–…â–†â–…â–†â–…â–…â–…â–„â–…â–…â–…â–…â–„â–…â–„â–…â–‚â–ƒâ–…</td></tr><tr><td>val_f1</td><td>â–â–…â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–…â–ƒâ–‚â–â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96531</td></tr><tr><td>Test loss</td><td>2.58513</td></tr><tr><td>Test macro F1</td><td>0.6546</td></tr><tr><td>Test precision</td><td>0.96029</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00333</td></tr><tr><td>train_acc</td><td>0.99751</td></tr><tr><td>train_f1</td><td>0.98094</td></tr><tr><td>train_loss</td><td>0.27957</td></tr><tr><td>val_acc</td><td>0.97352</td></tr><tr><td>val_f1</td><td>0.69198</td></tr><tr><td>val_loss</td><td>1.04867</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-jazz-8</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/ovt08ksg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_173523-ovt08ksg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'RNN_GloVe_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: GRU + GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'GRU',\n",
    "    embed_size = 300,\n",
    "    embedding = 'GloVe',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_173757-x1aqml96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96' target=\"_blank\">happy-valley-9</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x249272ac590>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 50])\n",
      "Label shape: torch.Size([32, 50])\n",
      "Output shape: torch.Size([32, 50, 3])\n",
      "Hidden shape: torch.Size([1, 32, 128])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_13344\\532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:42<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9691\n",
      "Test precision: 0.9649\n",
      "Test macro F1: 0.6841\n",
      "Test loss: 2.2265\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     14774\n",
      "           1       0.70      0.52      0.60       463\n",
      "           2       0.72      0.35      0.47       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.80      0.62      0.68     15480\n",
      "weighted avg       0.96      0.97      0.97     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69aa12be1714beaa90599091bdb579d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–â–â–†â–…â–â–„â–…â–…â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–‡â–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–‚â–†â–†â–ƒâ–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–‡â–ƒâ–„â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–‚â–â–</td></tr><tr><td>train_acc</td><td>â–â–‚â–ƒâ–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–‚â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–†â–†â–†â–†â–‡</td></tr><tr><td>val_f1</td><td>â–â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96912</td></tr><tr><td>Test loss</td><td>2.22648</td></tr><tr><td>Test macro F1</td><td>0.68414</td></tr><tr><td>Test precision</td><td>0.96489</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99286</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>0.99286</td></tr><tr><td>minibatch_loss</td><td>0.01689</td></tr><tr><td>train_acc</td><td>0.99736</td></tr><tr><td>train_f1</td><td>0.98021</td></tr><tr><td>train_loss</td><td>0.25545</td></tr><tr><td>val_acc</td><td>0.97774</td></tr><tr><td>val_f1</td><td>0.72336</td></tr><tr><td>val_loss</td><td>0.86991</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-valley-9</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/x1aqml96</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_173757-x1aqml96\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'GRU_GloVe_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: LSTM + GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'LSTM',\n",
    "    embed_size = 300,\n",
    "    embedding = 'GloVe',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12afb2131a0432f8d2af9441e613870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888925108, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_174559-g2ta00e7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7' target=\"_blank\">fearless-water-10</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x249272d94d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 66])\n",
      "Label shape: torch.Size([32, 66])\n",
      "Output shape: torch.Size([32, 66, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_13344\\36252246.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [08:05<00:00,  4.86s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9651\n",
      "Test precision: 0.9609\n",
      "Test macro F1: 0.6636\n",
      "Test loss: 2.5930\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     14774\n",
      "           1       0.62      0.52      0.57       463\n",
      "           2       0.65      0.33      0.44       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.75      0.61      0.66     15480\n",
      "weighted avg       0.96      0.97      0.96     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b561ab642f5643cca08b4e0e96845f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–â–„â–ƒâ–ƒâ–†â–„â–„â–…â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–„â–„â–„â–‡â–…â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–„â–…â–…â–‚â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–‚â–„â–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–ƒâ–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_f1</td><td>â–â–â–„â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96512</td></tr><tr><td>Test loss</td><td>2.59304</td></tr><tr><td>Test macro F1</td><td>0.6636</td></tr><tr><td>Test precision</td><td>0.96091</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00276</td></tr><tr><td>train_acc</td><td>0.99739</td></tr><tr><td>train_f1</td><td>0.98052</td></tr><tr><td>train_loss</td><td>0.2643</td></tr><tr><td>val_acc</td><td>0.97711</td></tr><tr><td>val_f1</td><td>0.72763</td></tr><tr><td>val_loss</td><td>0.93883</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-water-10</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/g2ta00e7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_174559-g2ta00e7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'LSTM_GloVe_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: RNN + FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for i in train_data:\n",
    "    train.append([train_data[i]['text'], train_data[i]['labels']])\n",
    "for i in val_data:\n",
    "    val.append([val_data[i]['text'], val_data[i]['labels']])\n",
    "for i in test_data:\n",
    "    test.append([test_data[i]['text'], test_data[i]['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 906\n",
      "Validation data size: 219\n",
      "Test data size: 328\n"
     ]
    }
   ],
   "source": [
    "print(f'Training data size: {len(train)}')\n",
    "print(f'Validation data size: {len(val)}')\n",
    "print(f'Test data size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I charge it at night and skip taking the cord with me because of the good battery life .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Text: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous .\n",
      "Labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Easy to start up and does not overheat as much as other laptops .\n",
      "Labels: ['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Great laptop that offers many great features !\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Text: One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text, labels = train[i][0], train[i][1]\n",
    "    print(f\"Text: {text}\\nLabels: {labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 3495\n",
      "Words and their counts: [('I', 637), ('charge', 10), ('it', 499), ('at', 70), ('night', 5)]\n"
     ]
    }
   ],
   "source": [
    "data = train + val + test\n",
    "# Finding number of unique words in the dataset\n",
    "word_count = {}\n",
    "for i in range(len(data)):\n",
    "    words = data[i][0].split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "print(f\"Number of unique words in the dataset: {len(word_count)}\")\n",
    "print(f\"Words and their counts: {list(word_count.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset after adding 'PAD' and 'UNK': 3497\n"
     ]
    }
   ],
   "source": [
    "word_list = list(word_count.keys())\n",
    "# adding 'PAD' and 'UNK' to the word list\n",
    "word_list.append('PAD')\n",
    "word_list.append('UNK')\n",
    "word_count['PAD'] = 0\n",
    "word_count['UNK'] = 0\n",
    "print(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-to-index: [('I', 0), ('charge', 1), ('it', 2), ('at', 3), ('night', 4)]\n",
      "Index-to-word: [(0, 'I'), (1, 'charge'), (2, 'it'), (3, 'at'), (4, 'night')]\n"
     ]
    }
   ],
   "source": [
    "# Word-to-index and index-to-word mapping from the dataset\n",
    "word_to_index = {word:idx for idx, word in enumerate(word_list)}\n",
    "index_to_word = {idx:word for word, idx in word_to_index.items()}\n",
    "label_to_idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "print(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\n",
    "print(f\"Index-to-word: {list(index_to_word.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word_embeddings: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fasttext_vectors = FastText(language=\"en\")\n",
    "word_embeddings = np.zeros((len(word_list), 300))\n",
    "for i in range(len(word_list)):\n",
    "    word = word_list[i]\n",
    "    idx = word_to_index[word]\n",
    "    if word in glove_vectors.stoi:\n",
    "        word_embeddings[idx] = fasttext_vectors[word]\n",
    "    else:\n",
    "        word_embeddings[idx] = fasttext_vectors['unk']\n",
    "print(f\"Shape of word_embeddings: {word_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word vectors: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "# List of word vectors\n",
    "word_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\n",
    "word_vectors = np.array(word_vectors)\n",
    "print(f\"Shape of word vectors: {word_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LaptopReviewDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "val_dataset = LaptopReviewDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "test_dataset = LaptopReviewDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Text shape: torch.Size([32, 59])\n",
      "Labels shape: torch.Size([32, 59])\n",
      "\n",
      "Text: We carry the netbook around here and there , hence it 's kinda of irritating when the LCD just `` slide '' downwards . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Text: The screen is framed by half- to a full-inch margin that is obviously unnecessary , reduces the screen size and increases the bulk . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O B O O O O O O O O O O O O O O O B I O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n",
    "    for j in range(2):\n",
    "        text = data['text'][j]\n",
    "        labels = data['labels'][j]\n",
    "        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n",
    "        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n",
    "        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb   \n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'RNN',\n",
    "    embed_size = 300,\n",
    "    embedding = 'FastText',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mprakhar432\u001b[0m (\u001b[33mnlp-assignments\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5168dace51c4a3e88c20b7280ceeb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_204707-kc019wqr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr' target=\"_blank\">dutiful-breeze-12</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1dff9475910>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 50])\n",
      "Label shape: torch.Size([32, 50])\n",
      "Output shape: torch.Size([32, 50, 3])\n",
      "Hidden shape: torch.Size([1, 32, 128])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_25772\\532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:12<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9640\n",
      "Test precision: 0.9585\n",
      "Test macro F1: 0.6402\n",
      "Test loss: 2.4762\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     14774\n",
      "           1       0.63      0.48      0.54       463\n",
      "           2       0.58      0.30      0.40       243\n",
      "\n",
      "    accuracy                           0.96     15480\n",
      "   macro avg       0.73      0.59      0.64     15480\n",
      "weighted avg       0.96      0.96      0.96     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37051617b0e24e7e8ae7c857bc2b8202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–â–ƒâ–…â–†â–„â–‚â–„â–†â–…â–†â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–„â–†â–†â–…â–„â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–„â–ƒâ–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–‚â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–‚â–†â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–†â–‡â–‡â–†â–†â–†â–…â–…â–†â–†â–†â–†â–†â–…â–…â–†â–…â–…â–…â–†â–…â–…â–…â–†â–†â–†â–†â–†â–†â–…</td></tr><tr><td>val_f1</td><td>â–â–‚â–†â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96402</td></tr><tr><td>Test loss</td><td>2.47619</td></tr><tr><td>Test macro F1</td><td>0.64025</td></tr><tr><td>Test precision</td><td>0.95848</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99643</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>0.99635</td></tr><tr><td>minibatch_loss</td><td>0.01415</td></tr><tr><td>train_acc</td><td>0.99689</td></tr><tr><td>train_f1</td><td>0.97567</td></tr><tr><td>train_loss</td><td>0.36378</td></tr><tr><td>val_acc</td><td>0.97438</td></tr><tr><td>val_f1</td><td>0.67163</td></tr><tr><td>val_loss</td><td>0.96429</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-breeze-12</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/kc019wqr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_204707-kc019wqr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'RNN_FastText_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: GRU + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'GRU',\n",
    "    embed_size = 300,\n",
    "    embedding = 'FastText',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c66f4b185b4abca9fe609fb2b6effb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_205313-s2wkk9ko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko' target=\"_blank\">sage-grass-13</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1dff94e54d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 66])\n",
      "Label shape: torch.Size([32, 66])\n",
      "Output shape: torch.Size([32, 66, 3])\n",
      "Hidden shape: torch.Size([1, 32, 128])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_25772\\532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [08:34<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9686\n",
      "Test precision: 0.9650\n",
      "Test macro F1: 0.6925\n",
      "Test loss: 2.3416\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     14774\n",
      "           1       0.67      0.56      0.61       463\n",
      "           2       0.71      0.37      0.48       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.79      0.64      0.69     15480\n",
      "weighted avg       0.97      0.97      0.97     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4603ad4938e04d0ca1cf0a3e28e38214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–‚â–â–â–„â–„â–„â–…â–…â–‡â–†â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–‚â–â–‚â–…â–…â–…â–†â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–ˆâ–‡â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–ƒâ–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–â–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–†â–†â–…â–…â–†â–…â–†â–†â–†â–…â–†â–†â–†â–†â–…â–…â–†</td></tr><tr><td>val_f1</td><td>â–â–â–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–‡â–†â–„â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.9686</td></tr><tr><td>Test loss</td><td>2.34158</td></tr><tr><td>Test macro F1</td><td>0.69246</td></tr><tr><td>Test precision</td><td>0.96502</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99355</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>0.99331</td></tr><tr><td>minibatch_loss</td><td>0.02451</td></tr><tr><td>train_acc</td><td>0.99662</td></tr><tr><td>train_f1</td><td>0.97301</td></tr><tr><td>train_loss</td><td>0.33865</td></tr><tr><td>val_acc</td><td>0.97633</td></tr><tr><td>val_f1</td><td>0.7114</td></tr><tr><td>val_loss</td><td>0.96742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-grass-13</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/s2wkk9ko</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_205313-s2wkk9ko\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'GRU_FastText_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: LSTM + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\prakh\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'LSTM',\n",
    "    embed_size = 300,\n",
    "    embedding = 'FastText',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\prakh\\Prakhar\\College\\Semester 6\\Natural Language Processing\\A2\\wandb\\run-20240309_210213-zm4zinnu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu' target=\"_blank\">swift-night-14</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1dff9581b90>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 51])\n",
      "Label shape: torch.Size([32, 51])\n",
      "Output shape: torch.Size([32, 51, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prakh\\AppData\\Local\\Temp\\ipykernel_25772\\36252246.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:55<00:00,  4.15s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9670\n",
      "Test precision: 0.9625\n",
      "Test macro F1: 0.6696\n",
      "Test loss: 2.5365\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     14774\n",
      "           1       0.67      0.53      0.59       463\n",
      "           2       0.64      0.33      0.43       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.76      0.62      0.67     15480\n",
      "weighted avg       0.96      0.97      0.96     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306cef892a4140a3b4e04365df00a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–ƒâ–ƒâ–â–„â–…â–…â–…â–†â–†â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–‚â–ƒâ–â–„â–†â–†â–†â–‡â–‡â–†â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–‡â–‡â–„â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–‚â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–‚â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–‡â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†</td></tr><tr><td>val_f1</td><td>â–â–â–ƒâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–ˆâ–‡â–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96699</td></tr><tr><td>Test loss</td><td>2.53652</td></tr><tr><td>Test macro F1</td><td>0.66959</td></tr><tr><td>Test precision</td><td>0.96254</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>0.99167</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>0.99114</td></tr><tr><td>minibatch_loss</td><td>0.02842</td></tr><tr><td>train_acc</td><td>0.99738</td></tr><tr><td>train_f1</td><td>0.9796</td></tr><tr><td>train_loss</td><td>0.27231</td></tr><tr><td>val_acc</td><td>0.97633</td></tr><tr><td>val_f1</td><td>0.71113</td></tr><tr><td>val_loss</td><td>0.93008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-night-14</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/zm4zinnu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_210213-zm4zinnu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'LSTM_FastText_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: RNN + Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = [], [], []\n",
    "for i in train_data:\n",
    "    train.append([train_data[i]['text'], train_data[i]['labels']])\n",
    "for i in val_data:\n",
    "    val.append([val_data[i]['text'], val_data[i]['labels']])\n",
    "for i in test_data:\n",
    "    test.append([test_data[i]['text'], test_data[i]['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 906\n",
      "Validation data size: 219\n",
      "Test data size: 328\n"
     ]
    }
   ],
   "source": [
    "print(f'Training data size: {len(train)}')\n",
    "print(f'Validation data size: {len(val)}')\n",
    "print(f'Test data size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I charge it at night and skip taking the cord with me because of the good battery life .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Text: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous .\n",
      "Labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Easy to start up and does not overheat as much as other laptops .\n",
      "Labels: ['O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Text: Great laptop that offers many great features !\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Text: One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text, labels = train[i][0], train[i][1]\n",
    "    print(f\"Text: {text}\\nLabels: {labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 3495\n",
      "Words and their counts: [('I', 637), ('charge', 10), ('it', 499), ('at', 70), ('night', 5)]\n"
     ]
    }
   ],
   "source": [
    "data = train + val + test\n",
    "# Finding number of unique words in the dataset\n",
    "word_count = {}\n",
    "for i in range(len(data)):\n",
    "    words = data[i][0].split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "print(f\"Number of unique words in the dataset: {len(word_count)}\")\n",
    "print(f\"Words and their counts: {list(word_count.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset after adding 'PAD' and 'UNK': 3497\n"
     ]
    }
   ],
   "source": [
    "word_list = list(word_count.keys())\n",
    "# adding 'PAD' and 'UNK' to the word list\n",
    "word_list.append('PAD')\n",
    "word_list.append('UNK')\n",
    "word_count['PAD'] = 0\n",
    "word_count['UNK'] = 0\n",
    "print(f\"Number of unique words in the dataset after adding 'PAD' and 'UNK': {len(word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-to-index: [('I', 0), ('charge', 1), ('it', 2), ('at', 3), ('night', 4)]\n",
      "Index-to-word: [(0, 'I'), (1, 'charge'), (2, 'it'), (3, 'at'), (4, 'night')]\n"
     ]
    }
   ],
   "source": [
    "# Word-to-index and index-to-word mapping from the dataset\n",
    "word_to_index = {word:idx for idx, word in enumerate(word_list)}\n",
    "index_to_word = {idx:word for word, idx in word_to_index.items()}\n",
    "label_to_idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "print(f\"Word-to-index: {list(word_to_index.items())[:5]}\")\n",
    "print(f\"Index-to-word: {list(index_to_word.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embeddings: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load word embeddings\n",
    "word_embeddings = []\n",
    "for word in word_list:\n",
    "    try:\n",
    "        word_embeddings.append(wv[word])\n",
    "    except:\n",
    "        word_embeddings.append(wv['unk'])\n",
    "word_embeddings = np.array(word_embeddings)\n",
    "print(f\"Shape of word embeddings: {word_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word vectors: (3497, 300)\n"
     ]
    }
   ],
   "source": [
    "# List of word vectors\n",
    "word_vectors = [word_embeddings[word_to_index[word]] for word in word_list]\n",
    "word_vectors = np.array(word_vectors)\n",
    "print(f\"Shape of word vectors: {word_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LaptopReviewDataset(train, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "val_dataset = LaptopReviewDataset(val, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)\n",
    "test_dataset = LaptopReviewDataset(test, vocab_size, embedding_size, word_to_index, index_to_word, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Text shape: torch.Size([32, 40])\n",
      "Labels shape: torch.Size([32, 40])\n",
      "\n",
      "Text: It is VERY easy to type on and feels great - besides the added feature that the keyboard is lighted . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O O O O O B O O O O O O O O B O O B O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n",
      "Text: It 's fast and has excellent battery life . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "Labels: O O O O O O B I O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    print(f\"Batch {i+1}\\nText shape: {data['text'].size()}\\nLabels shape: {data['labels'].size()}\\n\")\n",
    "    for j in range(2):\n",
    "        text = data['text'][j]\n",
    "        labels = data['labels'][j]\n",
    "        text_str = ' '.join([index_to_word[idx.item()] for idx in text])\n",
    "        labels_str = ' '.join([list(label_to_idx.keys())[idx.item()] for idx in labels])\n",
    "        print(f\"Text: {text_str}\\nLabels: {labels_str}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\saras\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb   \n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'RNN',\n",
    "    embed_size = 300,\n",
    "    embedding = 'Word2Vec',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\saras\\NLP-Assignments\\A2\\wandb\\run-20240309_234257-u9xxwblf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf' target=\"_blank\">helpful-flower-16</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x195ee2b1580>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 78])\n",
      "Label shape: torch.Size([32, 78])\n",
      "Output shape: torch.Size([32, 78, 3])\n",
      "Hidden shape: torch.Size([1, 32, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:25<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9661\n",
      "Test precision: 0.9633\n",
      "Test macro F1: 0.6907\n",
      "Test loss: 2.8735\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     14774\n",
      "           1       0.62      0.58      0.60       463\n",
      "           2       0.64      0.39      0.49       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.75      0.65      0.69     15480\n",
      "weighted avg       0.96      0.97      0.96     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–‚â–â–…â–ƒâ–„â–ƒâ–…â–†â–…â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–â–…â–…â–„â–„â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–‡â–„â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–‚â–ƒâ–„â–…â–…â–…â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–‚â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–‚â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–…â–†â–†â–†â–†â–…â–†â–†â–†â–†â–†â–†â–†</td></tr><tr><td>val_f1</td><td>â–â–ƒâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–ˆâ–„â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.96609</td></tr><tr><td>Test loss</td><td>2.87347</td></tr><tr><td>Test macro F1</td><td>0.69067</td></tr><tr><td>Test precision</td><td>0.96333</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00103</td></tr><tr><td>train_acc</td><td>0.99941</td></tr><tr><td>train_f1</td><td>0.99621</td></tr><tr><td>train_loss</td><td>0.05849</td></tr><tr><td>val_acc</td><td>0.97688</td></tr><tr><td>val_f1</td><td>0.72592</td></tr><tr><td>val_loss</td><td>0.9975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-flower-16</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/u9xxwblf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240309_234257-u9xxwblf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'RNN_Word2Vec_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: GRU + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\saras\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'GRU',\n",
    "    embed_size = 300,\n",
    "    embedding = 'Word2Vec',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\saras\\NLP-Assignments\\A2\\wandb\\run-20240310_012540-2fwxxnma</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma' target=\"_blank\">fragrant-lion-17</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x195ed046360>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 37])\n",
      "Label shape: torch.Size([32, 37])\n",
      "Output shape: torch.Size([32, 37, 3])\n",
      "Hidden shape: torch.Size([1, 32, 128])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saras\\AppData\\Local\\Temp\\ipykernel_18036\\532150329.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\\nHidden shape: {hidden.size()}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [07:22<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9705\n",
      "Test precision: 0.9680\n",
      "Test macro F1: 0.7259\n",
      "Test loss: 2.6822\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     14774\n",
      "           1       0.70      0.64      0.67       463\n",
      "           2       0.69      0.42      0.53       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.79      0.68      0.73     15480\n",
      "weighted avg       0.97      0.97      0.97     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–â–…â–â–…â–…â–„â–…â–†â–†â–†â–…â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–â–…â–ƒâ–†â–†â–…â–†â–†â–‡â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–ƒâ–…â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_f1</td><td>â–â–â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_loss</td><td>â–ˆâ–…â–ƒâ–‚â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.97048</td></tr><tr><td>Test loss</td><td>2.68217</td></tr><tr><td>Test macro F1</td><td>0.72587</td></tr><tr><td>Test precision</td><td>0.96802</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00061</td></tr><tr><td>train_acc</td><td>0.99972</td></tr><tr><td>train_f1</td><td>0.99811</td></tr><tr><td>train_loss</td><td>0.03184</td></tr><tr><td>val_acc</td><td>0.98016</td></tr><tr><td>val_f1</td><td>0.75544</td></tr><tr><td>val_loss</td><td>0.91494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-lion-17</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/2fwxxnma</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240310_012540-2fwxxnma\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'GRU_Word2Vec_Task2_Exp1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 81\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\saras\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    task = 2, \n",
    "    model = 'LSTM',\n",
    "    embed_size = 300,\n",
    "    embedding = 'Word2Vec',\n",
    "    hidden_size = 128,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 32,\n",
    "    epochs = 100, \n",
    "    padding = 'max_post', \n",
    "    loss = 'CrossEntropyLoss',\n",
    "    optimizer = 'Adam',\n",
    "    num_hidden = 1,\n",
    "    dropout = 0, \n",
    "    activation = 'tanh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\saras\\NLP-Assignments\\A2\\wandb\\run-20240310_013559-72kh58p7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7' target=\"_blank\">firm-energy-18</a></strong> to <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp-assignments/assignment-2' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1958c479400>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"assignment-2\", entity=\"nlp-assignments\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3497 3497 300 128\n",
      "Input shape: torch.Size([32, 56])\n",
      "Label shape: torch.Size([32, 56])\n",
      "Output shape: torch.Size([32, 56, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saras\\AppData\\Local\\Temp\\ipykernel_18036\\36252246.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vectors = torch.tensor(word_vectors)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "vocab_size = len(word_to_index)\n",
    "embed_size = model_config['embed_size'] # Size of the word embeddings\n",
    "hidden_size = model_config['hidden_size'] # Size of the hidden state\n",
    "word_vectors = torch.tensor(word_vectors)\n",
    "word_vectors = word_vectors.float()\n",
    "print(vocab_size, len(word_vectors), embed_size, hidden_size)\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, word_vectors, model_config)\n",
    "for i, data in enumerate(train_dataloader, 0):\n",
    "    text = data['text']\n",
    "    print(f\"Input shape: {text.size()}\")\n",
    "    print(f\"Label shape: {data['labels'].size()}\")\n",
    "    output, hidden = model(text)\n",
    "    print(f\"Output shape: {output.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:39<00:00,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9702\n",
      "Test precision: 0.9681\n",
      "Test macro F1: 0.7210\n",
      "Test loss: 2.3645\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     14774\n",
      "           1       0.67      0.65      0.66       463\n",
      "           2       0.71      0.40      0.51       243\n",
      "\n",
      "    accuracy                           0.97     15480\n",
      "   macro avg       0.79      0.68      0.72     15480\n",
      "weighted avg       0.97      0.97      0.97     15480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, macro_f1, loss, classification_report = evaluation(model, test_dataloader, criterion)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\\nTest precision: {precision:.4f}\\nTest macro F1: {macro_f1:.4f}\\nTest loss: {loss:.4f}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report}\")\n",
    "wandb.log({\"Test accuracy\": accuracy, \"Test precision\": precision, \"Test macro F1\": macro_f1, \"Test loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>â–</td></tr><tr><td>Test loss</td><td>â–</td></tr><tr><td>Test macro F1</td><td>â–</td></tr><tr><td>Test precision</td><td>â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_acc</td><td>â–ƒâ–‚â–â–…â–„â–„â–‡â–†â–†â–…â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_f1</td><td>â–ƒâ–‚â–â–†â–…â–…â–‡â–‡â–†â–†â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>minibatch_loss</td><td>â–ˆâ–ˆâ–ˆâ–„â–„â–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_acc</td><td>â–â–â–‚â–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_f1</td><td>â–â–â–‚â–„â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss</td><td>â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_acc</td><td>â–â–â–‚â–…â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>val_f1</td><td>â–â–â–ƒâ–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–†â–ƒâ–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–„â–…â–„â–„â–…â–…â–…â–…â–…â–…</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test accuracy</td><td>0.97016</td></tr><tr><td>Test loss</td><td>2.36452</td></tr><tr><td>Test macro F1</td><td>0.72098</td></tr><tr><td>Test precision</td><td>0.96807</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>minibatch_acc</td><td>1.0</td></tr><tr><td>minibatch_epoch</td><td>2899</td></tr><tr><td>minibatch_f1</td><td>1.0</td></tr><tr><td>minibatch_loss</td><td>0.00146</td></tr><tr><td>train_acc</td><td>0.99969</td></tr><tr><td>train_f1</td><td>0.99797</td></tr><tr><td>train_loss</td><td>0.03573</td></tr><tr><td>val_acc</td><td>0.98227</td></tr><tr><td>val_f1</td><td>0.78481</td></tr><tr><td>val_loss</td><td>0.79025</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">firm-energy-18</strong> at: <a href='https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7' target=\"_blank\">https://wandb.ai/nlp-assignments/assignment-2/runs/72kh58p7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240310_013559-72kh58p7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), 'LSTM_Word2Vec_Task2_Exp1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visiongpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
